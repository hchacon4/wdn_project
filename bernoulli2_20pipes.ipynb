{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hchacon4/wdn_project/blob/main/bernoulli2_20pipes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Epm5SJkrzLx"
      },
      "source": [
        "####Notes\n",
        "- Using SimData_to_csv notebook to create dataset csv\n",
        "- I may want a labels map (dict) for pipe labels like the one here\n",
        "  - https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterating-and-visualizing-the-dataset\n",
        "- Unclear what a one-hot encoded tensor might be used for\n",
        "  - used here as a target label transformation\n",
        "    - https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#lambda-transforms\n",
        "\n",
        "- How do I handle two labels?\n",
        "- Is normalization required?\n",
        " - ans: desirable when features have different ranges. https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=The%20goal%20of%20normalization%20is,when%20features%20have%20different%20ranges.\n",
        "- Complete a panda tutorial\n",
        "- He: Classifer tutorial; CIFAR10 dataset (3 channel images)\n",
        " - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "- Colab Pro\n",
        " - One important caveat to remember while using Colab is that the files you upload to it wonâ€™t be available forever. Colab is a temporary environment with an idle timeout of 90 minutes and an absolute timeout of 12 hours. This means that the runtime will disconnect if it has remained idle for 90 minutes, or if it has been in use for 12 hours [longer for Colab Pro]. On disconnection, you lose all your variables, states, installed packages, and files and will be connected to an entirely new and clean environment on reconnecting. source:https://neptune.ai/blog/google-colab-dealing-with-files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQzqACaz6D71"
      },
      "source": [
        "####Model framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPV_XlfZFlI-",
        "outputId": "4c1dc003-3ff3-4a4f-8e12-219958793f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import pandas as pd   # For loading csv file dataset\n",
        "import random\n",
        "\n",
        "import math   # Used for nan checking (math.isnan())\n",
        "\n",
        "# Straight-Through Estimator\n",
        "# https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html\n",
        "\n",
        "# Autograd\n",
        "# https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
        "\n",
        "# Define model\n",
        "#  -- when called, model returns a output_dim dimensional tensor\n",
        "# def sparseProbMap(batch_probMap, sparsity) :\n",
        "def sparseProbMap(probMap, sparsity) :\n",
        "  \"\"\"Rescale probability map (batch_probMap) to obtain desired sparsity in\n",
        "  measuremets that are turned on.\n",
        "  sparsity = budget (int) / training sample length (meas)\n",
        "    budget -- number of measurement to turn on/ sensor to deploy\n",
        "  \"\"\"\n",
        "  mean = torch.mean(probMap, dim=0, keepdim=True)\n",
        "  scalar = sparsity / mean\n",
        "  beta_scalar = (1 - sparsity) / (1 - mean)\n",
        "  toggle = torch.le(scalar, 1).float()\n",
        "  sparse_probMap = ( toggle * scalar * probMap\n",
        "                      + (1 - toggle) * (1 - (1 - probMap) * beta_scalar) )\n",
        "  # print(f'sparseProbMap(): samp_means {samp_means}')\n",
        "  # print(f'sparseProbMap(): samp_means size {samp_means.size()}')\n",
        "  # print(f'sparseProbMap(): samp_scalars {samp_scalars}')\n",
        "  # print(f'sparseProbMap(): samp_beta_scalars {samp_beta_scalars}')\n",
        "  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n",
        "  return sparse_probMap\n",
        "  \n",
        "  # Alt approaches\n",
        "  # torch method -- with prob_map repeat before this func call (i.e. batch_probMap)\n",
        "  # Notice batch_probMap contains batch_size copies of one prob_map\n",
        "  #  Might be able to make this even faster by running the calc once, then repeating after this call.\n",
        "  # Tensor version -- might speed up training time.\n",
        "  # samp_means = torch.mean(batch_probMap, dim=1, keepdim=True)\n",
        "  # samp_scalars = sparsity / samp_means\n",
        "  # samp_beta_scalars = (1 - sparsity) / (1 - samp_means)\n",
        "  # toggles = torch.le(samp_scalars, 1).float()\n",
        "  # sparse_probMap = ( toggles * samp_scalars * batch_probMap\n",
        "  #                     + (1 - toggles) * (1 - (1 - batch_probMap) * samp_beta_scalars) )\n",
        "\n",
        "  # for samp_probMap in batch_probMap :\n",
        "  #   mean_sampProbMap = torch.mean(samp_probMap)\n",
        "  #   scalar_sampProbMap = sparsity / mean_sampProbMap\n",
        "  #   beta_scalar = (1 - sparsity) / (1 - mean_sampProbMap) # Rename variable to something more descriptive\n",
        "  #   # print(f'sparseProbMap(): samp_probMap {samp_probMap}')\n",
        "  #   # print(f'sparseProbMap(): samp_probMap size {samp_probMap.size()}')\n",
        "  #   # print(f'sparseProbMap(): mean_sampProbMap {mean_sampProbMap}')\n",
        "  #   # assert False\n",
        "  #   toggle = torch.le(scalar_sampProbMap, 1).float()\n",
        "  #   scaled_sampMap = ( toggle * scalar_sampProbMap * samp_probMap\n",
        "  #                     + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar) ).reshape([1, -1])\n",
        "  #   # print(f'sparseProbMap(): scaled_sampMap type {type(scaled_sampMap)}')\n",
        "  #   if scaled_sampMaps == None :\n",
        "  #     scaled_sampMaps = torch.cat((scaled_sampMap,), dim=0).to(device)\n",
        "  #   else :\n",
        "  #     scaled_sampMaps = torch.cat((scaled_sampMaps, scaled_sampMap,), dim=0).to(device)\n",
        "  #   # scaled_sampMaps.append(toggle * scalar_sampProbMap * samp_probMap + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar))\n",
        "  # # print(f'sparseProbMap(): scaled_sampMaps size {scaled_sampMaps.size()}')\n",
        "\n",
        "  # NOTE: Dramatic slowdown after incorporating this function.\n",
        "  #  May be due to use of for loop.\n",
        "  # Use with scaled_sampMaps list version.\n",
        "  # One big tensor. I'd prefer rows. Try adding brackets to append statement.\n",
        "  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n",
        "  # sparse_probMap = torch.cat((scaled_sampMaps,), dim=0)\n",
        "  # sparse_probMap  = sparse_probMap.reshape([batch_size, -1])\n",
        "  # assert False\n",
        "  # return scaled_sampMaps   # Equivalent to sparse_probMap for a batch.\n",
        "\n",
        "class STEFunction(torch.autograd.Function) :\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, probability_mask) :\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input (prob_mask) and return\n",
        "        a Tensor containing the output (binary_mask).\n",
        "        ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        # Push probability map through a bernoulli sampling to create 0/1 mask.\n",
        "        # Return mask.\n",
        "\n",
        "        prob_mask_size = probability_mask.size()\n",
        "        # print(f'STEFunc() forward(): prob_mask_size {prob_mask_size}')\n",
        "        # print(f'STEFunc() forward(): batch_size {batch_size}')\n",
        "\n",
        "        # Bernoulli sampling.\n",
        "        # Sample from a uniform distribution.\n",
        "        # uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n",
        "        uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n",
        "        # Bernoulli sampled binary mask.\n",
        "        binary_mask = (probability_mask > uni_samples).float()\n",
        "        # Note the different sizes of probability_mask and uni_sample. The\n",
        "        #  following operation returns a tensor shaped like uni_sample.\n",
        "        # Note: torch.bernoulli() is not usable because func return only the prob mask,\n",
        "        #  but the uniform distributions samples are needed for gradient calcs.\n",
        "        # bin_mask = torch.bernoulli(probablility_mask)\n",
        "        # print(f'STEFunc() forward(): bin_mask_size {binary_mask.size()}')\n",
        "        # print(f'STEFunc() forward(): probability_mask {probability_mask},\\n\\tuni_samples {uni_samples}')\n",
        "        # assert False\n",
        "\n",
        "        ctx.save_for_backward(probability_mask, uni_samples)\n",
        "        return binary_mask\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output) :\n",
        "      # return F.hardtanh(grad_output)\n",
        "      # SigDeriv graph: https://www.desmos.com/calculator/icbxupp3dh\n",
        "      alpha = 1\n",
        "      prob_mask, uni_samples = ctx.saved_tensors\n",
        "\n",
        "      # Sigmoid function derivative\n",
        "      grad_est = (alpha * torch.exp(-alpha * (prob_mask - uni_samples))\n",
        "          / (1 + torch.exp(-alpha * (prob_mask - uni_samples))) ** 2)\n",
        "          # / torch.exp(1 + -alpha * (prob_mask - uni_samples)) ** 2)\n",
        "      return grad_est * grad_output\n",
        "\n",
        "class StraightThroughEstimator(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    # Consider moving probability map parameters here.\n",
        "    #  If so, change STEFunction call.\n",
        "    super(StraightThroughEstimator, self).__init__()\n",
        "  \n",
        "  def forward(self, probability_mask) :\n",
        "    binary_mask = STEFunction.apply(probability_mask)\n",
        "  # def forward(self, probability_mask, batch_size) :\n",
        "    # binary_mask = STEFunction.apply(probability_mask, batch_size)\n",
        "    return binary_mask\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    # Consider removing the default parameter values.\n",
        "    def __init__(self, input_dim=0, output_dim=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Note: nn.Sequential may take only a single argument. A tuple may work,\n",
        "        #  but unclear if this is stable.\n",
        "        self.binary_STE_stack = nn.Sequential(\n",
        "            # Define the input dimensions\n",
        "            # STE is placed at the bottleneck of the autoencoder.\n",
        "            StraightThroughEstimator(),\n",
        "        )\n",
        "        # self.ste = StraightThroughEstimator()\n",
        "        # self.steFunc = STEFunction().apply\n",
        "        self.sparProbMapFunc = sparseProbMap\n",
        "        unif_samp_tn = torch.zeros([input_dim]).uniform_()\n",
        "        self.mask_params = nn.Parameter(unif_samp_tn)\n",
        "        # fill_val_tn = torch.zeros([input_dim]).fill_(0.5)\n",
        "        # self.mask_params = nn.Parameter(fill_val_tn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        prob_mask = torch.sigmoid(self.mask_params)   # Ensures probabilities lie btwn (0, 1)\n",
        "        batch_size = x.size()[0]\n",
        "        # Used to create bin_mask for ea training sample.\n",
        "        #  To switch to one bin_mask per batch, comment out the following line.\n",
        "        # prob_mask = prob_mask.repeat(batch_size, 1)\n",
        "        sparse_probMask = self.sparProbMapFunc(prob_mask, sparsity=0.012)\n",
        "        sparse_probMask = sparse_probMask.repeat(batch_size, 1)\n",
        "        # sparse_probMask = prob_mask.repeat(batch_size, 1)\n",
        "        # Using a tuple input is problematic because the model expects a grad for all inputs to nn.Sequential.\n",
        "        binary_mask = self.binary_STE_stack(sparse_probMask)\n",
        "        # binary_mask = self.binary_STE_stack(prob_mask)\n",
        "        # binary_mask = self.ste(prob_mask)\n",
        "        # binary_mask = self.steFunc(prob_mask)\n",
        "        # print(f'Encoder.forward(): prob_mask size {prob_mask.size()}')\n",
        "        # print(f'Encoder.forward(): prob_mask size {prob_mask}')\n",
        "        # assert False\n",
        "        # print(x.size())\n",
        "        # print(f'Encoder.forward(): mask_params {self.mask_params}')\n",
        "        # print('Encoder.forward(): batch_size', batch_size)\n",
        "        # print('Encoder.forward(): binary_mask', binary_mask)\n",
        "        return x * binary_mask, binary_mask, self.mask_params\n",
        "\n",
        "    # May or may not need to define the backward behavior of this class.\n",
        "    # def backward(self, x):\n",
        "    #     pass\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):   # output_dim is not yet used.\n",
        "        super(Decoder, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_lrelu_stack = nn.Sequential(\n",
        "            # Define the input dimensions\n",
        "            nn.Linear(input_dim, 512),\n",
        "            # nn.Linear(input_dim, output_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # # Define the output dimensions\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        # Yisong: initialize the weights in the first layer, and the following layers will follow suit.\n",
        "        # self.linear_lrelu_stack[0].weight.data /= 100.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        # print(x.size())\n",
        "        logits = self.linear_lrelu_stack(x)\n",
        "        return logits\n",
        "\n",
        "class Autoencoder(nn.Module) :\n",
        "    def __init__(self, input_dim, output_dim) :\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # self.auto_stack = nn.Sequential(\n",
        "        #     Encoder(input_dim, output_dim),\n",
        "        #     Decoder(),\n",
        "        # )\n",
        "        self.encoder = Encoder(input_dim=input_dim)\n",
        "        self.decoder = Decoder(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, binMaskSizeLs=[], encode=False, decode=False) :\n",
        "        if encode :\n",
        "            pass\n",
        "        elif decode :\n",
        "            pass\n",
        "        else :\n",
        "            # x = self.auto_stack(x)\n",
        "            under_samp_meas, binary_mask, mask_params = self.encoder(x)\n",
        "            # Working out what to do with the binary_mask -- i.e. if it will be fed to decoder or not.\n",
        "            bin_mask_size = binary_mask.sum()\n",
        "            batch_size = x.size()[0]\n",
        "            # binMaskSizeLs.append(bin_mask_size)\n",
        "            # print(f'AE forward(): avg binary_mask size {int(bin_mask_size / batch_size)}')\n",
        "            x = self.decoder(under_samp_meas)\n",
        "        return x, mask_params\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Bs_956ARP3c"
      },
      "outputs": [],
      "source": [
        "# input_dim = 10\n",
        "# init_tensor = torch.zeros([input_dim]).uniform_()\n",
        "# print(init_tensor)\n",
        "# p_mask = nn.Parameter(init_tensor)\n",
        "# print(p_mask)\n",
        "# ste = STEFunction()\n",
        "# print(ste)\n",
        "# masks = ste.forward(init_tensor, p_mask)\n",
        "# a = torch.randn(1, 2, 3, 4)\n",
        "# b = torch.randn(2, 2)\n",
        "# print(a)\n",
        "# print(a.size())\n",
        "# print(a.view(1, 1, 2, 3, 2, 2))\n",
        "# print(b)\n",
        "# print(b * a.view(1, 1, 2, 3, 2, 2))\n",
        "\n",
        "# Create a mask for each sample by first creating a uniform value tn of size\n",
        "#  [batch_size, prob_mask_size], then \n",
        "# c = torch.zeros([10]).uniform_()\n",
        "# print(c)\n",
        "# tn = c.new_empty([3, 10]).uniform_()\n",
        "# print(tn)\n",
        "# bin_mask = (c > tn).float()\n",
        "# print(bin_mask)\n",
        "# End \"Create a mask ...\"\n",
        "\n",
        "# print(c)\n",
        "# mask = STEFunction.forward(c, c)   # Comment out ctx line.\n",
        "# print(mask)   # OK\n",
        "# module = StraightThroughEstimator()\n",
        "# mask = module(c)\n",
        "# print(mask)\n",
        "# module1 = Encoder(input_dim=10)\n",
        "# meas = torch.randint(100, (10,))\n",
        "# print(meas)\n",
        "# mask = module1(meas)\n",
        "# print(mask)   # OK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTY1vNInM3RI"
      },
      "source": [
        "####Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fi2L7VdNGqNb"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch=0, mod=100):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)   # For avg training loss\n",
        "    # print(f'train_loop(): dataset size: {size}')\n",
        "    # print(f'train_loop(): num_batches: {num_batches}')\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    confusion_matrix = torch.zeros(20, 20, dtype=torch.int32)\n",
        "    prevLoss = 100.\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # print(batch, X)\n",
        "        # print(y)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # Compute prediction and loss\n",
        "        pred, prob_params = model(X)  # params for flexibility\n",
        "        # print(y.size())\n",
        "        # print(pred.size())\n",
        "        # print(pred)\n",
        "        # print(prob_params)\n",
        "\n",
        "        # prob_mask * X -> classifier -> pred\n",
        "        # # print(f\"mask_params {model.state_dict()['encoder.mask_params']}\")\n",
        "        # prob_params = model.state_dict()['encoder.mask_params']  # could be slower than passing the params out\n",
        "        prob_map = torch.sigmoid(prob_params)   # Ensures probabilities lie btwn (0, 1)\n",
        "        splus = nn.functional.softplus(prob_params, -1)\n",
        "        # print(torch.max(splus))\n",
        "        # assert torch.max(splus) < 0\n",
        "        if (epoch < 10000) :\n",
        "          lamb = 1 / epoch   # Try 1 / sqrt(epoch)\n",
        "        else :\n",
        "          lamb = 0.\n",
        "        # loss_KL = ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) - torch.log(torch.tensor(0.5)) ).sum()\n",
        "        # p_m = torch.where(prob_map > 0, prob_map, torch.tensor(0.001).to(device))\n",
        "        # p_m = torch.where(prob_map < 1, prob_map, torch.tensor(0.999).to(device))\n",
        "        # # print(f'p_m max {torch.max(p_m)}, min {torch.min(p_m)}')\n",
        "        # # print(f'p_m range {torch.max(p_m) + abs(torch.min(p_m))}')\n",
        "        # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) - torch.log(torch.tensor(0.5)) ).sum()\n",
        "        # # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) ).sum()\n",
        "        # loss_KL = lamb * ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) ).sum()\n",
        "        # Natural log is the result of torch.log(...)\n",
        "        loss_KL = ( (1 - prob_map) * ((-1)*prob_params - splus) - prob_map * splus ).sum()\n",
        "        loss_KL = lamb * loss_KL   # Annealed\n",
        "        # # print(f'loss_KL {loss_KL}')\n",
        "        # # assert loss_KL != float('nan')\n",
        "        # assert not math.isnan(loss_KL)\n",
        "        # # print(loss_KL)\n",
        "\n",
        "        # **** Notice pred is formed on the batch level, but loss_KL is a one shot ****\n",
        "        loss = loss_fn(pred, y) + loss_KL  # returns single value; avg loss across batch\n",
        "        # loss = loss_fn(pred, y)   # returns single value; avg loss across batch\n",
        "        assert loss != float('nan')\n",
        "        # loss = loss_fn(pred, y)  # returns single value; avg loss across batch\n",
        "        # assert False\n",
        "        # print(loss * len(y))\n",
        "        # for i in range(len(y)) :\n",
        "        #   print(pred[i])\n",
        "        #   print(y[i])\n",
        "        #   ls = loss_fn(pred[i], y[i])\n",
        "        #   print(f'loss {i} {ls}')\n",
        "        # print(y)\n",
        "        # print(pred.argmax(1).type(y.dtype))\n",
        "        # # Confusion Matrix\n",
        "        for i in range(len(y)) :\n",
        "          preds = pred.argmax(1).type(y.dtype)\n",
        "          confusion_matrix[y[i]][preds[i]] += 1\n",
        "        # print(confusion_matrix)\n",
        "        \n",
        "        # Top-k predictions\n",
        "        # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
        "        k = 1\n",
        "        # top_k = torch.topk(input=pred, k=k, dim=1,)\n",
        "        # print(top_k)\n",
        "\n",
        "        # Disaggregate performance -- save model\n",
        "        #  goal: extract outliers (in another notebook)\n",
        "        #  Make into a function.\n",
        "        # if epoch > 2000 and loss.item() > (prevLoss * 15) :\n",
        "        # if epoch == 10000 :\n",
        "        #   print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n",
        "        #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n",
        "        # # print(prevLoss)\n",
        "        # prevLoss = loss.item()\n",
        "        # print(prevLoss > loss * 20)\n",
        "        # print(epoch)\n",
        "        # assert False\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # train_loss += loss_fn(pred, y).item() * len(y)   # I think this second call to loss_fn runs the grad twice. ???\n",
        "        train_loss += loss.item() * len(y)\n",
        "        # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n",
        "        train_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n",
        "        # print(loss)\n",
        "\n",
        "        if batch % mod == 0:\n",
        "            # print(batch)\n",
        "            # print(y)\n",
        "            # print(pred)\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # Save last model\n",
        "    # if epoch == 10000 :\n",
        "    #     print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n",
        "    #     torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n",
        "\n",
        "    train_loss /= size    # weighted avg training loss\n",
        "    train_accuracy /= size\n",
        "    print(f\"Training Error: \\n Accuracy: {(100*train_accuracy):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
        "    return train_loss, train_accuracy, confusion_matrix\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, out_dim=10):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # print(f'test dataset size: {size}')\n",
        "    # print(f'test num_batches: {num_batches}')\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    confusion_matrix = torch.zeros(out_dim, out_dim, dtype=torch.int32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            pred, _ = model(X)\n",
        "            # print(pred)\n",
        "            # print(y)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n",
        "            test_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n",
        "            # Confusion matrix\n",
        "            for i in range(len(y)) :\n",
        "              preds = pred.argmax(1).type(y.dtype)\n",
        "              confusion_matrix[y[i]][preds[i]] += 1\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    test_accuracy /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    # return test accuracy percentage for epoch\n",
        "    return test_accuracy, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydc9rlm86Hmt"
      },
      "source": [
        "####Animator (d2l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9KnHcBLdIJYH"
      },
      "outputs": [],
      "source": [
        "def use_svg_display():\n",
        "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
        "    display.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XKViWEWBIK2E"
      },
      "outputs": [],
      "source": [
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xM5q0mh96LMw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "      \n",
        "        self.nrows = nrows\n",
        "        self.ncols = ncols\n",
        "        self.figsize = figsize\n",
        "\n",
        "        self.xlabel = xlabel\n",
        "        self.ylabel = ylabel\n",
        "        self.xlim = xlim\n",
        "        self.ylim = ylim\n",
        "        self.xscale = xscale\n",
        "        self.yscale = yscale\n",
        "        self.legend = legend\n",
        "\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "\n",
        "    \n",
        "    def display_plt(self):\n",
        "        # Borrowed use_svg_display() implementation from d2l\n",
        "        use_svg_display()\n",
        "        # matplot function\n",
        "        self.fig, self.axes = plt.subplots(self.nrows, self.ncols, figsize=self.figsize)\n",
        "        if self.nrows * self.ncols == 1:\n",
        "            self.axes = [self.axes,]\n",
        "        # Use a lambda function to capture arguments; set_axes in d2l API\n",
        "        self.config_axes = lambda: set_axes(self.axes[0],\n",
        "                                            self.xlabel,\n",
        "                                            self.ylabel,\n",
        "                                            self.xlim,\n",
        "                                            self.ylim,\n",
        "                                            self.xscale, self.yscale, self.legend)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        # return self.fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocO0Om3ofeX"
      },
      "source": [
        "####Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AxqpHHcoomED"
      },
      "outputs": [],
      "source": [
        "# %run /content/drive/MyDrive/'Colab Notebooks'/'Water Distribution Network'/'Input Pipeline'/'Data To File'/SimData_to_csv(hdf)_v1.ipynb\n",
        "class Conf_Mat() :\n",
        "  def __init__(self, classes=10) :\n",
        "    \"\"\"classes (int): the number of classes in the classifier.\"\"\"\n",
        "    self.confusion_matrix = torch.zeros(classes, classes, dtype=torch.int32)\n",
        "    # Load labels via JSON or csv file.\n",
        "    self.labels_str = ['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024']\n",
        "  \n",
        "  def addValues(self, pred, y) :\n",
        "    \"\"\"Add values to the confusion matrix.\n",
        "    pred (tensor): tensor containing model predictions.\n",
        "    y (tensor): tensor containing ground truth labels.\n",
        "    return None\n",
        "    \"\"\"\n",
        "    # Confusion Matrix\n",
        "    for i in range(len(y)) :\n",
        "      preds = pred.argmax(1).type(y.dtype)\n",
        "      confusion_matrix[y[i]][preds[i]] += 1\n",
        "    print(confusion_matrix)\n",
        "  \n",
        "  def displayConfMat(self) :\n",
        "    pass\n",
        "\n",
        "def decode_labels() :\n",
        "  # Time consuming\n",
        "  # Might be easier to place the encoder in a file and read it here.\n",
        "  # WARNING: lab_subset order is not consistent.\n",
        "  # NOTE: sets are not subscriptable\n",
        "  # net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir, dest_dir, set_size, time_stamp, debug = feat_lab_args()\n",
        "  # encoded_targets, lab_subset = labels_ds(base_jfile, simdata_dir, set_size, debug)\n",
        "  return sorted(['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024'])\n",
        "# print(decode_labels())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVZsPo1p2RoP"
      },
      "source": [
        "####Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dDXCstVB2XIb"
      },
      "outputs": [],
      "source": [
        "def norm(features) :\n",
        "  # print(f'norm(): {features} size {features.size()}')\n",
        "  sq_feat = features ** 2\n",
        "  # print(f'norm(): {sq_feat} size {sq_feat.size()}')\n",
        "  sum_feat = sq_feat.sum(1)\n",
        "  # print(f'norm(): {sum_feat} size {sum_feat.size()}')\n",
        "  norm_feat = torch.sqrt(sum_feat)\n",
        "  # print(f'norm(): {norm_feat} size {norm_feat.size()}')\n",
        "  # print(norm_feat.view(features.size(0), 1))\n",
        "  unit_feat = features / norm_feat.view(features.size(0), 1)\n",
        "  # print(f'norm(): {unit_feat} size {unit_feat.size()}')\n",
        "  return unit_feat\n",
        "# _, observed, __ = SimData(net_char=4, tmstp=80)\n",
        "# norm(observed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmFOoA-k258k"
      },
      "source": [
        "####Subsampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wCj-uKc561c"
      },
      "source": [
        "#####Mask Generation\n",
        "- (1/0) Sensing Mask Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QZnE0eEEGO88"
      },
      "outputs": [],
      "source": [
        "def sensing_mask_rand(feature_vec, max_sense = 30):\n",
        "  \"\"\"\n",
        "  preconditions: feature tensor must be flat.\n",
        "\n",
        "  Parameters\n",
        "  feature_vec: feature tensor to be masked\n",
        "   shape: list of dimensions\n",
        "  max_sense: randomly choose 1 to max_sense to be 1, rest 0\n",
        "\n",
        "  returns 0/1 sensing mask tensor of shape feature_vec w/ max_sense elems set to 1 (rest 0)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create (0/1) mask populated w/ max_sense ones\n",
        "  mask = torch.zeros(feature_vec.size()).to(device)\n",
        "  # print(mask)\n",
        "  # indices = torch.randint(len(feature_vec), size=(max_sense,))   # may contain fewer than max_sense unique indices\n",
        "  # Katie: Randomize indices and choose the first max_sense\n",
        "  indices = [*range(len(feature_vec))]\n",
        "  random.shuffle(indices)\n",
        "  # print(indices)\n",
        "  # for idx in indices:   # for use with line 16\n",
        "  for idx in indices[:max_sense]:\n",
        "    mask[idx] = 1\n",
        "  # print(mask)\n",
        "\n",
        "  # Pass features through the mask\n",
        "  masked_features = feature_vec * mask\n",
        "\n",
        "  return mask, masked_features.to(device)\n",
        "# feature_vec = torch.rand([100])\n",
        "# print(feature_vec)\n",
        "# mask, masked_feats = sensing_mask_rand(feature_vec)\n",
        "# print(mask, '\\n', masked_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jaJ04dzFpbzL"
      },
      "outputs": [],
      "source": [
        "def sensing_mask_alternate(feature_vec):\n",
        "  \"\"\"\n",
        "  preconditions: feature tensor must be flat.\n",
        "\n",
        "  Parameters\n",
        "  feature_vec: feature tensor to be masked\n",
        "\n",
        "  returns 0/1 sensing mask tensor of shape feature_vec w/ even indexed elems set to 1 (rest 0)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create (0/1) mask populated w/ max_sense ones\n",
        "  mask = torch.zeros(feature_vec.size()).to(device)\n",
        "  # print(mask)\n",
        "  for idx in range(0, feature_vec.size()[0], 2) :\n",
        "    mask[idx] = 1\n",
        "  # print(mask)\n",
        "\n",
        "  # Pass features through the mask\n",
        "  # masked_features = feature_vec * mask\n",
        "\n",
        "  # Simplification: This version reduces the size of the feature\n",
        "  #  vector by half (mask not concat).\n",
        "  masked_features = feature_vec[0].reshape([1])\n",
        "  for idx in range(2, feature_vec.size()[0], 2) :\n",
        "    masked_features = torch.cat((masked_features, feature_vec[idx].reshape([1])))\n",
        "\n",
        "  return mask, masked_features.to(device)\n",
        "# feature_vec = torch.rand([100]).to(device)\n",
        "# print(feature_vec)\n",
        "# mask, masked_feats = sensing_mask_alternate(feature_vec)\n",
        "# print(mask, '\\n', masked_feats.size(), '\\n', masked_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KkFT5x93F5H"
      },
      "source": [
        "#####Random subsamples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xfeD3clN3QKu"
      },
      "outputs": [],
      "source": [
        "def random_subset(meas_tensor, size, seed=None) :\n",
        "  \"\"\"Random subset of measurements (fixed seed).\n",
        "  Must be able to change the cardinality of the subset.\n",
        "  Algorithm\n",
        "    create list of indices\n",
        "    randomize indices\n",
        "    select first x number of indices (where x is the subset cardinality)\n",
        "    form a tensor containing the the sample measurements matching those indices.\n",
        "    return this tensor\n",
        "  \"\"\"\n",
        "  gen = None\n",
        "  if seed :\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "\n",
        "  rand_idxs = torch.randperm(meas_tensor.size()[0], generator=gen)\n",
        "  sub_idxs = rand_idxs[:size]\n",
        "  # print('random_subset():', sub_idxs, sub_idxs.size())\n",
        "  subset = meas_tensor[sub_idxs]\n",
        "  # print(meas_tensor.size())\n",
        "  return subset.to(device), sub_idxs\n",
        "\n",
        "# Test code\n",
        "# X = torch.rand(100)\n",
        "# rand_sub = random_subset(X, 50, 1000)\n",
        "# print('random_subset results:', rand_sub)\n",
        "# print(rand_sub.size())\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Di_I5E4g4PDR"
      },
      "outputs": [],
      "source": [
        "def rand_sub_dataset(X, size, seed=None) :\n",
        "  reduced_meas_X = torch.zeros([X.size(0), size]).to(device)\n",
        "  # print(X.size(0))\n",
        "  for i in range(len(X)) :\n",
        "    rand_sub, sub_idxs = random_subset(X[i], size, seed)\n",
        "    # print('random_subset results:', rand_sub)\n",
        "    # print(rand_sub.size())\n",
        "    reduced_meas_X[i] = rand_sub\n",
        "  return reduced_meas_X, sub_idxs\n",
        "\n",
        "# Test Code\n",
        "# X = torch.rand([3, 20])\n",
        "# size, seed = 10, 1000\n",
        "# reduced_meas_X, sub_idxs = rand_sub_dataset(X, size, seed)\n",
        "# # print(sub_idxs)\n",
        "# for i, idx in enumerate(sub_idxs) :\n",
        "#   # print(reduced_meas_X[0, i], X[0, idx])\n",
        "#   if reduced_meas_X[0, i].item() != X[0, idx].item() :\n",
        "#     raise Exception('rand_sub_dataset(): Error: Value doesnt match.')\n",
        "# # print(X[0, 307], X[0, 536], X[0, 329])\n",
        "# # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n",
        "# print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if63qNQuS0HA"
      },
      "source": [
        "####Load SimData and form concat dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uY7IINhjLGSQ"
      },
      "outputs": [],
      "source": [
        "def SimData(net_char, tmstp) :\n",
        "  \"\"\"\n",
        "  Note: Dataset file generated in SimData_to_csv notebook\n",
        "   row 0: base_case\n",
        "   rows 1->set_size: observed; one leak scenario per row; 1hr (out of one week)\n",
        "   Labels included (last column)\n",
        "  \"\"\"\n",
        "\n",
        "  # Load dataframe\n",
        "  src_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/'\n",
        "  leak_pip_ct = 20\n",
        "  leak_pipes = f'leak_pipes_{leak_pip_ct}/'\n",
        "  if tmstp :\n",
        "    ts_dir = f'tmstp{tmstp}/'\n",
        "  else :\n",
        "    ts_dir = ''\n",
        "  # NOTE: double check dataset file name\n",
        "  if net_char == 0:   dataset_file = 'dataset1000_link_flowrate_area0.01_0.1.csv'\n",
        "  elif net_char == 1: dataset_file = 'dataset1000_link_headloss_area0.01_0.1.cs'\n",
        "  elif net_char == 2: dataset_file = 'dataset1000_link_velocity_area0.01_0.1.cs'\n",
        "  elif net_char == 3: dataset_file = 'dataset1000_node_demand_area0.01_0.1.cs'\n",
        "  elif net_char == 4: dataset_file = 'dataset1000_node_head_area0.01_0.1.csv'\n",
        "  elif net_char == 5: dataset_file = 'dataset1000_node_pressure_area0.01_0.1.csv'\n",
        "  \n",
        "  data_file = src_dir + leak_pipes + ts_dir + dataset_file\n",
        "  datast_ds = pd.read_csv(data_file)\n",
        "  # print(datast_ds.head())\n",
        "  \n",
        "  # Separate base_case, raw_data, and encoded labels\n",
        "  # Base Case\n",
        "  base_case = torch.tensor(datast_ds.values[0, :-1], dtype=torch.float32).to(device)\n",
        "  # print(base_case)\n",
        "  # raw_data\n",
        "  raw_data = torch.tensor(datast_ds.values[1: , :-1], dtype=torch.float32).to(device)\n",
        "  print(f'SimData(): raw_data {raw_data.size()}')\n",
        "  # encoded labels\n",
        "  labels = torch.tensor(datast_ds['Label'], dtype=torch.long)[1:].to(device)\n",
        "  print(f'SimData(): labels {labels.size()}')\n",
        "\n",
        "  return base_case, raw_data, labels\n",
        "# _, __, ___ = SimData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A6Xn9Due5kYf"
      },
      "outputs": [],
      "source": [
        "def leakpipe_subset(pipe_ct, raw, enc_labs):\n",
        "  \"Used to limit the number of pipes in the dataset\"\n",
        "  tmp_raw = torch.tensor([[0]]).to(device)   # dim trouble w/ cat\n",
        "  tmp_labs = torch.tensor([0]).to(device)\n",
        "  print(tmp_labs.size())\n",
        "\n",
        "  for i in range(len(enc_labs)):\n",
        "    if enc_labs[i] < pipe_ct:\n",
        "      tmp_raw = torch.cat((tmp_raw, raw[i].reshape([444])))\n",
        "      tmp_labs = torch.cat((tmp_labs, enc_labs[i].reshape([1])))\n",
        "\n",
        "  return tmp_raw[1:], tmp_labs[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "alN3uGMtzKbn"
      },
      "outputs": [],
      "source": [
        "def cat_net_attr(net_ls, tmstp, residual=False, norm_feats=False) :\n",
        "  \"\"\"Creates three flat vectors containing concatinations of base_case, measurement, and label data.\n",
        "     **If residual is true, don't use norm here.**\n",
        "     \"\"\"\n",
        "  cat_attrs = None\n",
        "  for net_char in net_ls :\n",
        "    # base_case, X_raw, encoded_labels\n",
        "    data = SimData(net_char, tmstp)\n",
        "    data_ls = list(data)   # Creates 3 elem list from data 3-tuple returned by SimData.\n",
        "\n",
        "    if residual :   # Residual before normalization of features.\n",
        "      data_ls[1] -= data_ls[0]\n",
        "    # print(data_ls[1])\n",
        "    # assert False\n",
        "    if norm_feats :   # Normalize each attribute data separtely before cat.\n",
        "      data_ls[1] = norm(data_ls[1])\n",
        "\n",
        "    if not cat_attrs :\n",
        "      cat_attrs = data_ls   # Includes labels. Only needed once.\n",
        "      # cat_attrs = data   # Creates 3 elem list from data 3-tuple returned by SimData.\n",
        "      # cat_attrs = list(cat_attrs)\n",
        "      # if norm_feats :\n",
        "      #   cat_attrs[1] = norm(data[1])\n",
        "    else :\n",
        "      cat_attrs[0] = torch.cat((cat_attrs[0], data_ls[0]))  # Update base_case by cat'ing next net_attr.\n",
        "      cat_attrs[1] = torch.cat((cat_attrs[1], data_ls[1]), dim=1)   # Update features by cat'ing next net_attr.\n",
        "      # if norm_feats :   # Normalize ea attribute data separtely, then cat.\n",
        "      #   cat_attrs[1] = torch.cat((cat_attrs[1], norm(data[1])), dim=1)\n",
        "      # else :\n",
        "      #   cat_attrs[1] = torch.cat((cat_attrs[1], data[1]), dim=1)\n",
        "      # only need one set of label; no need to cat those again.\n",
        "    # Normalize Features -- all together (i.e. if more than one attribute is cat'ed, all will be norm'ed together)\n",
        "    # if norm_feats:\n",
        "      # Raw data normed across all samples\n",
        "      #  Would it make more sense to normalize ea sample individually?\n",
        "      # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n",
        "      # cat_attrs[1] = norm(cat_attrs[1])\n",
        "  print('cat_net_attr():', 'Base', cat_attrs[0].size(), 'X', cat_attrs[1].size(), 'Labels', cat_attrs[2].size())\n",
        "  return cat_attrs   # cat_attrs is a list, but can be expanded by the caller.\n",
        "# _, __, ___ = cat_net_attr([0, 4], 80, True)\n",
        "# print(_.size(), __.size(), ___.size())\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WWwFaq5FMiPh"
      },
      "outputs": [],
      "source": [
        "def cat_data(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], tmstp=None) :\n",
        "  if isinstance(net_char, int) :\n",
        "    net_char = [net_char]\n",
        "  if residual :\n",
        "    n_fs = False\n",
        "  base_case, X_raw, encoded_labels = cat_net_attr(net_char, tmstp, residual, norm_feats=n_fs)   # Don't norm_feat in cat_net_attr if residual is true.\n",
        "  # base_case, X_raw, encoded_labels = SimData(net_char, tmstp)\n",
        "  # print(f'cat_data(): {base_case}')\n",
        "  print(f'cat_data(): X_raw max {torch.max(X_raw)}, min {torch.min(X_raw)}')\n",
        "  print(f'cat_data(): X_raw range {torch.max(X_raw) + abs(torch.min(X_raw))}')\n",
        "  print(f'cat_data(): X_raw {X_raw.size()}')\n",
        "  # print(f'cat_data(): X_raw {X_raw[0]}')\n",
        "  # print(f'cat_data(): base_case {base_case}')\n",
        "  \n",
        "  # work in progress\n",
        "  # X_raw, encoded_labels = leakpipe_subset(10, X_raw, encoded_labels)\n",
        "  # print(f'cat_data: encoded labels \\n{encoded_labels}')\n",
        "  # print(f'cat_data: encoded labels {encoded_labels.size()}')\n",
        "  # assert False\n",
        "\n",
        "  X_cat = None\n",
        "  norm_str = ''   # Used for debug output\n",
        "  div_by_base = base_case   # What is this used for?\n",
        "  # Residual -- handled in cat_net_data\n",
        "  # if residual:\n",
        "    # Avoid dividing by zero\n",
        "    # for idx in range(len(base_case)):\n",
        "      # if base_case[idx] < 1.0e-12:\n",
        "      #   div_by_base[idx] = 1.0e-10\n",
        "    # X_raw = X_raw / div_by_base\n",
        "    # X_raw -= base_case\n",
        "    # base_case /= base_case   # might divide by zero\n",
        "    # pass\n",
        "  # print(f'cat_data(): X_raw {X_raw[0]}')\n",
        "  \n",
        "  # Normalizations\n",
        "  if norm_base:\n",
        "    # Normalize base_case\n",
        "    #  orders of magnitude larger than residual data\n",
        "    #  v = v / max(||v||_p, epsilon)  where epsilon is a small value that void dividing by zero\n",
        "    # norm_base_case = torch.nn.functional.normalize(base_case.reshape([1,-1]))\n",
        "    base_case = torch.nn.functional.normalize(base_case, dim=0) * 10.0   # Out-dated, use with caution.\n",
        "    norm_str = 'norm_'\n",
        "  # if norm_feats:\n",
        "  #   # Raw data normed across all samples\n",
        "  #   #  Would it make more sense to normalize ea sample individually?\n",
        "  #   # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n",
        "  #   X_raw = norm(X_raw)\n",
        "  # print(f'cat_data(): {norm_str}base_case {base_case.size()}')\n",
        "  # # print(f'cat_data(): {norm_str}base_case {base_case}')\n",
        "\n",
        "  # I think I can move if mask outer and elim the for loop.\n",
        "  for feature_vec in X_raw:\n",
        "    if mask :\n",
        "      # Mask and masked features\n",
        "      #  May want sensing_mask_rand() that can process batches of samples\n",
        "      # mask_tn, masked_feats = sensing_mask_rand(feature_vec)\n",
        "      # Construct feature set from concatination of base_case, mask, and masked measuremnts.\n",
        "      # temp = torch.cat((masked_feats.to(device), mask.to(device), base_case.to(device))).reshape([1,-1])\n",
        "      # base_case measurements not included\n",
        "      # temp = torch.cat((masked_feats.to(device), mask_tn.to(device))).reshape([1, -1])\n",
        "      # Simplification: notice feature vector size is halved. Update col variable accordingly.\n",
        "      mask_tn, masked_feats = sensing_mask_alternate(feature_vec)\n",
        "      # Not Needed. Adjust size of label vector.\n",
        "      # masked_labels = encoded_labels[0].reshape([1])\n",
        "      # print(encoded_labels.size())\n",
        "      # assert False\n",
        "      # for idx in range(2, encoded_labels.size()[0], 2) :\n",
        "      #   masked_labels = torch.cat((masked_labels, encoded_labels[idx].reshape([1])))\n",
        "      temp = masked_feats.reshape([1,-1])\n",
        "      # encoded_labels = masked_labels\n",
        "      # print(f'cat_data(), mask: enc_lab size {encoded_labels.size()}')\n",
        "\n",
        "    else :\n",
        "      # Features (no mask)\n",
        "      # Construct feature set from concatination of base_case and observed measuremnts.\n",
        "      # temp = torch.cat((feature_vec.to(device), base_case.to(device))).reshape([1,-1])\n",
        "      # I don't think this is doing anything.  Check if X_raw is changed by this. May already be a flat tensor after cat_net_attrs call.\n",
        "      temp = feature_vec.reshape([1,-1])\n",
        "    # print(f'cat_data(): temp {temp}')\n",
        "\n",
        "    if X_cat is None:\n",
        "      X_cat = temp\n",
        "      # print(X_cat)\n",
        "    else:  \n",
        "      X_cat = torch.cat((X_cat, temp))\n",
        "      # print(X_cat)\n",
        "\n",
        "  print(f'cat_data(): X_cat {X_cat.size()}')\n",
        "  # print(f'cat_data(): X_cat {X_cat[0]}')\n",
        "  # assert False\n",
        "  return X_cat, encoded_labels   # Not returning base_case at this time.\n",
        "# print(f'output: {cat_data(residual=True, norm_base=False, norm_feats=False, mask=False)}')\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdawEj-SS9hP"
      },
      "source": [
        "####Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Zwmb6sYpbm"
      },
      "source": [
        "#####Set-up Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yLehjrJ5SV8M"
      },
      "outputs": [],
      "source": [
        "rows = 0\n",
        "cols = 0\n",
        "input_dim = 0\n",
        "output_dim = 0\n",
        "\n",
        "tr_dataset = None\n",
        "ts_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gJJN6lECYvM_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import statistics as stats\n",
        "\n",
        "def histo_pipe_dist(labels) :\n",
        "  \"\"\"Display histogram of leakpipe distribution.\n",
        "  labels (tensor) : pytorch tensor of labels (ints).\n",
        "  Note: requires matplotlib and Pandas.\n",
        "  \"\"\"\n",
        "  # Histo\n",
        "  Y = pd.Series(labels.cpu())\n",
        "  recounted = Counter(Y)\n",
        "  print(recounted)\n",
        "  std = stats.stdev(recounted.values())\n",
        "  print(f'stdev {std:.2}')\n",
        "  Y.plot.hist(grid=True, bins=10, alpha=0.7, rwidth=0.8, color='#607c8e', align='mid')\n",
        "  plt.title(f'Label Frequency for {len(Y)} Samples')\n",
        "  plt.xlabel('Label')\n",
        "  plt.grid(axis='x')\n",
        "  # plt.text(6, 200, r'class 5 = 229 (46%)')\n",
        "  # assert False\n",
        "  return std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PpisWMQEGjxL"
      },
      "outputs": [],
      "source": [
        "def randomize_dataset(X, y) :\n",
        "  random.seed(10343)\n",
        "  ls = []\n",
        "  for i in range(len(y)) :\n",
        "    zipped = X[i], y[i]\n",
        "    # print(zipped, end=\" \")\n",
        "    ls.append(zipped)\n",
        "  # print('randomize_dataset(): ls', len(ls))\n",
        "  random.shuffle(ls)\n",
        "  shuf_X = torch.empty([1, len(X[0])]).to(device)\n",
        "  # print(shuf_X.size(), shuf_X)\n",
        "  shuf_y = torch.empty([1], dtype=torch.long).to(device)\n",
        "  # print(shuf_X.size(), shuf_X)\n",
        "  for feat, label in ls :\n",
        "    # print('randomize_dataset(): feat', feat.reshape([1, -1]).size())\n",
        "    shuf_X = torch.cat((shuf_X, feat.reshape([1, -1])))\n",
        "    # print('randomize_dataset(): label', label.reshape([1]).size())\n",
        "    # label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "    shuf_y = torch.cat( (shuf_y, label.reshape([1])) )\n",
        "  # print(shuf_X)\n",
        "  # print(shuf_y)\n",
        "  # print('randomize_dataset(): shuf_X', shuf_X.size())\n",
        "  # print('randomize_dataset(): shuf_y', shuf_y.size())\n",
        "  return shuf_X[1:], shuf_y[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WfhI0Do7Ad_o"
      },
      "outputs": [],
      "source": [
        "def find_seed(X, y, tr_size, tr_or_ts) :\n",
        "  min_std = 100\n",
        "  # tr_or_ts = 0  # tr = 0, ts = 1\n",
        "  for i in range(20, 150) :\n",
        "    print(i)\n",
        "    subsets = torch.utils.data.random_split(TensorDataset(X, y),\n",
        "                                            [tr_size, len(y) - tr_size],\n",
        "                                            generator=torch.Generator().manual_seed(i),\n",
        "                                            )\n",
        "    # print(len(subsets),\n",
        "    #       subsets[1].dataset.tensors[1].size(),\n",
        "    #       type(subsets[1].indices),\n",
        "    #       len(subsets[1].indices),\n",
        "    #       subsets[1].dataset.tensors[1][[i for i in subsets[1].indices]],\n",
        "    #       )\n",
        "    std = histo_pipe_dist(subsets[tr_or_ts].dataset.tensors[1][[i for i in subsets[tr_or_ts].indices]])\n",
        "    if std < min_std :\n",
        "      min_std = std\n",
        "      seed = i\n",
        "  print(seed)\n",
        "  return seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qysp3xV5W2dg"
      },
      "source": [
        "#####MNISTfashion set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v2G9BTikRI0k",
        "outputId": "3279128b-1f8f-4ece-a01b-6b94019d1e89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntr_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=True,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nts_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=False,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nrows = 28\\ncols = 28\\ninput_dim = rows*cols\\noutput_dim = 10\\n\\nlearning_rate = 1e-3\\nepochs = 10\\nbatch_size = 64\\nmod = 100\\n#'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# mnist\n",
        "\"\"\"\n",
        "tr_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "ts_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "rows = 28\n",
        "cols = 28\n",
        "input_dim = rows*cols\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "mod = 100\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUCPLjMROO0"
      },
      "source": [
        "#####SimData Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sPNhrjauC1OW",
        "outputId": "3b06bd4c-3b0b-4de5-8a90-f06abd0760aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimData(): raw_data torch.Size([1000, 444])\n",
            "SimData(): labels torch.Size([1000])\n",
            "SimData(): raw_data torch.Size([1000, 396])\n",
            "SimData(): labels torch.Size([1000])\n",
            "cat_net_attr(): Base torch.Size([840]) X torch.Size([1000, 840]) Labels torch.Size([1000])\n",
            "cat_data(): X_raw max 40.62992858886719, min -14.938858032226562\n",
            "cat_data(): X_raw range 55.56878662109375\n",
            "cat_data(): X_raw torch.Size([1000, 840])\n",
            "cat_data(): X_cat torch.Size([1000, 840])\n",
            "Counter({5: 46, 18: 41, 1: 39, 14: 37, 11: 37, 10: 37, 0: 36, 8: 35, 12: 35, 2: 35, 13: 35, 3: 35, 9: 34, 4: 33, 17: 33, 7: 31, 15: 31, 19: 31, 16: 30, 6: 29})\n",
            "stdev 4.0\n",
            "cols 840\n",
            "Autoencoder(\n",
            "  (encoder): Encoder(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "    (binary_STE_stack): Sequential(\n",
            "      (0): StraightThroughEstimator()\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "    (linear_lrelu_stack): Sequential(\n",
            "      (0): Linear(in_features=840, out_features=512, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Linear(in_features=512, out_features=20, bias=True)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Network characterist options:\n",
        "#  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n",
        "#   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n",
        "residual = True  # Subtract measured from base_case; 0 if normal behavior, else non-zero.\n",
        "norm_base = False\n",
        "norm_feats = True   \n",
        "mask = False\n",
        "net_char = 0\n",
        "tmstp = 80\n",
        "cat_attrs = True\n",
        "subsample = False\n",
        "if cat_attrs :\n",
        "  net_char = [0, 4]\n",
        "X, y = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n",
        "# print(y)\n",
        "# torch.set_printoptions(edgeitems=50)\n",
        "# print(f'Training Set: X[0] {X[0]}')\n",
        "\n",
        "# Visualize the dataset leak pipe distribution.\n",
        "# histo_pipe_dist(y)\n",
        "\n",
        "if subsample :\n",
        "  perc_of_meas = 0.01\n",
        "  size = int(X.size(1) * perc_of_meas)\n",
        "  print('subsample: size ', size)\n",
        "  seed = 1001\n",
        "  reduced_meas_X, __ = rand_sub_dataset(X, size, seed)\n",
        "  print('subsample:', reduced_meas_X.size())\n",
        "  # print(X[0, 307], X[0, 536], X[0, 329])\n",
        "  # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n",
        "  X = reduced_meas_X\n",
        "\n",
        "torch.set_printoptions(edgeitems=3)\n",
        "tr_size = int(len(X)*0.7)\n",
        "ts_size = len(y) - tr_size\n",
        "# tr_dataset = TensorDataset(X[:split_idx], y[:split_idx])\n",
        "# ts_dataset = TensorDataset(X[split_idx:], y[split_idx:])\n",
        "# Find a seed that creates a training set pipe distribution with smallest stdev.\n",
        "seed = 135   # find_seed prints and returns best seed.\n",
        "# seed = find_seed(X, y, tr_size, 0)   # Commented out to eliminate the overhead of looking for a seed everytime.\n",
        "# assert False\n",
        "tr_dataset, ts_dataset = torch.utils.data.random_split(TensorDataset(X, y),\n",
        "                                        [tr_size, ts_size],\n",
        "                                        generator=torch.Generator().manual_seed(seed),\n",
        "                                        )\n",
        "\n",
        "# X, y = randomize_dataset(X, y)\n",
        "# histo_pipe_dist(y[:split_idx])\n",
        "# histo_pipe_dist(y[split_idx:])\n",
        "idx = tr_dataset.indices\n",
        "histo_pipe_dist(tr_dataset.dataset.tensors[1][[i for i in idx]])\n",
        "\n",
        "# Determine learning rate and measurement vector size\n",
        "if net_char == 0:\n",
        "  # link_flowrate\n",
        "  cols = 444   # links = 444; junctions = 396\n",
        "  # learning_rate = 9e-3   # single layer\n",
        "  learning_rate = 8e-2\n",
        "if net_char == 2:\n",
        "  # link_velocity\n",
        "  cols = 444   # links = 444; junctions = 396\n",
        "  learning_rate = 2e-7\n",
        "elif net_char == 3:\n",
        "  # node_demand\n",
        "  cols = 396   # links = 444; junctions = 396\n",
        "  learning_rate = 2e-6\n",
        "  epochs = 2000\n",
        "elif net_char == 4:\n",
        "  # node_head\n",
        "  cols = 396   # links = 444; junctions = 396\n",
        "  learning_rate = 7e-2\n",
        "  epochs = 2000\n",
        "elif net_char == 5:\n",
        "  # node_pressure\n",
        "  cols = 396   # links = 444; junctions = 396\n",
        "  learning_rate = 1e-3\n",
        "elif isinstance(net_char, list) :\n",
        "  if norm_feats and cat_attrs :\n",
        "    learning_rate = 2e-2\n",
        "  else :\n",
        "    learning_rate = 8e-2   #(f, h, nf, nh, f+h)\n",
        "\n",
        "# Determine number of concatenated vectors. Used for determining input_dim\n",
        "concats = 1\n",
        "if mask :\n",
        "  concats = 1\n",
        "\n",
        "elif norm_feats :\n",
        "  pass\n",
        "# Simplification adjustments\n",
        "cols = X.size(1)\n",
        "print(f'cols {cols}')\n",
        "\n",
        "\n",
        "rows = 1\n",
        "input_dim = rows*cols*concats\n",
        "output_dim = 20\n",
        "epochs = 20000\n",
        "batch_size = 128\n",
        "mod = 5\n",
        "\n",
        "# Instantiate model framework\n",
        "# model = Decoder(input_dim, output_dim).to(device)\n",
        "model = Autoencoder(input_dim, output_dim).to(device)\n",
        "print(model)\n",
        "# print(*model.parameters())\n",
        "# print(f\"mask_params {model.state_dict()['encoder.mask_params'].size()}\")\n",
        "# assert False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQdyRZRYXiQJ"
      },
      "source": [
        "#####Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LItPDTO9LCoY",
        "outputId": "4ddc9f2f-7cf2-4ea0-dafc-740c928f09b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.721882 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.816666 \n",
            "\n",
            "Epoch 14337\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.646275 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.829238 \n",
            "\n",
            "Epoch 14338\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.641048 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.844340 \n",
            "\n",
            "Epoch 14339\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.619407 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.744324 \n",
            "\n",
            "Epoch 14340\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.642892 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.853744 \n",
            "\n",
            "Epoch 14341\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.700174 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.828314 \n",
            "\n",
            "Epoch 14342\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.664668 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.809039 \n",
            "\n",
            "Epoch 14343\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.650534 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.840441 \n",
            "\n",
            "Epoch 14344\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.650064 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.852266 \n",
            "\n",
            "Epoch 14345\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.659980 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.739686 \n",
            "\n",
            "Epoch 14346\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.1%, Avg loss: 1.623362 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.730256 \n",
            "\n",
            "Epoch 14347\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.679736 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.760141 \n",
            "\n",
            "Epoch 14348\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.630074 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.813319 \n",
            "\n",
            "Epoch 14349\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.650753 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.796589 \n",
            "\n",
            "Epoch 14350\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.675547 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.758911 \n",
            "\n",
            "Epoch 14351\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.657658 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.742157 \n",
            "\n",
            "Epoch 14352\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.670515 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.824666 \n",
            "\n",
            "Epoch 14353\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.647288 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.830478 \n",
            "\n",
            "Epoch 14354\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.675016 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.804353 \n",
            "\n",
            "Epoch 14355\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.676190 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.861398 \n",
            "\n",
            "Epoch 14356\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.653100 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.779178 \n",
            "\n",
            "Epoch 14357\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.624694 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.829442 \n",
            "\n",
            "Epoch 14358\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.656150 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.817356 \n",
            "\n",
            "Epoch 14359\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.649353 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.735816 \n",
            "\n",
            "Epoch 14360\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.627839 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.825260 \n",
            "\n",
            "Epoch 14361\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.660969 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.792505 \n",
            "\n",
            "Epoch 14362\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.686847 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.798867 \n",
            "\n",
            "Epoch 14363\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.642567 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.736542 \n",
            "\n",
            "Epoch 14364\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.683357 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.714684 \n",
            "\n",
            "Epoch 14365\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.615479 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.877278 \n",
            "\n",
            "Epoch 14366\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.3%, Avg loss: 1.615521 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.757076 \n",
            "\n",
            "Epoch 14367\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.619337 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 2.153781 \n",
            "\n",
            "Epoch 14368\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.659782 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.737856 \n",
            "\n",
            "Epoch 14369\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.677434 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.760838 \n",
            "\n",
            "Epoch 14370\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.625644 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.919118 \n",
            "\n",
            "Epoch 14371\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.670799 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.760378 \n",
            "\n",
            "Epoch 14372\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.642009 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.893876 \n",
            "\n",
            "Epoch 14373\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.636970 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.793131 \n",
            "\n",
            "Epoch 14374\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.611633 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.866989 \n",
            "\n",
            "Epoch 14375\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.671272 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.733343 \n",
            "\n",
            "Epoch 14376\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.671723 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.895715 \n",
            "\n",
            "Epoch 14377\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.644612 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.735056 \n",
            "\n",
            "Epoch 14378\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.626583 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.866556 \n",
            "\n",
            "Epoch 14379\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.710322 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.856740 \n",
            "\n",
            "Epoch 14380\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.645460 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.861129 \n",
            "\n",
            "Epoch 14381\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.600662 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.833029 \n",
            "\n",
            "Epoch 14382\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.653834 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.767780 \n",
            "\n",
            "Epoch 14383\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.707914 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.845295 \n",
            "\n",
            "Epoch 14384\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.665100 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.822461 \n",
            "\n",
            "Epoch 14385\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.639449 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.765580 \n",
            "\n",
            "Epoch 14386\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.651961 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.842754 \n",
            "\n",
            "Epoch 14387\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.651234 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.783040 \n",
            "\n",
            "Epoch 14388\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.646155 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.750932 \n",
            "\n",
            "Epoch 14389\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.680628 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.858860 \n",
            "\n",
            "Epoch 14390\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.640759 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.737503 \n",
            "\n",
            "Epoch 14391\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.634561 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.852795 \n",
            "\n",
            "Epoch 14392\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.648960 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.724300 \n",
            "\n",
            "Epoch 14393\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.644581 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.787431 \n",
            "\n",
            "Epoch 14394\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.641964 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.770007 \n",
            "\n",
            "Epoch 14395\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.638533 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.985545 \n",
            "\n",
            "Epoch 14396\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.687582 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.785781 \n",
            "\n",
            "Epoch 14397\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.671194 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.780122 \n",
            "\n",
            "Epoch 14398\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.631986 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.880647 \n",
            "\n",
            "Epoch 14399\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.668354 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.821828 \n",
            "\n",
            "Epoch 14400\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.647736 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.776539 \n",
            "\n",
            "Epoch 14401\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.644050 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.752533 \n",
            "\n",
            "Epoch 14402\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.666088 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.823291 \n",
            "\n",
            "Epoch 14403\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 32.9%, Avg loss: 1.693967 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 2.007873 \n",
            "\n",
            "Epoch 14404\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.682661 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.853094 \n",
            "\n",
            "Epoch 14405\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.682848 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.815527 \n",
            "\n",
            "Epoch 14406\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.666628 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.853622 \n",
            "\n",
            "Epoch 14407\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.678177 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.782468 \n",
            "\n",
            "Epoch 14408\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.592119 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.809158 \n",
            "\n",
            "Epoch 14409\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.659384 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.867659 \n",
            "\n",
            "Epoch 14410\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.667424 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.732501 \n",
            "\n",
            "Epoch 14411\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.634285 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.821032 \n",
            "\n",
            "Epoch 14412\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.678085 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.867693 \n",
            "\n",
            "Epoch 14413\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.656205 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 28.3%, Avg loss: 1.761275 \n",
            "\n",
            "Epoch 14414\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.610279 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.765067 \n",
            "\n",
            "Epoch 14415\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.694891 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.723272 \n",
            "\n",
            "Epoch 14416\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.669605 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.747565 \n",
            "\n",
            "Epoch 14417\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.643671 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 1.816202 \n",
            "\n",
            "Epoch 14418\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.616892 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.984769 \n",
            "\n",
            "Epoch 14419\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.634455 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.799796 \n",
            "\n",
            "Epoch 14420\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.651154 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.793221 \n",
            "\n",
            "Epoch 14421\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.634153 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.743541 \n",
            "\n",
            "Epoch 14422\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.655112 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.805855 \n",
            "\n",
            "Epoch 14423\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.652595 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.901949 \n",
            "\n",
            "Epoch 14424\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.693662 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.822191 \n",
            "\n",
            "Epoch 14425\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.672466 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.825055 \n",
            "\n",
            "Epoch 14426\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.643269 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.842893 \n",
            "\n",
            "Epoch 14427\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.624770 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.971039 \n",
            "\n",
            "Epoch 14428\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.696225 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.891409 \n",
            "\n",
            "Epoch 14429\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.614179 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.761530 \n",
            "\n",
            "Epoch 14430\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.650466 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.781378 \n",
            "\n",
            "Epoch 14431\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.682072 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.793316 \n",
            "\n",
            "Epoch 14432\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.662692 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.845583 \n",
            "\n",
            "Epoch 14433\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.624203 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.821304 \n",
            "\n",
            "Epoch 14434\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.685667 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.753629 \n",
            "\n",
            "Epoch 14435\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.643572 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.829855 \n",
            "\n",
            "Epoch 14436\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.630129 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.752922 \n",
            "\n",
            "Epoch 14437\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.657473 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.854137 \n",
            "\n",
            "Epoch 14438\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.670915 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.745580 \n",
            "\n",
            "Epoch 14439\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.671264 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.860062 \n",
            "\n",
            "Epoch 14440\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.666514 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.753577 \n",
            "\n",
            "Epoch 14441\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.728776 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.745436 \n",
            "\n",
            "Epoch 14442\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.653935 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.955687 \n",
            "\n",
            "Epoch 14443\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.640577 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.762478 \n",
            "\n",
            "Epoch 14444\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.683678 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.855788 \n",
            "\n",
            "Epoch 14445\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.661205 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.768499 \n",
            "\n",
            "Epoch 14446\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.645914 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.812653 \n",
            "\n",
            "Epoch 14447\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.613765 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.803378 \n",
            "\n",
            "Epoch 14448\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.645559 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.838418 \n",
            "\n",
            "Epoch 14449\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.748091 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.788032 \n",
            "\n",
            "Epoch 14450\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.656830 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.877554 \n",
            "\n",
            "Epoch 14451\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.665704 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.873829 \n",
            "\n",
            "Epoch 14452\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.687405 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.912598 \n",
            "\n",
            "Epoch 14453\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.625938 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.766989 \n",
            "\n",
            "Epoch 14454\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.672980 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.722007 \n",
            "\n",
            "Epoch 14455\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.647879 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.809055 \n",
            "\n",
            "Epoch 14456\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.655779 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.799294 \n",
            "\n",
            "Epoch 14457\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.642692 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.833126 \n",
            "\n",
            "Epoch 14458\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.654412 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.869558 \n",
            "\n",
            "Epoch 14459\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.639334 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.752221 \n",
            "\n",
            "Epoch 14460\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.631571 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.766099 \n",
            "\n",
            "Epoch 14461\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.667508 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.727416 \n",
            "\n",
            "Epoch 14462\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.691474 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.696271 \n",
            "\n",
            "Epoch 14463\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.617419 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.890524 \n",
            "\n",
            "Epoch 14464\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.663275 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.833451 \n",
            "\n",
            "Epoch 14465\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.655085 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.910392 \n",
            "\n",
            "Epoch 14466\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.689732 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.723426 \n",
            "\n",
            "Epoch 14467\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 33.6%, Avg loss: 1.677471 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.808747 \n",
            "\n",
            "Epoch 14468\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.613110 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.762740 \n",
            "\n",
            "Epoch 14469\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.618731 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.863576 \n",
            "\n",
            "Epoch 14470\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.654582 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.872240 \n",
            "\n",
            "Epoch 14471\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.635417 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.784783 \n",
            "\n",
            "Epoch 14472\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.667089 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.755087 \n",
            "\n",
            "Epoch 14473\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.645157 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.942283 \n",
            "\n",
            "Epoch 14474\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.660244 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.810856 \n",
            "\n",
            "Epoch 14475\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.667679 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.814043 \n",
            "\n",
            "Epoch 14476\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.653795 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.897380 \n",
            "\n",
            "Epoch 14477\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.639386 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.847228 \n",
            "\n",
            "Epoch 14478\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.654092 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.906906 \n",
            "\n",
            "Epoch 14479\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.638961 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.786724 \n",
            "\n",
            "Epoch 14480\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.679169 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.956509 \n",
            "\n",
            "Epoch 14481\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.660553 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.860647 \n",
            "\n",
            "Epoch 14482\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.615974 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.807566 \n",
            "\n",
            "Epoch 14483\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.713123 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.766715 \n",
            "\n",
            "Epoch 14484\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.698637 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.815000 \n",
            "\n",
            "Epoch 14485\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.613026 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.795817 \n",
            "\n",
            "Epoch 14486\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.644360 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.785815 \n",
            "\n",
            "Epoch 14487\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.692233 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.813647 \n",
            "\n",
            "Epoch 14488\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.661529 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.874506 \n",
            "\n",
            "Epoch 14489\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.689299 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.804150 \n",
            "\n",
            "Epoch 14490\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.659092 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.783147 \n",
            "\n",
            "Epoch 14491\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.624049 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.861341 \n",
            "\n",
            "Epoch 14492\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.635356 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.757194 \n",
            "\n",
            "Epoch 14493\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.661846 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.840921 \n",
            "\n",
            "Epoch 14494\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.679244 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.839414 \n",
            "\n",
            "Epoch 14495\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.633429 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.945302 \n",
            "\n",
            "Epoch 14496\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.703688 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.809499 \n",
            "\n",
            "Epoch 14497\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.675697 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.745999 \n",
            "\n",
            "Epoch 14498\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.689661 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.879395 \n",
            "\n",
            "Epoch 14499\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.663794 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.785359 \n",
            "\n",
            "Epoch 14500\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.651297 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.835512 \n",
            "\n",
            "Epoch 14501\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.627262 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.796266 \n",
            "\n",
            "Epoch 14502\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.651527 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.779735 \n",
            "\n",
            "Epoch 14503\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.678583 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.756367 \n",
            "\n",
            "Epoch 14504\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.713505 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.723319 \n",
            "\n",
            "Epoch 14505\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.668790 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.712238 \n",
            "\n",
            "Epoch 14506\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.623559 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.964170 \n",
            "\n",
            "Epoch 14507\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.670386 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.764639 \n",
            "\n",
            "Epoch 14508\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.674654 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.830341 \n",
            "\n",
            "Epoch 14509\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.677845 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.833692 \n",
            "\n",
            "Epoch 14510\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.652724 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.827436 \n",
            "\n",
            "Epoch 14511\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.6%, Avg loss: 1.637145 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.775224 \n",
            "\n",
            "Epoch 14512\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.643871 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.800689 \n",
            "\n",
            "Epoch 14513\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.676196 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.800399 \n",
            "\n",
            "Epoch 14514\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.592863 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.862113 \n",
            "\n",
            "Epoch 14515\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.685510 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.864315 \n",
            "\n",
            "Epoch 14516\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.667163 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.840513 \n",
            "\n",
            "Epoch 14517\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.616477 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.795770 \n",
            "\n",
            "Epoch 14518\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.640358 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.770245 \n",
            "\n",
            "Epoch 14519\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.633713 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.722287 \n",
            "\n",
            "Epoch 14520\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.662336 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.748574 \n",
            "\n",
            "Epoch 14521\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.704425 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.868361 \n",
            "\n",
            "Epoch 14522\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.642002 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.806098 \n",
            "\n",
            "Epoch 14523\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.632620 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.747785 \n",
            "\n",
            "Epoch 14524\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.649905 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.805027 \n",
            "\n",
            "Epoch 14525\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.683499 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.932128 \n",
            "\n",
            "Epoch 14526\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.692261 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.770044 \n",
            "\n",
            "Epoch 14527\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.658221 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.773615 \n",
            "\n",
            "Epoch 14528\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.659339 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.739787 \n",
            "\n",
            "Epoch 14529\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.604300 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.811325 \n",
            "\n",
            "Epoch 14530\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.620497 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.772785 \n",
            "\n",
            "Epoch 14531\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.645317 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.749907 \n",
            "\n",
            "Epoch 14532\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.686013 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.764193 \n",
            "\n",
            "Epoch 14533\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.615373 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.810601 \n",
            "\n",
            "Epoch 14534\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.636592 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.776903 \n",
            "\n",
            "Epoch 14535\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.651278 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.798299 \n",
            "\n",
            "Epoch 14536\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.694443 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.832154 \n",
            "\n",
            "Epoch 14537\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.687110 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.805150 \n",
            "\n",
            "Epoch 14538\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.621983 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.743524 \n",
            "\n",
            "Epoch 14539\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.653874 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.856795 \n",
            "\n",
            "Epoch 14540\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.635831 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.758822 \n",
            "\n",
            "Epoch 14541\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.668054 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.848468 \n",
            "\n",
            "Epoch 14542\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.646515 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.760423 \n",
            "\n",
            "Epoch 14543\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.634688 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.914212 \n",
            "\n",
            "Epoch 14544\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.642208 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.825432 \n",
            "\n",
            "Epoch 14545\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.647695 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.796858 \n",
            "\n",
            "Epoch 14546\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.652225 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.835989 \n",
            "\n",
            "Epoch 14547\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.683795 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.896711 \n",
            "\n",
            "Epoch 14548\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.615290 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.909761 \n",
            "\n",
            "Epoch 14549\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.648998 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.797115 \n",
            "\n",
            "Epoch 14550\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.712648 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.846667 \n",
            "\n",
            "Epoch 14551\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.660217 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.796468 \n",
            "\n",
            "Epoch 14552\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.639330 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.902220 \n",
            "\n",
            "Epoch 14553\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.609189 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.880142 \n",
            "\n",
            "Epoch 14554\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.706009 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.871970 \n",
            "\n",
            "Epoch 14555\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.665986 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.833167 \n",
            "\n",
            "Epoch 14556\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.649141 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.844258 \n",
            "\n",
            "Epoch 14557\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.662112 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.810378 \n",
            "\n",
            "Epoch 14558\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.629564 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.859514 \n",
            "\n",
            "Epoch 14559\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.6%, Avg loss: 1.637577 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.759756 \n",
            "\n",
            "Epoch 14560\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.658940 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.929883 \n",
            "\n",
            "Epoch 14561\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.696764 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.804434 \n",
            "\n",
            "Epoch 14562\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.648332 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.727669 \n",
            "\n",
            "Epoch 14563\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.625839 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.011016 \n",
            "\n",
            "Epoch 14564\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.707163 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.804644 \n",
            "\n",
            "Epoch 14565\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.619206 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.785456 \n",
            "\n",
            "Epoch 14566\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.625525 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.841845 \n",
            "\n",
            "Epoch 14567\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.700592 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.869766 \n",
            "\n",
            "Epoch 14568\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.626595 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.824669 \n",
            "\n",
            "Epoch 14569\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.643847 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.826974 \n",
            "\n",
            "Epoch 14570\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.678974 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.778616 \n",
            "\n",
            "Epoch 14571\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.635644 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.994789 \n",
            "\n",
            "Epoch 14572\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.679097 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.820594 \n",
            "\n",
            "Epoch 14573\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.685771 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.763506 \n",
            "\n",
            "Epoch 14574\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.661407 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.886682 \n",
            "\n",
            "Epoch 14575\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.622236 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.757023 \n",
            "\n",
            "Epoch 14576\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.602105 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.787647 \n",
            "\n",
            "Epoch 14577\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.641890 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.906310 \n",
            "\n",
            "Epoch 14578\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.680880 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.869990 \n",
            "\n",
            "Epoch 14579\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.656277 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.701037 \n",
            "\n",
            "Epoch 14580\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.688078 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.716721 \n",
            "\n",
            "Epoch 14581\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.638638 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.908295 \n",
            "\n",
            "Epoch 14582\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.663808 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.794230 \n",
            "\n",
            "Epoch 14583\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.628360 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.762309 \n",
            "\n",
            "Epoch 14584\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.609423 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.765040 \n",
            "\n",
            "Epoch 14585\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.658957 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.724612 \n",
            "\n",
            "Epoch 14586\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.640353 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.787082 \n",
            "\n",
            "Epoch 14587\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.685299 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.691362 \n",
            "\n",
            "Epoch 14588\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.643279 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.852319 \n",
            "\n",
            "Epoch 14589\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.663517 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.823465 \n",
            "\n",
            "Epoch 14590\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.633447 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.833324 \n",
            "\n",
            "Epoch 14591\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.676279 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.782672 \n",
            "\n",
            "Epoch 14592\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.663307 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.888576 \n",
            "\n",
            "Epoch 14593\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.691208 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.828655 \n",
            "\n",
            "Epoch 14594\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.666730 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.743439 \n",
            "\n",
            "Epoch 14595\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.640186 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.858530 \n",
            "\n",
            "Epoch 14596\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.668822 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.778549 \n",
            "\n",
            "Epoch 14597\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.644675 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.831452 \n",
            "\n",
            "Epoch 14598\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.698165 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.092647 \n",
            "\n",
            "Epoch 14599\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.673342 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.757075 \n",
            "\n",
            "Epoch 14600\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.633422 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.814016 \n",
            "\n",
            "Epoch 14601\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.679082 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.818905 \n",
            "\n",
            "Epoch 14602\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.610535 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.756841 \n",
            "\n",
            "Epoch 14603\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.642334 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.820958 \n",
            "\n",
            "Epoch 14604\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.667055 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.986878 \n",
            "\n",
            "Epoch 14605\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.654847 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.919307 \n",
            "\n",
            "Epoch 14606\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.635896 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.858078 \n",
            "\n",
            "Epoch 14607\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.613211 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.732925 \n",
            "\n",
            "Epoch 14608\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.673468 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.777749 \n",
            "\n",
            "Epoch 14609\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.673727 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.724580 \n",
            "\n",
            "Epoch 14610\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.634576 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.797606 \n",
            "\n",
            "Epoch 14611\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.608346 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.874643 \n",
            "\n",
            "Epoch 14612\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.642444 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.745821 \n",
            "\n",
            "Epoch 14613\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.701512 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.833954 \n",
            "\n",
            "Epoch 14614\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.683540 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.785811 \n",
            "\n",
            "Epoch 14615\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.611472 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.893531 \n",
            "\n",
            "Epoch 14616\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.624013 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.781126 \n",
            "\n",
            "Epoch 14617\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.676005 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.856602 \n",
            "\n",
            "Epoch 14618\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.609158 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.741402 \n",
            "\n",
            "Epoch 14619\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.712784 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.744626 \n",
            "\n",
            "Epoch 14620\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.682177 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.811695 \n",
            "\n",
            "Epoch 14621\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.686588 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.860749 \n",
            "\n",
            "Epoch 14622\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.623166 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.797277 \n",
            "\n",
            "Epoch 14623\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.1%, Avg loss: 1.654434 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.779207 \n",
            "\n",
            "Epoch 14624\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.649828 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.798380 \n",
            "\n",
            "Epoch 14625\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.682274 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.915550 \n",
            "\n",
            "Epoch 14626\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.719700 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.800944 \n",
            "\n",
            "Epoch 14627\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.708885 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 1.877897 \n",
            "\n",
            "Epoch 14628\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.682235 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.828810 \n",
            "\n",
            "Epoch 14629\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.653813 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.886066 \n",
            "\n",
            "Epoch 14630\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.671823 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.852325 \n",
            "\n",
            "Epoch 14631\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.706550 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.816899 \n",
            "\n",
            "Epoch 14632\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.691954 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.856759 \n",
            "\n",
            "Epoch 14633\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.670018 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.777077 \n",
            "\n",
            "Epoch 14634\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.695053 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.859479 \n",
            "\n",
            "Epoch 14635\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.643772 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.782559 \n",
            "\n",
            "Epoch 14636\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.613951 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.867187 \n",
            "\n",
            "Epoch 14637\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.692740 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.887260 \n",
            "\n",
            "Epoch 14638\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.689981 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.778025 \n",
            "\n",
            "Epoch 14639\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.670757 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.751734 \n",
            "\n",
            "Epoch 14640\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.669774 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.750460 \n",
            "\n",
            "Epoch 14641\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.661495 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.812133 \n",
            "\n",
            "Epoch 14642\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.653055 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.823635 \n",
            "\n",
            "Epoch 14643\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.676547 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.903475 \n",
            "\n",
            "Epoch 14644\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.638450 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.883145 \n",
            "\n",
            "Epoch 14645\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.654968 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.788856 \n",
            "\n",
            "Epoch 14646\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.668667 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.767780 \n",
            "\n",
            "Epoch 14647\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.646523 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.881188 \n",
            "\n",
            "Epoch 14648\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.696467 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.773619 \n",
            "\n",
            "Epoch 14649\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.646327 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.901413 \n",
            "\n",
            "Epoch 14650\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.705944 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.742408 \n",
            "\n",
            "Epoch 14651\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.633935 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.729965 \n",
            "\n",
            "Epoch 14652\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.691627 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.812044 \n",
            "\n",
            "Epoch 14653\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.657003 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.820502 \n",
            "\n",
            "Epoch 14654\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.735571 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.835584 \n",
            "\n",
            "Epoch 14655\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.656658 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.766907 \n",
            "\n",
            "Epoch 14656\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.691544 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.797779 \n",
            "\n",
            "Epoch 14657\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.659661 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.881398 \n",
            "\n",
            "Epoch 14658\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.622520 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.832978 \n",
            "\n",
            "Epoch 14659\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.645502 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.791201 \n",
            "\n",
            "Epoch 14660\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.676534 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.864125 \n",
            "\n",
            "Epoch 14661\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.691711 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.800512 \n",
            "\n",
            "Epoch 14662\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.659108 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.749212 \n",
            "\n",
            "Epoch 14663\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.650436 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.816443 \n",
            "\n",
            "Epoch 14664\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.656639 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.838665 \n",
            "\n",
            "Epoch 14665\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.671784 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.836128 \n",
            "\n",
            "Epoch 14666\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.651976 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.861978 \n",
            "\n",
            "Epoch 14667\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.682196 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.856017 \n",
            "\n",
            "Epoch 14668\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.706966 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.804843 \n",
            "\n",
            "Epoch 14669\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.649404 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.818352 \n",
            "\n",
            "Epoch 14670\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.657886 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.806093 \n",
            "\n",
            "Epoch 14671\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.652496 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.744887 \n",
            "\n",
            "Epoch 14672\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.635543 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.839132 \n",
            "\n",
            "Epoch 14673\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.1%, Avg loss: 1.639856 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.804761 \n",
            "\n",
            "Epoch 14674\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.650455 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 1.727898 \n",
            "\n",
            "Epoch 14675\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.661950 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.777314 \n",
            "\n",
            "Epoch 14676\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.666293 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.894693 \n",
            "\n",
            "Epoch 14677\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.606658 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.791805 \n",
            "\n",
            "Epoch 14678\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.616480 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.780777 \n",
            "\n",
            "Epoch 14679\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.687613 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.808558 \n",
            "\n",
            "Epoch 14680\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.640672 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.821483 \n",
            "\n",
            "Epoch 14681\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.658019 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.727577 \n",
            "\n",
            "Epoch 14682\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.651752 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.888614 \n",
            "\n",
            "Epoch 14683\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.621805 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.840087 \n",
            "\n",
            "Epoch 14684\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.678556 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.804524 \n",
            "\n",
            "Epoch 14685\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.599793 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.800264 \n",
            "\n",
            "Epoch 14686\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.618456 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.764521 \n",
            "\n",
            "Epoch 14687\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.660846 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.746654 \n",
            "\n",
            "Epoch 14688\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.676804 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.781684 \n",
            "\n",
            "Epoch 14689\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.658205 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.715661 \n",
            "\n",
            "Epoch 14690\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.662124 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.918027 \n",
            "\n",
            "Epoch 14691\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.618763 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.730886 \n",
            "\n",
            "Epoch 14692\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.708457 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.829281 \n",
            "\n",
            "Epoch 14693\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.700275 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.765414 \n",
            "\n",
            "Epoch 14694\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.654700 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.855556 \n",
            "\n",
            "Epoch 14695\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.634819 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.763279 \n",
            "\n",
            "Epoch 14696\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.696550 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.854341 \n",
            "\n",
            "Epoch 14697\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.629371 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.778546 \n",
            "\n",
            "Epoch 14698\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.665387 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.742465 \n",
            "\n",
            "Epoch 14699\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.647398 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.824785 \n",
            "\n",
            "Epoch 14700\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.663600 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.722350 \n",
            "\n",
            "Epoch 14701\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.622864 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.830078 \n",
            "\n",
            "Epoch 14702\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.654348 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.846480 \n",
            "\n",
            "Epoch 14703\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.690693 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.879104 \n",
            "\n",
            "Epoch 14704\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.733641 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.777095 \n",
            "\n",
            "Epoch 14705\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.640332 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.801069 \n",
            "\n",
            "Epoch 14706\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.636411 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.959528 \n",
            "\n",
            "Epoch 14707\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.662163 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.831575 \n",
            "\n",
            "Epoch 14708\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.6%, Avg loss: 1.615888 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.893648 \n",
            "\n",
            "Epoch 14709\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.669282 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.791742 \n",
            "\n",
            "Epoch 14710\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.668229 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.762126 \n",
            "\n",
            "Epoch 14711\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.667014 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.823902 \n",
            "\n",
            "Epoch 14712\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.679463 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.796511 \n",
            "\n",
            "Epoch 14713\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.633107 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.843266 \n",
            "\n",
            "Epoch 14714\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.653390 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.741004 \n",
            "\n",
            "Epoch 14715\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.585987 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.793588 \n",
            "\n",
            "Epoch 14716\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.688864 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.799466 \n",
            "\n",
            "Epoch 14717\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.651313 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.742180 \n",
            "\n",
            "Epoch 14718\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.629921 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.739115 \n",
            "\n",
            "Epoch 14719\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.644356 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.827322 \n",
            "\n",
            "Epoch 14720\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.678053 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.885923 \n",
            "\n",
            "Epoch 14721\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.638337 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.766353 \n",
            "\n",
            "Epoch 14722\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.633264 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.795069 \n",
            "\n",
            "Epoch 14723\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.620353 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.860957 \n",
            "\n",
            "Epoch 14724\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.697313 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.843509 \n",
            "\n",
            "Epoch 14725\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.599799 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.798196 \n",
            "\n",
            "Epoch 14726\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.647386 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.746081 \n",
            "\n",
            "Epoch 14727\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.641239 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.947928 \n",
            "\n",
            "Epoch 14728\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.607317 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.787036 \n",
            "\n",
            "Epoch 14729\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.704786 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.837428 \n",
            "\n",
            "Epoch 14730\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.648117 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.881536 \n",
            "\n",
            "Epoch 14731\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.629749 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.750291 \n",
            "\n",
            "Epoch 14732\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.712194 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.987147 \n",
            "\n",
            "Epoch 14733\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.647833 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.780351 \n",
            "\n",
            "Epoch 14734\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.706533 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.761780 \n",
            "\n",
            "Epoch 14735\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.630376 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.809810 \n",
            "\n",
            "Epoch 14736\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.638355 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.844949 \n",
            "\n",
            "Epoch 14737\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.678442 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.787658 \n",
            "\n",
            "Epoch 14738\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.605127 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.727861 \n",
            "\n",
            "Epoch 14739\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.606374 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.824907 \n",
            "\n",
            "Epoch 14740\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.676217 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.739115 \n",
            "\n",
            "Epoch 14741\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.657475 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.790851 \n",
            "\n",
            "Epoch 14742\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.672578 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.746002 \n",
            "\n",
            "Epoch 14743\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.655714 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.884737 \n",
            "\n",
            "Epoch 14744\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.648350 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.801056 \n",
            "\n",
            "Epoch 14745\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.657290 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.703941 \n",
            "\n",
            "Epoch 14746\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.661267 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.750483 \n",
            "\n",
            "Epoch 14747\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.649970 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.899656 \n",
            "\n",
            "Epoch 14748\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.647757 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.875034 \n",
            "\n",
            "Epoch 14749\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.628282 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.746378 \n",
            "\n",
            "Epoch 14750\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.644379 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.766028 \n",
            "\n",
            "Epoch 14751\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.627316 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.785426 \n",
            "\n",
            "Epoch 14752\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.642142 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.833033 \n",
            "\n",
            "Epoch 14753\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.657154 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.832426 \n",
            "\n",
            "Epoch 14754\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.603785 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.826450 \n",
            "\n",
            "Epoch 14755\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.621235 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.823477 \n",
            "\n",
            "Epoch 14756\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 31.9%, Avg loss: 1.708392 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.719452 \n",
            "\n",
            "Epoch 14757\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.680689 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.801143 \n",
            "\n",
            "Epoch 14758\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.681622 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.805708 \n",
            "\n",
            "Epoch 14759\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.615506 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.797871 \n",
            "\n",
            "Epoch 14760\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.627155 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.885384 \n",
            "\n",
            "Epoch 14761\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.649270 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.755164 \n",
            "\n",
            "Epoch 14762\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.662548 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.840600 \n",
            "\n",
            "Epoch 14763\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.658295 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.830575 \n",
            "\n",
            "Epoch 14764\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.691355 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.813616 \n",
            "\n",
            "Epoch 14765\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.652207 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.810622 \n",
            "\n",
            "Epoch 14766\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.587709 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.813267 \n",
            "\n",
            "Epoch 14767\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.1%, Avg loss: 1.621305 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.828998 \n",
            "\n",
            "Epoch 14768\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.665181 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.828862 \n",
            "\n",
            "Epoch 14769\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.652999 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.707195 \n",
            "\n",
            "Epoch 14770\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.628904 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.803457 \n",
            "\n",
            "Epoch 14771\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.632837 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.721500 \n",
            "\n",
            "Epoch 14772\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.619888 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.819351 \n",
            "\n",
            "Epoch 14773\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.649513 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.870217 \n",
            "\n",
            "Epoch 14774\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.617694 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.785197 \n",
            "\n",
            "Epoch 14775\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.660868 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.886120 \n",
            "\n",
            "Epoch 14776\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.636060 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.830799 \n",
            "\n",
            "Epoch 14777\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.625984 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.973988 \n",
            "\n",
            "Epoch 14778\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.3%, Avg loss: 1.633336 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.713949 \n",
            "\n",
            "Epoch 14779\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.610913 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.800885 \n",
            "\n",
            "Epoch 14780\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.657900 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.900692 \n",
            "\n",
            "Epoch 14781\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.632797 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.744458 \n",
            "\n",
            "Epoch 14782\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.645783 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.798096 \n",
            "\n",
            "Epoch 14783\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.605232 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.796676 \n",
            "\n",
            "Epoch 14784\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.635050 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.926182 \n",
            "\n",
            "Epoch 14785\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.676500 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.685601 \n",
            "\n",
            "Epoch 14786\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.625764 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.779288 \n",
            "\n",
            "Epoch 14787\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.673036 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.795700 \n",
            "\n",
            "Epoch 14788\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.666382 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.732924 \n",
            "\n",
            "Epoch 14789\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.654895 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.722710 \n",
            "\n",
            "Epoch 14790\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.612300 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.728442 \n",
            "\n",
            "Epoch 14791\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.7%, Avg loss: 1.600814 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.799309 \n",
            "\n",
            "Epoch 14792\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.642601 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.760792 \n",
            "\n",
            "Epoch 14793\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.650843 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.803380 \n",
            "\n",
            "Epoch 14794\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.634761 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.807991 \n",
            "\n",
            "Epoch 14795\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.640988 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.857829 \n",
            "\n",
            "Epoch 14796\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.673996 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.806727 \n",
            "\n",
            "Epoch 14797\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.666197 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.831587 \n",
            "\n",
            "Epoch 14798\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.629733 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.638211 \n",
            "\n",
            "Epoch 14799\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.619447 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.827084 \n",
            "\n",
            "Epoch 14800\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.669336 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.829116 \n",
            "\n",
            "Epoch 14801\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.632938 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.791116 \n",
            "\n",
            "Epoch 14802\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.632724 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.771136 \n",
            "\n",
            "Epoch 14803\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.6%, Avg loss: 1.583236 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.857436 \n",
            "\n",
            "Epoch 14804\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.632277 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.774420 \n",
            "\n",
            "Epoch 14805\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.643917 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.724855 \n",
            "\n",
            "Epoch 14806\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.646602 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.826239 \n",
            "\n",
            "Epoch 14807\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.649126 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.927895 \n",
            "\n",
            "Epoch 14808\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.600917 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.747949 \n",
            "\n",
            "Epoch 14809\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.643084 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.869017 \n",
            "\n",
            "Epoch 14810\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.1%, Avg loss: 1.624293 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.743384 \n",
            "\n",
            "Epoch 14811\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.653502 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.816670 \n",
            "\n",
            "Epoch 14812\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.645849 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.822570 \n",
            "\n",
            "Epoch 14813\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.688241 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.826152 \n",
            "\n",
            "Epoch 14814\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.593072 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.720536 \n",
            "\n",
            "Epoch 14815\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.642952 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.738684 \n",
            "\n",
            "Epoch 14816\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.675647 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.777263 \n",
            "\n",
            "Epoch 14817\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.650738 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 1.837970 \n",
            "\n",
            "Epoch 14818\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.666868 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.844201 \n",
            "\n",
            "Epoch 14819\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.640926 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.751309 \n",
            "\n",
            "Epoch 14820\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.686857 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.835352 \n",
            "\n",
            "Epoch 14821\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.787518 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.957033 \n",
            "\n",
            "Epoch 14822\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.625120 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.787815 \n",
            "\n",
            "Epoch 14823\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.644774 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.900418 \n",
            "\n",
            "Epoch 14824\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.676349 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.840385 \n",
            "\n",
            "Epoch 14825\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.628972 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.890570 \n",
            "\n",
            "Epoch 14826\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.688993 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.818935 \n",
            "\n",
            "Epoch 14827\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.658671 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.959865 \n",
            "\n",
            "Epoch 14828\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.704159 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.834905 \n",
            "\n",
            "Epoch 14829\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.587683 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.820119 \n",
            "\n",
            "Epoch 14830\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.619432 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.738322 \n",
            "\n",
            "Epoch 14831\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.682184 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.951735 \n",
            "\n",
            "Epoch 14832\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.648133 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.754965 \n",
            "\n",
            "Epoch 14833\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.661224 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.738281 \n",
            "\n",
            "Epoch 14834\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.663552 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.853298 \n",
            "\n",
            "Epoch 14835\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.629503 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.771899 \n",
            "\n",
            "Epoch 14836\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.636379 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.719069 \n",
            "\n",
            "Epoch 14837\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.659546 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.785120 \n",
            "\n",
            "Epoch 14838\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.615561 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.787917 \n",
            "\n",
            "Epoch 14839\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.620792 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.842058 \n",
            "\n",
            "Epoch 14840\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.4%, Avg loss: 1.634838 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.795598 \n",
            "\n",
            "Epoch 14841\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.644942 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.821065 \n",
            "\n",
            "Epoch 14842\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.609810 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.809328 \n",
            "\n",
            "Epoch 14843\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.631056 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.767516 \n",
            "\n",
            "Epoch 14844\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.1%, Avg loss: 1.645064 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.799913 \n",
            "\n",
            "Epoch 14845\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.633997 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.801609 \n",
            "\n",
            "Epoch 14846\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.664435 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.803755 \n",
            "\n",
            "Epoch 14847\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.674037 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.942263 \n",
            "\n",
            "Epoch 14848\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.690215 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.776761 \n",
            "\n",
            "Epoch 14849\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.9%, Avg loss: 1.681368 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.784630 \n",
            "\n",
            "Epoch 14850\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.1%, Avg loss: 1.667144 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.835149 \n",
            "\n",
            "Epoch 14851\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.615183 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.831771 \n",
            "\n",
            "Epoch 14852\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.662832 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.857871 \n",
            "\n",
            "Epoch 14853\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.647235 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.723732 \n",
            "\n",
            "Epoch 14854\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.683203 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.682148 \n",
            "\n",
            "Epoch 14855\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.4%, Avg loss: 1.658906 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 1.898566 \n",
            "\n",
            "Epoch 14856\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.635555 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.849493 \n",
            "\n",
            "Epoch 14857\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.655474 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.765487 \n",
            "\n",
            "Epoch 14858\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.619283 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.863025 \n",
            "\n",
            "Epoch 14859\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.592488 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.772306 \n",
            "\n",
            "Epoch 14860\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.3%, Avg loss: 1.622400 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.802509 \n",
            "\n",
            "Epoch 14861\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.630533 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.126509 \n",
            "\n",
            "Epoch 14862\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.668031 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.874494 \n",
            "\n",
            "Epoch 14863\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.629028 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.855853 \n",
            "\n",
            "Epoch 14864\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.670664 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.770980 \n",
            "\n",
            "Epoch 14865\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.615071 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.865660 \n",
            "\n",
            "Epoch 14866\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.616804 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.837972 \n",
            "\n",
            "Epoch 14867\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.663289 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.852506 \n",
            "\n",
            "Epoch 14868\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.670104 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.869226 \n",
            "\n",
            "Epoch 14869\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.628593 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.897366 \n",
            "\n",
            "Epoch 14870\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.631530 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.808068 \n",
            "\n",
            "Epoch 14871\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.662365 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.580048 \n",
            "\n",
            "Epoch 14872\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.646507 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.850239 \n",
            "\n",
            "Epoch 14873\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.666730 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.891998 \n",
            "\n",
            "Epoch 14874\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.648116 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.871974 \n",
            "\n",
            "Epoch 14875\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.614371 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.893787 \n",
            "\n",
            "Epoch 14876\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.600561 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.761427 \n",
            "\n",
            "Epoch 14877\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.623304 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.832086 \n",
            "\n",
            "Epoch 14878\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.659086 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.743856 \n",
            "\n",
            "Epoch 14879\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.612434 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.813917 \n",
            "\n",
            "Epoch 14880\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.607805 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.796303 \n",
            "\n",
            "Epoch 14881\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.597270 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.817714 \n",
            "\n",
            "Epoch 14882\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.638085 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.794826 \n",
            "\n",
            "Epoch 14883\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.683602 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.833514 \n",
            "\n",
            "Epoch 14884\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.635747 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 28.7%, Avg loss: 1.881789 \n",
            "\n",
            "Epoch 14885\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.658235 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.878333 \n",
            "\n",
            "Epoch 14886\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.628934 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.799545 \n",
            "\n",
            "Epoch 14887\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.667057 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.810880 \n",
            "\n",
            "Epoch 14888\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.637204 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.731448 \n",
            "\n",
            "Epoch 14889\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.644754 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.845464 \n",
            "\n",
            "Epoch 14890\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.666805 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.886386 \n",
            "\n",
            "Epoch 14891\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.680246 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.764644 \n",
            "\n",
            "Epoch 14892\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.674952 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.825984 \n",
            "\n",
            "Epoch 14893\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.659493 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.757682 \n",
            "\n",
            "Epoch 14894\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.607026 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.763850 \n",
            "\n",
            "Epoch 14895\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.678500 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.160089 \n",
            "\n",
            "Epoch 14896\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.631132 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.917436 \n",
            "\n",
            "Epoch 14897\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.627184 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.834047 \n",
            "\n",
            "Epoch 14898\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.659288 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.808397 \n",
            "\n",
            "Epoch 14899\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.695425 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.748658 \n",
            "\n",
            "Epoch 14900\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.679318 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.920174 \n",
            "\n",
            "Epoch 14901\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.682599 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.750787 \n",
            "\n",
            "Epoch 14902\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.596515 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.843396 \n",
            "\n",
            "Epoch 14903\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.630208 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.769741 \n",
            "\n",
            "Epoch 14904\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.646657 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.820925 \n",
            "\n",
            "Epoch 14905\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.624134 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.990970 \n",
            "\n",
            "Epoch 14906\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.735090 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 1.808732 \n",
            "\n",
            "Epoch 14907\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.628617 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.787850 \n",
            "\n",
            "Epoch 14908\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.623379 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.803724 \n",
            "\n",
            "Epoch 14909\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.700188 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.847069 \n",
            "\n",
            "Epoch 14910\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.620992 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.810965 \n",
            "\n",
            "Epoch 14911\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.641595 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.792274 \n",
            "\n",
            "Epoch 14912\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.651213 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 1.795543 \n",
            "\n",
            "Epoch 14913\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.640521 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.844362 \n",
            "\n",
            "Epoch 14914\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.627373 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.758498 \n",
            "\n",
            "Epoch 14915\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.639655 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.790945 \n",
            "\n",
            "Epoch 14916\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.626725 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.785592 \n",
            "\n",
            "Epoch 14917\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.632061 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.865099 \n",
            "\n",
            "Epoch 14918\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.658733 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.814303 \n",
            "\n",
            "Epoch 14919\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.625490 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.844756 \n",
            "\n",
            "Epoch 14920\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.694481 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.288597 \n",
            "\n",
            "Epoch 14921\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.630078 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.822528 \n",
            "\n",
            "Epoch 14922\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.648880 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.766075 \n",
            "\n",
            "Epoch 14923\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.645705 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.812546 \n",
            "\n",
            "Epoch 14924\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.675408 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.856625 \n",
            "\n",
            "Epoch 14925\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.648804 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.822138 \n",
            "\n",
            "Epoch 14926\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.621444 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.953458 \n",
            "\n",
            "Epoch 14927\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.632284 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.835824 \n",
            "\n",
            "Epoch 14928\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.6%, Avg loss: 1.648839 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.820666 \n",
            "\n",
            "Epoch 14929\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 39.1%, Avg loss: 1.602913 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.865242 \n",
            "\n",
            "Epoch 14930\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.725721 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.872592 \n",
            "\n",
            "Epoch 14931\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.625785 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.913795 \n",
            "\n",
            "Epoch 14932\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.630603 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.856192 \n",
            "\n",
            "Epoch 14933\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.702423 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.854274 \n",
            "\n",
            "Epoch 14934\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.618177 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.734586 \n",
            "\n",
            "Epoch 14935\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.649069 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.749341 \n",
            "\n",
            "Epoch 14936\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.613516 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.813116 \n",
            "\n",
            "Epoch 14937\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.4%, Avg loss: 1.626551 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 1.834864 \n",
            "\n",
            "Epoch 14938\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.646970 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 1.830627 \n",
            "\n",
            "Epoch 14939\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.635019 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.797064 \n",
            "\n",
            "Epoch 14940\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.631241 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.765921 \n",
            "\n",
            "Epoch 14941\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.647532 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.770356 \n",
            "\n",
            "Epoch 14942\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.634788 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.708126 \n",
            "\n",
            "Epoch 14943\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.647822 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.850061 \n",
            "\n",
            "Epoch 14944\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.6%, Avg loss: 1.717338 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.846598 \n",
            "\n",
            "Epoch 14945\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.623339 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.829349 \n",
            "\n",
            "Epoch 14946\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.637991 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 1.841662 \n",
            "\n",
            "Epoch 14947\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.654564 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.823269 \n",
            "\n",
            "Epoch 14948\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.691447 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.887804 \n",
            "\n",
            "Epoch 14949\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.682404 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.887549 \n",
            "\n",
            "Epoch 14950\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.664397 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.873431 \n",
            "\n",
            "Epoch 14951\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.661275 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.829632 \n",
            "\n",
            "Epoch 14952\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.633401 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 1.794697 \n",
            "\n",
            "Epoch 14953\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.647000 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.761850 \n",
            "\n",
            "Epoch 14954\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.618252 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.855183 \n",
            "\n",
            "Epoch 14955\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.689173 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.3%, Avg loss: 1.776905 \n",
            "\n",
            "Epoch 14956\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.637425 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.744586 \n",
            "\n",
            "Epoch 14957\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.611164 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.814699 \n",
            "\n",
            "Epoch 14958\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.640041 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.771492 \n",
            "\n",
            "Epoch 14959\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.667341 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.735968 \n",
            "\n",
            "Epoch 14960\n",
            "-------------------------------\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.651935 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 1.758355 \n",
            "\n",
            "Epoch 14961\n",
            "-------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Initialize the loss function\n",
        "#  nn.CrossEntropyLoss() encapsulates nn.LogSoftmax and nn.NLLLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Parameter adjustment protocol\n",
        "# He: always start w/ Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For plotting results\n",
        "animator = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0.0, 1.0],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "binMaskSizeLs = []\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # for x,y in train_dataloader:\n",
        "    #   print(x,y)\n",
        "    #   break\n",
        "    tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, loss_fn, optimizer, epoch=t+1, mod=mod)\n",
        "    train_metrics = (tr_loss, __)\n",
        "\n",
        "    # Disaggregate data -- save models; note epoch\n",
        "    # if t > 2000 and tr_loss > prevLoss * 10 :\n",
        "    #   print(f'train_loop(): epoch {t} -- loss jumped from {prevLoss:.3} to {tr_loss:.3}')\n",
        "    #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{t}.pt')\n",
        "    # prevLoss = tr_loss\n",
        "\n",
        "    test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n",
        "    test_acc, confusion_matrix = test_loop(test_dataloader, model, loss_fn, out_dim=output_dim)\n",
        "    # animator\n",
        "    animator.add(t + 1, train_metrics + (test_acc,))\n",
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/testing_display_of_sensor_grid.pt')\n",
        "# Not sure the following block is necessary\n",
        "# train_loss, train_acc = train_metrics\n",
        "# assert train_loss < 0.5, train_loss\n",
        "# assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
        "# assert test_acc <= 1 and test_acc > 0.7, test_acc\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gTMxTXWcdMz"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "animator.display_plt()\n",
        "# Automate improved filename description.\n",
        "animator.fig.savefig('loss.png', bbox_inches='tight')\n",
        "# plt.savefig('loss.png', bbox_inches='tight')   # Less specific. Targets active figure.\n",
        "\n",
        "# pipe labels are located in data to csv notebook\n",
        "# predictions = [f'{i}' for i in range(output_dim)]\n",
        "predictions = decode_labels()\n",
        "# labels = range(output_dim)\n",
        "labels = decode_labels()\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "# fig.set_facecolor('#7d7f7c')\n",
        "im = ax.imshow(confusion_matrix)\n",
        "ax.set_xticks(np.arange(len(predictions)))\n",
        "ax.set_yticks(np.arange(len(labels)))\n",
        "ax.set_xticklabels(predictions)\n",
        "ax.set_yticklabels(labels)\n",
        "\n",
        "# Set-up for white grid lines on minor ticks. Creates spacing effect.\n",
        "ax.set_xticks(np.arange(len(predictions)+1) - 0.5, minor=True)\n",
        "ax.set_yticks(np.arange(len(labels)+1) - 0.5, minor=True)\n",
        "# Print white grid to space out the squares.\n",
        "ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
        "# Remove spines for clarity.\n",
        "for k, v in ax.spines.items() :\n",
        "  v.set_visible(False)\n",
        "# ax.spines['top'].set_visible(False)   # Can't slice a dictionary.\n",
        "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "# Horizontal labeling displays on top\n",
        "ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "# Rotate tick labels and set alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=-45, ha='right', rotation_mode='anchor')\n",
        "plt.xlabel(f'Predictions -- {ts_size}')\n",
        "# Move the x labels to the top\n",
        "ax.xaxis.set_label_position('top')\n",
        "plt.ylabel('Labels')\n",
        "# Annotate matrix with values by looping over data dimensions\n",
        "for i in range(len(labels)) :\n",
        "  for j in range(len(predictions)) :\n",
        "    text = ax.text(j, i, confusion_matrix[i, j].item(),\n",
        "                   ha='center', va='center', color='white')\n",
        "\n",
        "ax.set_title(f'Confusion Matrix -- Epoch {t+1}')\n",
        "fig.tight_layout()\n",
        "# Save to file that is replaced on every run.\n",
        "fig.savefig('conf.png')\n",
        "# plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcjWmtFdRdsZ"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/midTrainingModel.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W6b5qaI8ZVj"
      },
      "source": [
        "####Sanity Check: Pass a sample to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp6zYEZN8mx5"
      },
      "outputs": [],
      "source": [
        "def predict_ch3(net, sample, samp_idx=0):\n",
        "    \"\"\"Predict labels (redefined from d2l Chapter 3.6.7).\"\"\"\n",
        "    print('Model Evaluation')\n",
        "    X, y = sample[samp_idx]\n",
        "    X = X.reshape([1,-1])\n",
        "    preds = net(X).argmax(axis=1)\n",
        "    print(f'Scenario {samp_idx} (pred={preds.item()}, label={y.item()})')\n",
        "\n",
        "predict_ch3(model, tr_dataset, samp_idx=28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv4iOGftxLZi"
      },
      "outputs": [],
      "source": [
        "# Test trained model on time stamps it hasn't seen before\n",
        "# tmstp = 168\n",
        "# X, y = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n",
        "# ts_dataset = TensorDataset(X, y)\n",
        "\n",
        "# test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n",
        "# test_acc = test_loop(test_dataloader, model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5xwqcQyNoBV"
      },
      "source": [
        "####Scratch Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCehwyn6imcp"
      },
      "outputs": [],
      "source": [
        "print(0.991 > 0.99)\n",
        "x = torch.rand(5)\n",
        "print(x)\n",
        "y = torch.where(x > 0.6, x, torch.tensor(0.))\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGxSZV4cDLu7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "x = float('nan')\n",
        "print(x)\n",
        "print(not math.isnan(x))\n",
        "print(1 + 0.0 * x)\n",
        "print(torch.log(torch.tensor(0.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f8VELnBsjNg"
      },
      "outputs": [],
      "source": [
        "x_tup = ([2], [3], [4])\n",
        "# x_tup = (2, 3, 4)\n",
        "x_tup[1][0] -= x_tup[0][0]\n",
        "# x_tup[1] -= x_tup[0]\n",
        "# print(x_tup[1] - x_tup[0])\n",
        "print(x_tup[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKcdDEqt7LVH"
      },
      "outputs": [],
      "source": [
        "t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n",
        "print(t)\n",
        "t1 = torch.rand(10, generator=torch.Generator().manual_seed(11))\n",
        "print(t1)\n",
        "print(t[t1 > 0.5])   # returns a tensor containing only those elems for which test returns true.\n",
        "t[t1 > 0.5] = 0   # Assigns zero to only those elems for which the test returns true.\n",
        "print(t)\n",
        "print(t[t1 > 0.5])\n",
        "print(t[0])   # Returns a zero dim tensor. (num)\n",
        "print(t[0:1])   # Returns a 1-dim tensor. ([num])\n",
        "print(t[0:1].new(1))\n",
        "print(t[0:1].new_empty(1).uniform_())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9aks4N-SHt5"
      },
      "outputs": [],
      "source": [
        "t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n",
        "print(' t:', t)\n",
        "idxs = torch.randperm(5, generator=torch.Generator().manual_seed(10))\n",
        "print(idxs)\n",
        "tn = t[idxs]\n",
        "print(' t:', t)\n",
        "print('tn:', tn)\n",
        "print(' t:', t[2])\n",
        "print('tn:', tn[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpjdHiXWtwOl"
      },
      "outputs": [],
      "source": [
        "y_hat = torch.arange(20).reshape([2, -1])\n",
        "print(y_hat)\n",
        "len(y_hat)\n",
        "print(y_hat.sum(1))\n",
        "print(y_hat.argmax(dim=0))\n",
        "print(y_hat.argmax(dim=1))\n",
        "data = [[1.0, 1.0], [1.0, 1.0]] * 2   # multiplies the number of elements (like if you had 2 apples and then multiplied them by 2; you now have four apples)\n",
        "print(data)\n",
        "# print(data / data)   # dividing a list is not defined\n",
        "(1,2) + (3,)   # cats the three elems\n",
        "len((1,2))   # tuples have __len__ defined\n",
        "[[] for _ in range(3)]\n",
        "rows = [[1,1]]\n",
        "print(rows)\n",
        "[rows.append(i) for i in [[3,3],[4,4]]]\n",
        "print(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPQBiMXCOahN"
      },
      "outputs": [],
      "source": [
        "# X_masked = None\n",
        "# masked_feats = torch.rand(15)\n",
        "# print(masked_feats)\n",
        "# mask = torch.randint(2, [15])\n",
        "# print(mask)\n",
        "# # Mask and masked features\n",
        "# # May want sensing_mask_rand() to process batches of samples\n",
        "# for i in range(5):\n",
        "#   temp = torch.cat((masked_feats, mask)).reshape([1,-1])\n",
        "#   print(temp)\n",
        "\n",
        "#   if X_masked is None:\n",
        "#     X_masked = temp   \n",
        "#     print(X_masked)\n",
        "#   else:  \n",
        "#     X_masked = torch.cat((X_masked, temp))\n",
        "#     print(X_masked)\n",
        "\n",
        "# for i in X_masked:\n",
        "#   print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4kiAKeKfBIp"
      },
      "outputs": [],
      "source": [
        "# size = [100,]\n",
        "# K = 20\n",
        "# tn = torch.zeros(size)\n",
        "# mask = torch.zeros(tn.size())\n",
        "# print(mask.size())\n",
        "# print(mask)\n",
        "# indices = torch.randint(len(tn), size=(K,))\n",
        "# print(indices)\n",
        "# for idx in indices:\n",
        "#   mask[idx] = 1\n",
        "# print(mask)\n",
        "\n",
        "# mask = torch.cuda.FloatTensor(3, 3).uniform_()\n",
        "# # tensor of floats\n",
        "# mask = torch.FloatTensor(3,3).uniform_()\n",
        "# print(mask)\n",
        "# # tensor of booleans (?? how ??)\n",
        "# mask = torch.FloatTensor(3,3).uniform_() > 0.8\n",
        "# print(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxvQSxfCAX_R"
      },
      "outputs": [],
      "source": [
        "# converting string labs to labels ranging from 0 -> num_of_classes (i.e. possible leak locations)\n",
        "#  what if not all of the possible leak locations are used?\n",
        "#  1) I can set the output dim to len of label_subset (easiser)\n",
        "#  2) I can force the set to be all the possible fixed pipe locations (coordinating this will be tricky)\n",
        "# labs = [1,2,2,3,1,4,4,3]\n",
        "# lab_dict = {}\n",
        "# encoded_labs = []\n",
        "# label_subset = set(labs)\n",
        "# print(label_subset)\n",
        "# print(type(label_subset))\n",
        "# print(len(label_subset))\n",
        "# for i, key in enumerate(label_subset):\n",
        "#   lab_dict[key] = i\n",
        "# print(lab_dict)\n",
        "# for key in labs:\n",
        "#   encoded_labs.append(lab_dict[key])\n",
        "# print(encoded_labs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snpJEOWKoc5Y"
      },
      "outputs": [],
      "source": [
        "# Reshaping practice\n",
        "# base_file = 'simdata/_base_/node_demand.csv'\n",
        "# data_file = base_file\n",
        "# data = pd.read_csv(data_file)\n",
        "# data_tn = torch.tensor(data.values, dtype=torch.float32)\n",
        "# data_tn[:,1:].reshape([1,-1])\n",
        "\n",
        "# data_tn = torch.arange(20).reshape([4,5])\n",
        "# print(data_tn)\n",
        "# data_tn = data_tn[:,1:].reshape([1,-1])\n",
        "# print(data_tn)\n",
        "# data_tn1 = torch.arange(20).reshape([4,5])\n",
        "# print(data_tn1)\n",
        "# data_tn1 = data_tn1[:,1:].reshape([1, -1])\n",
        "# print(data_tn1)\n",
        "# torch.cat((data_tn1, data_tn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DGW06k6V-qI"
      },
      "outputs": [],
      "source": [
        "# Extracting an intelligible answer from the model\n",
        "# x = torch.arange(16, dtype=torch.float32).reshape((4,4))\n",
        "# print(x)\n",
        "# print(x.sum(axis=0))\n",
        "# print(x.sum(axis=[0,1]))\n",
        "# mean = x.sum() / x.numel()\n",
        "# print(mean)\n",
        "# # notice we keep all dims (tensor of a tensor ie. two brackets) vs above we lost one (just a tensor)\n",
        "# print(x.sum(dim=0, keepdim=True))\n",
        "\n",
        "# y = torch.tensor([3,3,3,3])\n",
        "# # x.argmax(1).type(y.dtype) == y\n",
        "# correct = 0\n",
        "# correct += (x.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n",
        "# correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kndCfp0DOku8"
      },
      "outputs": [],
      "source": [
        "# Handy timer class\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3m7r6ps_rDM"
      },
      "outputs": [],
      "source": [
        "# Target transform\n",
        "from torchvision.transforms import Lambda\n",
        "\n",
        "train_size = 700\n",
        "# target_transform = Lambda(lambda y: torch.zeros(\n",
        "#     (train_size, output_dim), dtype=torch.float).scatter_(\n",
        "#         dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))\n",
        "\n",
        "# one-hot classification label vector\n",
        "target_transform = Lambda(lambda y: torch.scatter_(\n",
        "        dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4Epm5SJkrzLx",
        "VQzqACaz6D71",
        "BTY1vNInM3RI",
        "ydc9rlm86Hmt",
        "3ocO0Om3ofeX",
        "RVZsPo1p2RoP",
        "TmFOoA-k258k",
        "5wCj-uKc561c",
        "7KkFT5x93F5H",
        "j_Zwmb6sYpbm",
        "qysp3xV5W2dg",
        "4W6b5qaI8ZVj",
        "k5xwqcQyNoBV"
      ],
      "name": "bernoulli2_20pipes.ipynb",
      "provenance": [],
      "mount_file_id": "1YeSJUnijlqIt0y5eDayQvwNGRwSKSkYZ",
      "authorship_tag": "ABX9TyP0w2rS9QOBBwxKdZ8f2kVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}