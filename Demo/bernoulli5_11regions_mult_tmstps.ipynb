{"cells":[{"cell_type":"markdown","metadata":{"id":"j1q0Q-CJEFf5"},"source":["####Notes and Imports"]},{"cell_type":"markdown","metadata":{"id":"4Epm5SJkrzLx"},"source":["#####Notes\n","- Using SimData_to_csv notebook to create dataset csv\n","- I may want a labels map (dict) for pipe labels like the one here\n","  - https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterating-and-visualizing-the-dataset\n","- Unclear what a one-hot encoded tensor might be used for\n","  - used here as a target label transformation\n","    - https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#lambda-transforms\n","\n","- How do I handle two labels?\n","- Is normalization required?\n"," - ans: desirable when features have different ranges. https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=The%20goal%20of%20normalization%20is,when%20features%20have%20different%20ranges.\n","- Complete a panda tutorial\n","- He: Classifer tutorial; CIFAR10 dataset (3 channel images)\n"," - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","- Colab Pro\n"," - One important caveat to remember while using Colab is that the files you upload to it wonâ€™t be available forever. Colab is a temporary environment with an idle timeout of 90 minutes and an absolute timeout of 12 hours. This means that the runtime will disconnect if it has remained idle for 90 minutes, or if it has been in use for 12 hours [longer for Colab Pro]. On disconnection, you lose all your variables, states, installed packages, and files and will be connected to an entirely new and clean environment on reconnecting. source:https://neptune.ai/blog/google-colab-dealing-with-files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZl-N7iuq70P"},"outputs":[],"source":["# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') \u003e= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)\n","\n","# from psutil import virtual_memory\n","# ram_gb = virtual_memory().total / 1e9\n","# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","# if ram_gb \u003c 20:\n","#   print('Not using a high-RAM runtime')\n","# else:\n","#   print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"VQzqACaz6D71"},"source":["####Model framework"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1070,"status":"ok","timestamp":1652893942368,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"YPV_XlfZFlI-","outputId":"a6f7fa55-7f08-4b9e-a6ae-d231a6cc39a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["%matplotlib inline\n","\n","import os\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","\n","import pandas as pd   # For loading csv file dataset\n","import random\n","\n","import math   # Used for nan checking (math.isnan())\n","import numpy as np   # Used w/ pipe_to_pipeIdx file and graphics.\n","import pickle   # Used for loading reg_dict (stored w/ training data csv's; constructed in graph_partition notebook)\n","\n","# Straight-Through Estimator\n","# https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html\n","\n","# Autograd\n","# https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n","\n","# Define model\n","#  -- when called, model returns a output_dim dimensional tensor\n","# def sparseProbMap(batch_probMap, sparsity) :\n","def sparseProbMap(probMap, sparsity) :\n","  \"\"\"Rescale probability map (batch_probMap) to obtain desired sparsity in\n","  measuremets that are turned on.\n","  sparsity = budget (int) / training sample length (meas)\n","    budget -- number of measurement to turn on/ sensor to deploy\n","  \"\"\"\n","  mean = torch.mean(probMap, dim=0, keepdim=True)\n","  scalar = sparsity / mean\n","  beta_scalar = (1 - sparsity) / (1 - mean)\n","  toggle = torch.le(scalar, 1).float()\n","  sparse_probMap = ( toggle * scalar * probMap\n","                      + (1 - toggle) * (1 - (1 - probMap) * beta_scalar) )\n","  # print(f'sparseProbMap(): samp_means {samp_means}')\n","  # print(f'sparseProbMap(): samp_means size {samp_means.size()}')\n","  # print(f'sparseProbMap(): samp_scalars {samp_scalars}')\n","  # print(f'sparseProbMap(): samp_beta_scalars {samp_beta_scalars}')\n","  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n","  return sparse_probMap\n","  \n","  # Alt approaches\n","  # torch method -- with prob_map repeat before this func call (i.e. batch_probMap)\n","  # Notice batch_probMap contains batch_size copies of one prob_map\n","  #  Might be able to make this even faster by running the calc once, then repeating after this call.\n","  # Tensor version -- might speed up training time.\n","  # samp_means = torch.mean(batch_probMap, dim=1, keepdim=True)\n","  # samp_scalars = sparsity / samp_means\n","  # samp_beta_scalars = (1 - sparsity) / (1 - samp_means)\n","  # toggles = torch.le(samp_scalars, 1).float()\n","  # sparse_probMap = ( toggles * samp_scalars * batch_probMap\n","  #                     + (1 - toggles) * (1 - (1 - batch_probMap) * samp_beta_scalars) )\n","\n","  # for samp_probMap in batch_probMap :\n","  #   mean_sampProbMap = torch.mean(samp_probMap)\n","  #   scalar_sampProbMap = sparsity / mean_sampProbMap\n","  #   beta_scalar = (1 - sparsity) / (1 - mean_sampProbMap) # Rename variable to something more descriptive\n","  #   # print(f'sparseProbMap(): samp_probMap {samp_probMap}')\n","  #   # print(f'sparseProbMap(): samp_probMap size {samp_probMap.size()}')\n","  #   # print(f'sparseProbMap(): mean_sampProbMap {mean_sampProbMap}')\n","  #   # assert False\n","  #   toggle = torch.le(scalar_sampProbMap, 1).float()\n","  #   scaled_sampMap = ( toggle * scalar_sampProbMap * samp_probMap\n","  #                     + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar) ).reshape([1, -1])\n","  #   # print(f'sparseProbMap(): scaled_sampMap type {type(scaled_sampMap)}')\n","  #   if scaled_sampMaps == None :\n","  #     scaled_sampMaps = torch.cat((scaled_sampMap,), dim=0).to(device)\n","  #   else :\n","  #     scaled_sampMaps = torch.cat((scaled_sampMaps, scaled_sampMap,), dim=0).to(device)\n","  #   # scaled_sampMaps.append(toggle * scalar_sampProbMap * samp_probMap + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar))\n","  # # print(f'sparseProbMap(): scaled_sampMaps size {scaled_sampMaps.size()}')\n","\n","  # NOTE: Dramatic slowdown after incorporating this function.\n","  #  May be due to use of for loop.\n","  # Use with scaled_sampMaps list version.\n","  # One big tensor. I'd prefer rows. Try adding brackets to append statement.\n","  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n","  # sparse_probMap = torch.cat((scaled_sampMaps,), dim=0)\n","  # sparse_probMap  = sparse_probMap.reshape([batch_size, -1])\n","  # assert False\n","  # return scaled_sampMaps   # Equivalent to sparse_probMap for a batch.\n","\n","class STEFunction(torch.autograd.Function) :\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, probability_mask) :\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input (prob_mask) and return\n","        a Tensor containing the output (binary_mask).\n","        ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        # Push probability map through a bernoulli sampling to create 0/1 mask.\n","        # Return mask.\n","\n","        prob_mask_size = probability_mask.size()\n","        # print(f'STEFunc() forward(): prob_mask_size {prob_mask_size}')\n","        # print(f'STEFunc() forward(): batch_size {batch_size}')\n","\n","        # Bernoulli sampling.\n","        # Sample from a uniform distribution.\n","        # uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n","        uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n","        # Bernoulli sampled binary mask.\n","        binary_mask = (probability_mask \u003e uni_samples).float()\n","        # Note the different sizes of probability_mask and uni_sample. The\n","        #  following operation returns a tensor shaped like uni_sample.\n","        # Note: torch.bernoulli() is not usable because func return only the prob mask,\n","        #  but the uniform distributions samples are needed for gradient calcs.\n","        # bin_mask = torch.bernoulli(probablility_mask)\n","        # print(f'STEFunc() forward(): bin_mask_size {binary_mask.size()}')\n","        # print(f'STEFunc() forward(): probability_mask {probability_mask},\\n\\tuni_samples {uni_samples}')\n","        # assert False\n","\n","        ctx.save_for_backward(probability_mask, uni_samples)\n","        return binary_mask\n","    \n","    @staticmethod\n","    def backward(ctx, grad_output) :\n","      # return F.hardtanh(grad_output)\n","      # SigDeriv graph: https://www.desmos.com/calculator/icbxupp3dh\n","      alpha = 1\n","      prob_mask, uni_samples = ctx.saved_tensors\n","\n","      # Sigmoid function derivative\n","      grad_est = (alpha * torch.exp(-alpha * (prob_mask - uni_samples))\n","          / (1 + torch.exp(-alpha * (prob_mask - uni_samples))) ** 2)\n","          # / torch.exp(1 + -alpha * (prob_mask - uni_samples)) ** 2)\n","      return grad_est * grad_output\n","\n","class StraightThroughEstimator(nn.Module) :\n","  def __init__(self) :\n","    # Consider moving probability map parameters here.\n","    #  If so, change STEFunction call.\n","    super(StraightThroughEstimator, self).__init__()\n","  \n","  def forward(self, probability_mask) :\n","    binary_mask = STEFunction.apply(probability_mask)\n","  # def forward(self, probability_mask, batch_size) :\n","    # binary_mask = STEFunction.apply(probability_mask, batch_size)\n","    return binary_mask\n","\n","class Encoder(nn.Module):\n","    # Consider removing the default parameter values.\n","    def __init__(self, input_dim=0, output_dim=0):\n","        super(Encoder, self).__init__()\n","        self.flatten = nn.Flatten()\n","        # Note: nn.Sequential may take only a single argument. A tuple may work,\n","        #  but unclear if this is stable.\n","        self.binary_STE_stack = nn.Sequential(\n","            # Define the input dimensions\n","            # STE is placed at the bottleneck of the autoencoder.\n","            StraightThroughEstimator(),\n","        )\n","        # self.ste = StraightThroughEstimator()\n","        # self.steFunc = STEFunction().apply\n","        self.sparProbMapFunc = sparseProbMap\n","        unif_samp_tn = torch.zeros([input_dim]).uniform_()\n","        self.mask_params = nn.Parameter(unif_samp_tn)\n","        # fill_val_tn = torch.zeros([input_dim]).fill_(0.5)\n","        # self.mask_params = nn.Parameter(fill_val_tn)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        prob_mask = torch.sigmoid(self.mask_params)   # Ensures probabilities lie btwn (0, 1)\n","        batch_size = x.size()[0]\n","        # Used to create bin_mask for ea training sample.\n","        #  To switch to one bin_mask per batch, comment out the following line.\n","        # prob_mask = prob_mask.repeat(batch_size, 1)\n","        sparse_probMask = self.sparProbMapFunc(prob_mask, sparsity=0.012)\n","        sparse_probMask = sparse_probMask.repeat(batch_size, 1)\n","        # sparse_probMask = prob_mask.repeat(batch_size, 1)\n","        # Using a tuple input is problematic because the model expects a grad for all inputs to nn.Sequential.\n","        binary_mask = self.binary_STE_stack(sparse_probMask)\n","        # binary_mask = self.binary_STE_stack(prob_mask)\n","        # binary_mask = self.ste(prob_mask)\n","        # binary_mask = self.steFunc(prob_mask)\n","        # print(f'Encoder.forward(): prob_mask size {prob_mask.size()}')\n","        # print(f'Encoder.forward(): prob_mask size {prob_mask}')\n","        # assert False\n","        # print(x.size())\n","        # print(f'Encoder.forward(): mask_params {self.mask_params}')\n","        # print('Encoder.forward(): batch_size', batch_size)\n","        # print('Encoder.forward(): binary_mask', binary_mask)\n","        return x * binary_mask, binary_mask, self.mask_params\n","\n","    # May or may not need to define the backward behavior of this class.\n","    # def backward(self, x):\n","    #     pass\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, input_dim, output_dim):   # output_dim is not yet used.\n","        super(Decoder, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_lrelu_stack = nn.Sequential(\n","            # Define the input dimensions\n","            nn.Linear(input_dim, 512),\n","            # nn.Linear(input_dim, output_dim),\n","            nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # # Define the output dimensions\n","            nn.Linear(512, output_dim),\n","            nn.LeakyReLU(),\n","        )\n","        # Yisong: initialize the weights in the first layer, and the following layers will follow suit.\n","        # self.linear_lrelu_stack[0].weight.data /= 100.\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        # print(x.size())\n","        logits = self.linear_lrelu_stack(x)\n","        return logits\n","\n","class Autoencoder(nn.Module) :\n","    def __init__(self, input_dim, output_dim) :\n","        super(Autoencoder, self).__init__()\n","        # self.auto_stack = nn.Sequential(\n","        #     Encoder(input_dim, output_dim),\n","        #     Decoder(),\n","        # )\n","        self.encoder = Encoder(input_dim=input_dim)\n","        self.decoder = Decoder(input_dim, output_dim)\n","\n","    def forward(self, x, binMaskSizeLs=[], encode=False, decode=False) :\n","        if encode :\n","            pass\n","        elif decode :\n","            pass\n","        else :\n","            # x = self.auto_stack(x)\n","            under_samp_meas, binary_mask, mask_params = self.encoder(x)\n","            # Working out what to do with the binary_mask -- i.e. if it will be fed to decoder or not.\n","            bin_mask_size = binary_mask.sum()\n","            batch_size = x.size()[0]\n","            # binMaskSizeLs.append(bin_mask_size)\n","            # print(f'AE forward(): avg binary_mask size {int(bin_mask_size / batch_size)}')\n","            # print(f'Auto.forward(): mask_params {mask_params}')\n","            x = self.decoder(under_samp_meas)\n","        return x, mask_params\n","\n","# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using {} device\".format(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Bs_956ARP3c"},"outputs":[],"source":["# input_dim = 10\n","# init_tensor = torch.zeros([input_dim]).uniform_()\n","# print(init_tensor)\n","# p_mask = nn.Parameter(init_tensor)\n","# print(p_mask)\n","# ste = STEFunction()\n","# print(ste)\n","# masks = ste.forward(init_tensor, p_mask)\n","# a = torch.randn(1, 2, 3, 4)\n","# b = torch.randn(2, 2)\n","# print(a)\n","# print(a.size())\n","# print(a.view(1, 1, 2, 3, 2, 2))\n","# print(b)\n","# print(b * a.view(1, 1, 2, 3, 2, 2))\n","\n","# Create a mask for each sample by first creating a uniform value tn of size\n","#  [batch_size, prob_mask_size], then \n","# c = torch.zeros([10]).uniform_()\n","# print(c)\n","# tn = c.new_empty([3, 10]).uniform_()\n","# print(tn)\n","# bin_mask = (c \u003e tn).float()\n","# print(bin_mask)\n","# End \"Create a mask ...\"\n","\n","# print(c)\n","# mask = STEFunction.forward(c, c)   # Comment out ctx line.\n","# print(mask)   # OK\n","# module = StraightThroughEstimator()\n","# mask = module(c)\n","# print(mask)\n","# module1 = Encoder(input_dim=10)\n","# meas = torch.randint(100, (10,))\n","# print(meas)\n","# mask = module1(meas)\n","# print(mask)   # OK"]},{"cell_type":"markdown","metadata":{"id":"BTY1vNInM3RI"},"source":["####Training and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fi2L7VdNGqNb"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer, pToPIdxDict=None, epoch=0, mod=100):\n","    \"\"\"\n","    pipIdx (for loop) is a tensor containing the pipe indices associated with a training leak scenario.\n","    pToPIdxDict is a dict that maps pipe names (str) to a tuple of (region, pipe_idx)\n","      used to determine number of rows.\n","    \"\"\"\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)   # For avg training loss\n","    # print(f'train_loop(): dataset size: {size}')\n","    # print(f'train_loop(): num_batches: {num_batches}')\n","    train_loss, train_accuracy = 0, 0\n","    confusion_matrix = torch.zeros(len(pToPIdxDict), 4, dtype=torch.int16)   # Think of way to make # of regions automatic.\n","                                                                             # Maybe add a 'lengths' key, value: (pipe_ct, reg_ct)\n","    prevLoss = 100.\n","    for batch, (X, y, pipIdx) in enumerate(dataloader):\n","        # print(batch, X)\n","        # print(y)\n","        # print(pipIdx)\n","        X = X.to(device)\n","        y = y.to(device)\n","        # Compute prediction and loss\n","        pred, prob_params = model(X)  # params for flexibility\n","        # print(y.size())\n","        # print(pred.size())\n","        # print(pred)\n","        # print(prob_params)\n","        \n","        if (True) :\n","        # if (epoch \u003c 100) :\n","          # prob_mask * X -\u003e classifier -\u003e pred\n","          # # print(f\"mask_params {model.state_dict()['encoder.mask_params']}\")\n","          # prob_params = model.state_dict()['encoder.mask_params']  # could be slower than passing the params out\n","          prob_map = torch.sigmoid(prob_params)   # Ensures values (probabilities) lie btwn (0, 1)\n","          splus = nn.functional.softplus(prob_params, -1)   # -log(1 + e^(-x)); careful with the sign when using in expressions.\n","          # print(splus[0], prob_params[0])   # Sanity check\n","          # print(torch.max(splus))\n","          # assert torch.max(splus) \u003c 0\n","          # print(epoch)\n","          # loss_KL = ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) - torch.log(torch.tensor(0.5)) ).sum()\n","          # p_m = torch.where(prob_map \u003e 0, prob_map, torch.tensor(0.001).to(device))\n","          # p_m = torch.where(prob_map \u003c 1, prob_map, torch.tensor(0.999).to(device))\n","          # # print(f'p_m max {torch.max(p_m)}, min {torch.min(p_m)}')\n","          # # print(f'p_m range {torch.max(p_m) + abs(torch.min(p_m))}')\n","          # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) - torch.log(torch.tensor(0.5)) ).sum()\n","          # # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) ).sum()\n","          # loss_KL = lamb * ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) ).sum()\n","          # Natural log is the result of torch.log(...)\n","          loss_KL = ( (1 - prob_map) * ((-1)*prob_params + splus) + prob_map * splus - torch.log(torch.tensor(0.5)) ).sum()   # includes constant term\n","          # loss_KL = ( (1 - prob_map) * ((-1)*prob_params + splus) + prob_map * splus ).sum()\n","          # Dramatic change between including lamb and not may affect learning.\n","          lamb = torch.exp(torch.tensor(-1/25*epoch))   # zeros around 250 epochs; https://www.desmos.com/calculator/miayf0qrbe\n","          # lamb = torch.exp(torch.tensor(-1/4000*epoch))   # zeros around 28k epochs; https://www.desmos.com/calculator/6fi4dfzuji\n","          # lamb = torch.exp(torch.tensor(-1/6000*epoch))   # zeros around 44k epochs; https://www.desmos.com/calculator/cwlhq5xe4n\n","          # lamb = torch.exp(torch.tensor(-1/10000*epoch))   # zeros around 70k epochs \n","          # lamb = torch.exp(torch.tensor(-1/14000*epoch))   # zeros around 100k epochs # Try 1 / sqrt(epoch)   # Was using 1 / e^(-alpha*x) =\u003e e^(alpha*x) i.e. not what I wanted.\n","          # lamb = 1 / epoch\n","          loss_KL = lamb * loss_KL   # Anneal diversity loss\n","        else :\n","          loss_KL = 0.0\n","        # # print(f'loss_KL {loss_KL}')\n","        # # assert loss_KL != float('nan')\n","        # assert not math.isnan(loss_KL)\n","\n","        # **** Notice pred is formed on the batch level, but loss_KL is a one shot ****\n","        loss = loss_fn(pred, y) + loss_KL  # returns single value; avg loss across batch\n","        # loss = loss_fn(pred, y)   # returns single value; avg loss across batch\n","        assert loss != float('nan')\n","        # loss = loss_fn(pred, y)  # returns single value; avg loss across batch\n","\n","        # print(loss * len(y))\n","        # for i in range(len(y)) :\n","        #   print(pred[i])\n","        #   print(y[i])\n","        #   ls = loss_fn(pred[i], y[i])\n","        #   print(f'loss {i} {ls}')\n","        # print(y)\n","        # print(pred.argmax(1).type(y.dtype))\n","\n","        # Confusion Matrix\n","        # (pipe label vs region) i.e. the resolution of the row is pipe name (not just region)\n","        # m(n, r)  where n is the cardinality of the leakpipe set, r is the number of regions\n","        # At the time of assignment,\n","        #  need a dict that maps pipe str to pipe idx\n","        #  the pipe label (str) (not the region label) for the training sample\n","        #  the model predicted region\n","        # preds = pred.argmax(1).type(y.dtype)\n","        # for i in range(len(y)) :\n","        #   confusion_matrix[pipIdx[i]][preds[i]] += 1\n","        # print(f'train_loop(): preds {preds}')\n","        # print(confusion_matrix)\n","        # print(f'train_loop(): conf_mat {confusion_matrix.size()}')\n","        # assert False\n","        # Older version of conf mat.\n","        # preds = pred.argmax(1).type(y.dtype)\n","        # for i in range(len(y)) :\n","        #   confusion_matrix[y[i]][preds[i]] += 1\n","        \n","        # Top-k predictions\n","        # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -\u003e (Tensor, LongTensor)\n","        # k = 1\n","        # top_k = torch.topk(input=pred, k=k, dim=1,)\n","        # print(top_k)\n","\n","        # Disaggregate performance -- save model\n","        #  goal: extract outliers (in another notebook)\n","        #  Make into a function.\n","        # if epoch \u003e 2000 and loss.item() \u003e (prevLoss * 15) :\n","        # if epoch == 10000 :\n","        #   print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n","        #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n","        # # print(prevLoss)\n","        # prevLoss = loss.item()\n","        # print(prevLoss \u003e loss * 20)\n","        # print(epoch)\n","        # assert False\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # train_loss += loss_fn(pred, y).item() * len(y)   # I think this second call to loss_fn runs the grad twice. ???\n","        train_loss += loss.item() * len(y)\n","        # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n","        train_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","        # print(loss)\n","\n","        if batch % mod == 0:\n","            # print(batch)\n","            # print(y)\n","            # print(pred)\n","            loss, current = loss.item(), batch * len(X)\n","            # print(f\"loss: {loss:\u003e7f}  [{current:\u003e5d}/{size:\u003e5d}]\")\n","\n","    # Save last model\n","    # if epoch == 10000 :\n","    #     print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n","    #     torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n","\n","    train_loss /= size    # weighted avg training loss\n","    train_accuracy /= size\n","    print(f\"Training Error: \\n Accuracy: {(100*train_accuracy):\u003e0.1f}%, Avg loss: {train_loss:\u003e8f} \\n\")\n","    return train_loss, train_accuracy, confusion_matrix\n","\n","\n","def test_loop(dataloader, model, loss_fn, pToPIdxDict=None, out_dim=10):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    # print(f'test dataset size: {size}')\n","    # print(f'test num_batches: {num_batches}')\n","    test_loss, test_accuracy = 0, 0\n","    confusion_matrix = torch.zeros(len(pToPIdxDict), out_dim, dtype=torch.int32)\n","\n","    with torch.no_grad():\n","        for X, y, pipIdx in dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","\n","            pred, _ = model(X)\n","            # print(pred)\n","            # print(y)\n","            test_loss += loss_fn(pred, y).item()\n","            # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n","            test_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","            # Confusion matrix\n","            preds = pred.argmax(1).type(y.dtype)\n","            for i in range(len(y)) :\n","              # confusion_matrix[y[i]][preds[i]] += 1\n","              confusion_matrix[pipIdx[i]][preds[i]] += 1\n","            # print(f'test_loop(): conf mat {confusion_matrix}')\n","\n","    test_loss /= num_batches\n","    test_accuracy /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):\u003e0.1f}%, Avg loss: {test_loss:\u003e8f} \\n\")\n","    # return test accuracy percentage for epoch\n","    return test_accuracy, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"ydc9rlm86Hmt"},"source":["####Animator (d2l)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KnHcBLdIJYH"},"outputs":[],"source":["def use_svg_display():\n","    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n","    display.set_matplotlib_formats('svg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKViWEWBIK2E"},"outputs":[],"source":["def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n","    \"\"\"Set the axes for matplotlib.\"\"\"\n","    axes.set_xlabel(xlabel)\n","    axes.set_ylabel(ylabel)\n","    axes.set_xscale(xscale)\n","    axes.set_yscale(yscale)\n","    axes.set_xlim(xlim)\n","    axes.set_ylim(ylim)\n","    if legend:\n","        axes.legend(legend)\n","    axes.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xM5q0mh96LMw"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","from IPython import display   # Try commenting out. Maybe I'll be able to save image from print out (rather than files)\n","\n","class Animator:\n","    \"\"\"For plotting data in animation.\"\"\"\n","    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n","                 ylim=None, xscale='linear', yscale='linear',\n","                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n","                 figsize=(3.5, 2.5)):\n","        # Incrementally plot multiple lines\n","        if legend is None:\n","            legend = []\n","      \n","        self.nrows = nrows\n","        self.ncols = ncols\n","        self.figsize = figsize\n","\n","        self.xlabel = xlabel\n","        self.ylabel = ylabel\n","        self.xlim = xlim\n","        self.ylim = ylim\n","        self.xscale = xscale\n","        self.yscale = yscale\n","        self.legend = legend\n","\n","        self.X, self.Y, self.fmts = None, None, fmts\n","\n","    def add(self, x, y):\n","        # Add multiple data points into the figure\n","        if not hasattr(y, \"__len__\"):\n","            y = [y]\n","        n = len(y)\n","        if not hasattr(x, \"__len__\"):\n","            x = [x] * n\n","        if not self.X:\n","            self.X = [[] for _ in range(n)]\n","        if not self.Y:\n","            self.Y = [[] for _ in range(n)]\n","        for i, (a, b) in enumerate(zip(x, y)):\n","            if a is not None and b is not None:\n","                self.X[i].append(a)\n","                self.Y[i].append(b)\n","\n","    \n","    def display_plt(self):\n","        # Borrowed use_svg_display() implementation from d2l\n","        use_svg_display()\n","        # matplot function\n","        self.fig, self.axes = plt.subplots(self.nrows, self.ncols, figsize=self.figsize)\n","        if self.nrows * self.ncols == 1:\n","            self.axes = [self.axes,]\n","        # Use a lambda function to capture arguments; set_axes in d2l API\n","        self.config_axes = lambda: set_axes(self.axes[0],\n","                                            self.xlabel,\n","                                            self.ylabel,\n","                                            self.xlim,\n","                                            self.ylim,\n","                                            self.xscale, self.yscale, self.legend)\n","        self.axes[0].cla()\n","        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n","            self.axes[0].plot(x, y, fmt)\n","        self.config_axes()\n","        display.display(self.fig)\n","        display.clear_output(wait=True)\n","\n","        # return self.fig"]},{"cell_type":"markdown","metadata":{"id":"3ocO0Om3ofeX"},"source":["####Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxqpHHcoomED"},"outputs":[],"source":["# %run /content/drive/MyDrive/'Colab Notebooks'/'Water Distribution Network'/'Input Pipeline'/'Data To File'/SimData_to_csv(hdf)_v1.ipynb\n","class Conf_Mat() :\n","  def __init__(self, classes=10) :\n","    \"\"\"classes (int): the number of classes in the classifier.\"\"\"\n","    self.confusion_matrix = torch.zeros(classes, classes, dtype=torch.int32)\n","    # Load labels via JSON or csv file.\n","    self.labels_str = ['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024']\n","  \n","  def addValues(self, pred, y) :\n","    \"\"\"Add values to the confusion matrix.\n","    pred (tensor): tensor containing model predictions.\n","    y (tensor): tensor containing ground truth labels.\n","    return None\n","    \"\"\"\n","    # Confusion Matrix\n","    for i in range(len(y)) :\n","      preds = pred.argmax(1).type(y.dtype)\n","      confusion_matrix[y[i]][preds[i]] += 1\n","    print(confusion_matrix)\n","  \n","  def displayConfMat(self) :\n","    pass\n","\n","def decode_labels() :\n","  # Time consuming\n","  # Might be easier to place the encoder in a file and read it here.\n","  # WARNING: lab_subset order is not consistent.\n","  # NOTE: sets are not subscriptable\n","  # net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir, dest_dir, set_size, time_stamp, debug = feat_lab_args()\n","  # encoded_targets, lab_subset = labels_ds(base_jfile, simdata_dir, set_size, debug)\n","  # return sorted(['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024'])\n","  regdict_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/leak_pipes_all/tmstp80/regionAndPipeLabels/11regions/00/'\n","  partitions = 11\n","  file_nm = f'region_dict_{partitions}.pickle'\n","  load_loc = regdict_dir + file_nm\n","\n","  # For loading\n","  with open(load_loc, 'rb') as handle:\n","      reg_dict = pickle.load(handle)\n","  \n","  prediction_labels = []\n","  for reg_nm in reg_dict['reg_partits'] :   # reg_partits is a dict; key: reg_nm (str), value: ls of pipe in region\n","    prediction_labels.append(reg_nm)\n","  return sorted(prediction_labels)\n","# print(decode_labels())\n","\n","def leak_pipe_strs() :\n","  # Would like to develop an automatic method to transfer pipe names of labels (rather copy paste).\n","  #  In this noteboook, labels have been encoded (mapped) to a unique int in \n","  # Copy-paste list of pipe names for labels here from simdata_to_csv notebook.\n","  return sorted({'P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P1022',\n","                 'P1023', 'P1024', 'P1025', 'P1026', 'P1027', 'P1028', 'P1029', \n","                 'P103', 'P1030', 'P1031', 'P1032', 'P1033', 'P1034', 'P1035', \n","                 'P1036', 'P1039', 'P104', 'P1040', 'P1041', 'P1042', 'P1044', \n","                 'P1045', 'P106', 'P107', 'P108', 'P109', 'P11', 'P110', 'P111', \n","                 'P112', 'P113', 'P115', 'P116', 'P117', 'P118', 'P119', 'P12', \n","                 'P120', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', \n","                 'P128', 'P129', 'P13', 'P130', 'P131', 'P132', 'P134', 'P136', \n","                 'P138', 'P139', 'P14', 'P140', 'P141', 'P142', 'P144', 'P147', \n","                 'P148', 'P15', 'P150', 'P154', 'P155', 'P156', 'P157', 'P158', \n","                 'P159', 'P16', 'P160', 'P161', 'P162', 'P163', 'P165', 'P166', \n","                 'P17', 'P174', 'P177', 'P18', 'P184', 'P19', 'P195', 'P2', 'P20', \n","                 'P201', 'P21', 'P211', 'P215', 'P218', 'P219', 'P22', 'P220', 'P223', \n","                 'P225', 'P228', 'P23', 'P230', 'P231', 'P233', 'P234', 'P235', 'P237', \n","                 'P238', 'P24', 'P241', 'P242', 'P243', 'P245', 'P246', 'P248', 'P249', \n","                 'P25', 'P251', 'P252', 'P255', 'P256', 'P258', 'P259', 'P26', 'P264', \n","                 'P266', 'P267', 'P268', 'P27', 'P270', 'P272', 'P275', 'P28', 'P280', \n","                 'P282', 'P284', 'P285', 'P286', 'P287', 'P288', 'P29', 'P290', 'P291', \n","                 'P292', 'P293', 'P294', 'P295', 'P296', 'P297', 'P298', 'P299', 'P3', \n","                 'P30', 'P301', 'P302', 'P303', 'P304', 'P305', 'P307', 'P308', 'P309', 'P31', 'P310', 'P316', 'P319', 'P32', 'P320', 'P322', 'P323', 'P329', 'P33', 'P330', 'P331', 'P336', 'P337', 'P338', 'P339', 'P34', 'P340', 'P341', 'P343', 'P344', 'P346', 'P347', 'P348', 'P349', 'P35', 'P350', 'P37', 'P372', 'P374', 'P375', 'P376', 'P378', 'P379', 'P38', 'P380', 'P381', 'P383', 'P384', 'P385', 'P386', 'P39', 'P397', 'P398', 'P399', 'P40', 'P402', 'P403', 'P409', 'P410', 'P42', 'P424', 'P43', 'P44', 'P443', 'P445', 'P446', 'P450', 'P46', 'P465', 'P467', 'P468', 'P48', 'P482', 'P484', 'P49', 'P492', 'P5', 'P500', 'P501', 'P502', 'P51', 'P510', 'P52', 'P524', 'P527', 'P529', 'P53', 'P54', 'P55', 'P57', 'P58', 'P596', 'P597', 'P6', 'P609', 'P610', 'P63', 'P633', 'P64', 'P65', 'P67', 'P670', 'P671', 'P68', 'P69', 'P697', 'P7', 'P70', 'P71', 'P72', 'P724', 'P725', 'P752', 'P753', 'P754', 'P755', 'P756', 'P757', 'P758', 'P759', 'P760', 'P761', 'P763', 'P766', 'P767', 'P768', 'P769', 'P771', 'P772', 'P775', 'P776', 'P777', 'P779', 'P780', 'P781', 'P783', 'P784', 'P785', 'P786', 'P787', 'P788', 'P789', 'P791', 'P794', 'P795', 'P796', 'P797', 'P798', 'P8', 'P800', 'P801', 'P804', 'P805', 'P806', 'P807', 'P808', 'P809', 'P810', 'P811', 'P813', 'P815', 'P817', 'P819', 'P821', 'P822', 'P823', 'P826', 'P827', 'P83', 'P830', 'P831', 'P84', 'P840', 'P841', 'P842', 'P844', 'P846', 'P847', 'P85', 'P850', 'P851', 'P852', 'P853', 'P855', 'P858', 'P859', 'P86', 'P861', 'P866', 'P87', 'P871', 'P880', 'P889', 'P89', 'P892', 'P9', 'P90', 'P91', 'P914', 'P915', 'P92', 'P924', 'P927', 'P929', 'P930', 'P931', 'P932', 'P933', 'P934', 'P935', 'P937', 'P938', 'P939', 'P94', 'P940', 'P941', 'P942', 'P943', 'P944', 'P946', 'P947', 'P948', 'P949', 'P95', 'P951', 'P953', 'P954', 'P955', 'P956', 'P957', 'P958', 'P959', 'P96', 'P961', 'P962', 'P963', 'P964', 'P965', 'P966', 'P967', 'P968', 'P969', 'P97', 'P970', 'P971', 'P972', 'P973', 'P974', 'P975', 'P976', 'P977', 'P978', 'P98', 'P981', 'P982', 'P983', 'P984', 'P986', 'P987', 'P988', 'P989', 'P99', 'P990', 'P991', 'P992', 'P993', 'P994', 'P995', 'P996', 'P997', 'P998', 'P999'})"]},{"cell_type":"markdown","metadata":{"id":"RVZsPo1p2RoP"},"source":["####Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDXCstVB2XIb"},"outputs":[],"source":["def norm(features) :\n","  # print(f'norm(): {features} size {features.size()}')\n","  sq_feat = features ** 2\n","  # print(f'norm(): {sq_feat} size {sq_feat.size()}')\n","  sum_feat = sq_feat.sum(1)\n","  # print(f'norm(): {sum_feat} size {sum_feat.size()}')\n","  norm_feat = torch.sqrt(sum_feat)\n","  # print(f'norm(): {norm_feat} size {norm_feat.size()}')\n","  # print(norm_feat.view(features.size(0), 1))\n","  unit_feat = features / norm_feat.view(features.size(0), 1)\n","  # print(f'norm(): {unit_feat} size {unit_feat.size()}')\n","  return unit_feat\n","# _, observed, __ = SimData(net_char=4, tmstp=80)\n","# norm(observed)"]},{"cell_type":"markdown","metadata":{"id":"TmFOoA-k258k"},"source":["####Subsampling"]},{"cell_type":"markdown","metadata":{"id":"5wCj-uKc561c"},"source":["#####Mask Generation\n","- (1/0) Sensing Mask Vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZnE0eEEGO88"},"outputs":[],"source":["def sensing_mask_rand(feature_vec, max_sense = 30):\n","  \"\"\"\n","  preconditions: feature tensor must be flat.\n","\n","  Parameters\n","  feature_vec: feature tensor to be masked\n","   shape: list of dimensions\n","  max_sense: randomly choose 1 to max_sense to be 1, rest 0\n","\n","  returns 0/1 sensing mask tensor of shape feature_vec w/ max_sense elems set to 1 (rest 0)\n","  \"\"\"\n","\n","  # Create (0/1) mask populated w/ max_sense ones\n","  mask = torch.zeros(feature_vec.size()).to(device)\n","  # print(mask)\n","  # indices = torch.randint(len(feature_vec), size=(max_sense,))   # may contain fewer than max_sense unique indices\n","  # Katie: Randomize indices and choose the first max_sense\n","  indices = [*range(len(feature_vec))]\n","  random.shuffle(indices)\n","  # print(indices)\n","  # for idx in indices:   # for use with line 16\n","  for idx in indices[:max_sense]:\n","    mask[idx] = 1\n","  # print(mask)\n","\n","  # Pass features through the mask\n","  masked_features = feature_vec * mask\n","\n","  return mask, masked_features.to(device)\n","# feature_vec = torch.rand([100])\n","# print(feature_vec)\n","# mask, masked_feats = sensing_mask_rand(feature_vec)\n","# print(mask, '\\n', masked_feats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaJ04dzFpbzL"},"outputs":[],"source":["def sensing_mask_alternate(feature_vec):\n","  \"\"\"\n","  preconditions: feature tensor must be flat.\n","\n","  Parameters\n","  feature_vec: feature tensor to be masked\n","\n","  returns 0/1 sensing mask tensor of shape feature_vec w/ even indexed elems set to 1 (rest 0)\n","  \"\"\"\n","\n","  # Create (0/1) mask populated w/ max_sense ones\n","  mask = torch.zeros(feature_vec.size()).to(device)\n","  # print(mask)\n","  for idx in range(0, feature_vec.size()[0], 2) :\n","    mask[idx] = 1\n","  # print(mask)\n","\n","  # Pass features through the mask\n","  # masked_features = feature_vec * mask\n","\n","  # Simplification: This version reduces the size of the feature\n","  #  vector by half (mask not concat).\n","  masked_features = feature_vec[0].reshape([1])\n","  for idx in range(2, feature_vec.size()[0], 2) :\n","    masked_features = torch.cat((masked_features, feature_vec[idx].reshape([1])))\n","\n","  return mask, masked_features.to(device)\n","# feature_vec = torch.rand([100]).to(device)\n","# print(feature_vec)\n","# mask, masked_feats = sensing_mask_alternate(feature_vec)\n","# print(mask, '\\n', masked_feats.size(), '\\n', masked_feats)"]},{"cell_type":"markdown","metadata":{"id":"7KkFT5x93F5H"},"source":["#####Random subsamples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfeD3clN3QKu"},"outputs":[],"source":["def random_subset(meas_tensor, size, seed=None) :\n","  \"\"\"Random subset of measurements (fixed seed).\n","  Must be able to change the cardinality of the subset.\n","  Algorithm\n","    create list of indices\n","    randomize indices\n","    select first x number of indices (where x is the subset cardinality)\n","    form a tensor containing the the sample measurements matching those indices.\n","    return this tensor\n","  \"\"\"\n","  gen = None\n","  if seed :\n","    gen = torch.Generator().manual_seed(seed)\n","\n","  rand_idxs = torch.randperm(meas_tensor.size()[0], generator=gen)\n","  sub_idxs = rand_idxs[:size]\n","  # print('random_subset():', sub_idxs, sub_idxs.size())\n","  subset = meas_tensor[sub_idxs]\n","  # print(meas_tensor.size())\n","  return subset.to(device), sub_idxs\n","\n","# Test code\n","# X = torch.rand(100)\n","# rand_sub = random_subset(X, 50, 1000)\n","# print('random_subset results:', rand_sub)\n","# print(rand_sub.size())\n","# assert False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Di_I5E4g4PDR"},"outputs":[],"source":["def rand_sub_dataset(X, size, seed=None) :\n","  reduced_meas_X = torch.zeros([X.size(0), size]).to(device)\n","  # print(X.size(0))\n","  for i in range(len(X)) :\n","    rand_sub, sub_idxs = random_subset(X[i], size, seed)\n","    # print('random_subset results:', rand_sub)\n","    # print(rand_sub.size())\n","    reduced_meas_X[i] = rand_sub\n","  return reduced_meas_X, sub_idxs\n","\n","# Test Code\n","# X = torch.rand([3, 20])\n","# size, seed = 10, 1000\n","# reduced_meas_X, sub_idxs = rand_sub_dataset(X, size, seed)\n","# # print(sub_idxs)\n","# for i, idx in enumerate(sub_idxs) :\n","#   # print(reduced_meas_X[0, i], X[0, idx])\n","#   if reduced_meas_X[0, i].item() != X[0, idx].item() :\n","#     raise Exception('rand_sub_dataset(): Error: Value doesnt match.')\n","# # print(X[0, 307], X[0, 536], X[0, 329])\n","# # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n","# print('Done!')"]},{"cell_type":"markdown","metadata":{"id":"if63qNQuS0HA"},"source":["####Load CSV (SimData) and form concat dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uY7IINhjLGSQ"},"outputs":[],"source":["def SimData(net_char, tmstp) :\n","  \"\"\"\n","  Note: Dataset file generated in SimData_to_csv notebook\n","   row 0: base_case\n","   rows 1-\u003eset_size: observed; one leak scenario per row; 1hr (out of one week)\n","    Labels included (last column)\n","  \"\"\"\n","\n","  # Load dataframe\n","  src_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/'\n","  leak_pip_ct = 'all'\n","  leak_pipes = f'leak_pipes_{leak_pip_ct}/'\n","  if tmstp :\n","    ts_dir = f'tmstp{tmstp}/'\n","  else :\n","    ts_dir = ''\n","  # NOTE: double check dataset file name\n","  dset_size = 5000\n","  dataset_file = f'regionAndPipeLabels/11regions/00/dataset{dset_size}'\n","  if net_char == 0:   dataset_file += '_link_flowrate_area0.01_0.1.csv'\n","  elif net_char == 1: dataset_file += '_link_headloss_area0.01_0.1.csv'\n","  elif net_char == 2: dataset_file += '_link_velocity_area0.01_0.1.csv'\n","  elif net_char == 3: dataset_file += '_node_demand_area0.01_0.1.csv'\n","  elif net_char == 4: dataset_file += '_node_head_area0.01_0.1.csv'\n","  elif net_char == 5: dataset_file += '_node_pressure_area0.01_0.1.csv'\n","  # areaLo, areaHi = 0.01, 0.1\n","  # leak_area = f'_area{areaLo}_{areaHi}'\n","  # dataset_file += '.csv'\n","  \n","  data_file = src_dir + leak_pipes + ts_dir + dataset_file\n","  datast_ds = pd.read_csv(data_file)\n","  # print(datast_ds.head())   # View csv data.\n","  \n","  # Separate base_case, raw_data, and encoded labels\n","  # Base Case\n","  # print(datast_ds.values[0, :-1].astype(np.float32))\n","  # print(type(datast_ds.values[0, :-1].astype(np.float32)))\n","  # base_case = torch.tensor(datast_ds.values[0, :-1], dtype=torch.float32).to(device)\n","  # base_case = torch.tensor(datast_ds.values[0, :-1].astype(np.float32), dtype=torch.float32).to(device)\n","  base_case = torch.tensor(datast_ds.values[0, :-2], dtype=torch.float32).to(device)   # Used w/ separate cols for pipeIdx and Label (no tuple)\n","  # print(base_case)\n","  # raw_data\n","  # raw_data = torch.tensor(datast_ds.values[1: , :-1], dtype=torch.float32).to(device)\n","  # raw_data = torch.tensor(datast_ds.values[1: , :-1].astype(np.float32), dtype=torch.float32).to(device)  # Had to add astype() when I added tuples in label col\n","  raw_data = torch.tensor(datast_ds.values[1: , :-2], dtype=torch.float32).to(device)   # Used w/ separate cols for pipeIdx and Label (no tuple)\n","  print(f'SimData(): raw_data {raw_data.size()}')\n","  # encoded labels\n","  # print(datast_ds['Label'][1])\n","  # print(type(datast_ds['Label'].astype('int')[1]))\n","  pipe_idxs = torch.tensor(datast_ds['PipeIdx'], dtype=torch.long)[1:].to(device)\n","  labels = torch.tensor(datast_ds['Label'], dtype=torch.long)[1:].to(device)\n","  # Possible implementation for pipe_idx (mapping of pipe_nms to pipe_idx done in simdata_to_csv notebook)\n","  #  Change base_case and raw_data to [1: , :-2]; the last two rows would contain pipe_idx and label values respectively.\n","  # pipe_idx = torch.tensor(datast_ds['Pipe_idx'], dtype=torch.long)[1:].to(device)\n","  print(f'SimData(): labels {labels.size()}')\n","\n","  return base_case, raw_data, labels, pipe_idxs\n","# _, __, ___ = SimData(0, 80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6Xn9Due5kYf"},"outputs":[],"source":["def leakpipe_subset(pipe_ct, raw, enc_labs):\n","  \"Used to limit the number of pipes in the dataset\"\n","  tmp_raw = torch.tensor([[0]]).to(device)   # dim trouble w/ cat\n","  tmp_labs = torch.tensor([0]).to(device)\n","  print(tmp_labs.size())\n","\n","  for i in range(len(enc_labs)):\n","    if enc_labs[i] \u003c pipe_ct:\n","      tmp_raw = torch.cat((tmp_raw, raw[i].reshape([444])))\n","      tmp_labs = torch.cat((tmp_labs, enc_labs[i].reshape([1])))\n","\n","  return tmp_raw[1:], tmp_labs[1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alN3uGMtzKbn"},"outputs":[],"source":["def cat_net_attr(net_ls, tmstp, residual=False, norm_feats=False) :\n","  \"\"\"Creates three flat vectors containing concatinations of each attribute's\n","       (1) base_case, (2) observed measurements, and (3) a single copy of label data (leak labels for all attrs are the same).\n","     **If residual is true, don't use norm here.**\n","     \"\"\"\n","  cat_attrs = None\n","  for net_char in net_ls :\n","    # base_case, X_raw, encoded_labels\n","    data = SimData(net_char, tmstp)\n","    data_ls = list(data)   # Creates 3 elem list from data 3-tuple returned by SimData.\n","\n","    if residual :   # Residual before normalization of features.\n","      data_ls[1] -= data_ls[0]\n","    # print(data_ls[1])\n","    # assert False\n","    if norm_feats :   # Normalize each attribute data separtely before cat.\n","      data_ls[1] = norm(data_ls[1])\n","\n","    if not cat_attrs :\n","      cat_attrs = data_ls   # Includes labels. Only needed once.\n","    else :\n","      cat_attrs[0] = torch.cat((cat_attrs[0], data_ls[0]))  # Update base_case by cat'ing next net_attr.\n","      cat_attrs[1] = torch.cat((cat_attrs[1], data_ls[1]), dim=1)   # Update features by cat'ing next net_attr.\n","      # only need one set of label; no need to cat those again.\n","    # Normalize Features -- all together (i.e. if more than one attribute is cat'ed, all will be norm'ed together)\n","    # if norm_feats:\n","      # Raw data normed across all samples\n","      #  Would it make more sense to normalize ea sample individually?\n","      # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n","      # cat_attrs[1] = norm(cat_attrs[1])\n","  print(f'cat_net_attr(): Base {cat_attrs[0].size()}, X {cat_attrs[1].size()}, Labels {cat_attrs[2].size()}')\n","  return cat_attrs   # cat_attrs is a list (of tensors), but can be expanded by the caller.\n","# _, __, ___ = cat_net_attr([0, 4], 80, True)\n","# print(_.size(), __.size(), ___.size())\n","# assert False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWwFaq5FMiPh"},"outputs":[],"source":["def cat_data(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], tmstp=None) :\n","  \"\"\"Idea: pass list of tmstps to this function, for loop the list to create a training set w/ training samples from mult tmstps.\n","  \"\"\"\n","  if isinstance(net_char, int) :\n","    net_char = [net_char]\n","  \n","  n_fs = norm_feats\n","  if residual :\n","    n_fs = False\n","  \n","  # Consider writing a script that automates both pulling mult tmstps of sim data together, but also cats features\n","  #  or perhaps just a dataset constructor script where multiple tmstps can be assembled into a single training set.\n","  base_case, X_raw, encoded_labels, pipIdxs = cat_net_attr(net_char, tmstp, residual, norm_feats=n_fs)   # Don't norm_feat in cat_net_attr if residual is true.\n","  # base_case, X_raw, encoded_labels = SimData(net_char, tmstp)\n","  print(f'cat_data(): X_raw max {torch.max(X_raw)}, min {torch.min(X_raw)}')\n","  print(f'cat_data(): X_raw range {torch.max(X_raw) + abs(torch.min(X_raw))}')\n","  print(f'cat_data(): X_raw {X_raw.size()}')\n","  # print(f'cat_data(): X_raw {X_raw[0]}')\n","  # print(f'cat_data(): base_case {base_case}')\n","  # print(f'cat_data(): pipe_idxs {encoded_labels}')\n","  \n","  # work in progress\n","  # X_raw, encoded_labels = leakpipe_subset(10, X_raw, encoded_labels)\n","  # print(f'cat_data: encoded labels \\n{encoded_labels}')\n","  # print(f'cat_data: encoded labels {encoded_labels.size()}')\n","\n","  # X_cat = None\n","  # norm_str = ''   # Used for debug output\n","  # Residual -- handled in cat_net_data\n","  # div_by_base = base_case   # What is this used for?\n","  # if residual:\n","    # Avoid dividing by zero\n","    # for idx in range(len(base_case)):\n","      # if base_case[idx] \u003c 1.0e-12:\n","      #   div_by_base[idx] = 1.0e-10\n","    # X_raw = X_raw / div_by_base\n","    # X_raw -= base_case\n","    # base_case /= base_case   # might divide by zero\n","    # pass\n","  # print(f'cat_data(): X_raw {X_raw[0]}')\n","  \n","  # Normalizations\n","  #  Not sure this code is doing anything as X_raw has already been formed.\n","  if norm_base:\n","    # Normalize base_case\n","    #  orders of magnitude larger than residual data\n","    #  v = v / max(||v||_p, epsilon)  where epsilon is a small value that void dividing by zero\n","    # norm_base_case = torch.nn.functional.normalize(base_case.reshape([1,-1]))\n","    base_case = torch.nn.functional.normalize(base_case, dim=0) * 10.0   # Out-dated, use with caution.\n","    norm_str = 'norm_'\n","  # if norm_feats:\n","  #   # Raw data normed across all samples\n","  #   #  Would it make more sense to normalize ea sample individually?\n","  #   # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n","  #   X_raw = norm(X_raw)\n","  # print(f'cat_data(): {norm_str}base_case {base_case.size()}')\n","  # # print(f'cat_data(): {norm_str}base_case {base_case}')\n","\n","  # print(X_raw[0])   # X_raw elements are already a flat tensor.\n","  # I think I can move if mask outer and elim the for loop.\n","  # for feature_vec in X_raw:\n","  #   if mask :\n","  #     # Mask and masked features\n","  #     #  May want sensing_mask_rand() that can process batches of samples\n","  #     # mask_tn, masked_feats = sensing_mask_rand(feature_vec)\n","  #     # Construct feature set from concatination of base_case, mask, and masked measuremnts.\n","  #     # temp = torch.cat((masked_feats.to(device), mask.to(device), base_case.to(device))).reshape([1,-1])\n","  #     # base_case measurements not included\n","  #     # temp = torch.cat((masked_feats.to(device), mask_tn.to(device))).reshape([1, -1])\n","  #     # Simplification: notice feature vector size is halved. Update col variable accordingly.\n","  #     mask_tn, masked_feats = sensing_mask_alternate(feature_vec)\n","  #     # Not Needed. Adjust size of label vector.\n","  #     # masked_labels = encoded_labels[0].reshape([1])\n","  #     # print(encoded_labels.size())\n","  #     # assert False\n","  #     # for idx in range(2, encoded_labels.size()[0], 2) :\n","  #     #   masked_labels = torch.cat((masked_labels, encoded_labels[idx].reshape([1])))\n","  #     temp = masked_feats.reshape([1,-1])\n","  #     # encoded_labels = masked_labels\n","  #     # print(f'cat_data(), mask: enc_lab size {encoded_labels.size()}')\n","\n","  #   else :\n","  #     # Features (no mask)\n","  #     # Construct feature set from concatination of base_case and observed measuremnts.\n","  #     # temp = torch.cat((feature_vec.to(device), base_case.to(device))).reshape([1,-1])\n","  #     # I don't think this is doing anything.  Check if X_raw is changed by this. May already be a flat tensor after cat_net_attrs call.\n","  #     temp = feature_vec.reshape([1,-1])   # Tensor containing a tensor. Not sure that's needed.\n","  #   # print(f'cat_data(): temp {temp}')\n","\n","    # if X_cat is None:\n","    #   X_cat = temp\n","    #   # print(X_cat)\n","    # else:  \n","    #   X_cat = torch.cat((X_cat, temp))\n","    #   # print(X_cat)\n","\n","  # print(f'cat_data(): X_cat {X_cat.size()}')\n","  # print(f'cat_data(): X_cat {X_cat[0]}')\n","  # assert False\n","  # return X_cat, encoded_labels   # Not returning base_case at this time.\n","  return X_raw, encoded_labels, pipIdxs   # Not returning base_case at this time.\n","# print(f'output: {cat_data(residual=True, norm_base=False, norm_feats=False, mask=False)}')\n","# assert False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lxQz9XbUBgO"},"outputs":[],"source":["def cat_data_mult_tmstps(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], tmstps=None) :\n","  \"\"\"This func wraps cat_data() to concat traning samples from multiple time stamps.\n","  Notice tmstps is a list of time stamps to be included in the training set\n","      tmstps can include one time stamp to mimic previouis version, but must be in a list.\n","  \"\"\"\n","  X_raw, encoded_labels, pipIdxs = None, None, None\n","\n","  for tmstp in tmstps :\n","    X_r, enc_labs, pipIds = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","    if X_raw is None :\n","      X_raw = torch.cat([X_r])\n","      encoded_labels = torch.cat([enc_labs])\n","      pipIdxs = torch.cat([pipIds])\n","    else :\n","      X_raw = torch.cat([X_raw, X_r])\n","      encoded_labels = torch.cat([encoded_labels, enc_labs])\n","      pipIdxs = torch.cat([pipIdxs, pipIds])\n","  # print(f'X_raw {X_raw.size()}')\n","  # assert False\n","\n","  return X_raw, encoded_labels, pipIdxs"]},{"cell_type":"markdown","metadata":{"id":"CdawEj-SS9hP"},"source":["####Train the model"]},{"cell_type":"markdown","metadata":{"id":"j_Zwmb6sYpbm"},"source":["#####Set-up Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLehjrJ5SV8M"},"outputs":[],"source":["rows = 0\n","cols = 0\n","input_dim = 0\n","output_dim = 0\n","\n","tr_dataset = None\n","ts_dataset = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJJN6lECYvM_"},"outputs":[],"source":["from collections import Counter\n","import statistics as stats\n","\n","def histo_pipe_dist(labels) :\n","  \"\"\"Display histogram of leakpipe distribution.\n","  labels (tensor) : pytorch tensor of labels (ints).\n","  Note: requires matplotlib and Pandas.\n","  \"\"\"\n","  # Histo\n","  Y = pd.Series(labels.cpu())\n","  recounted = Counter(Y)\n","  print(recounted)\n","  std = stats.stdev(recounted.values())\n","  print(f'stdev {std:.2}')\n","  Y.plot.hist(grid=True, bins=10, alpha=0.7, rwidth=0.8, color='#607c8e', align='mid')\n","  plt.title(f'Label Frequency for {len(Y)} Samples')\n","  plt.xlabel('Label')\n","  plt.grid(axis='x')\n","  # plt.text(6, 200, r'class 5 = 229 (46%)')\n","  # assert False\n","  return std"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpisWMQEGjxL"},"outputs":[],"source":["def randomize_dataset(X, y) :\n","  random.seed(10343)\n","  ls = []\n","  for i in range(len(y)) :\n","    zipped = X[i], y[i]\n","    # print(zipped, end=\" \")\n","    ls.append(zipped)\n","  # print('randomize_dataset(): ls', len(ls))\n","  random.shuffle(ls)\n","  shuf_X = torch.empty([1, len(X[0])]).to(device)\n","  # print(shuf_X.size(), shuf_X)\n","  shuf_y = torch.empty([1], dtype=torch.long).to(device)\n","  # print(shuf_X.size(), shuf_X)\n","  for feat, label in ls :\n","    # print('randomize_dataset(): feat', feat.reshape([1, -1]).size())\n","    shuf_X = torch.cat((shuf_X, feat.reshape([1, -1])))\n","    # print('randomize_dataset(): label', label.reshape([1]).size())\n","    # label = torch.tensor(label, dtype=torch.long).to(device)\n","    shuf_y = torch.cat( (shuf_y, label.reshape([1])) )\n","  # print(shuf_X)\n","  # print(shuf_y)\n","  # print('randomize_dataset(): shuf_X', shuf_X.size())\n","  # print('randomize_dataset(): shuf_y', shuf_y.size())\n","  return shuf_X[1:], shuf_y[1:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfhI0Do7Ad_o"},"outputs":[],"source":["def find_seed(X, y, tr_size, tr_or_ts) :\n","  min_std = 1e9\n","  # tr_or_ts = 0  # tr = 0, ts = 1\n","  for i in range(20, 150) :\n","    print(i)\n","    subsets = torch.utils.data.random_split(TensorDataset(X, y),\n","                                            [tr_size, len(y) - tr_size],\n","                                            generator=torch.Generator().manual_seed(i),\n","                                            )\n","    # print(len(subsets),\n","    #       subsets[1].dataset.tensors[1].size(),\n","    #       type(subsets[1].indices),\n","    #       len(subsets[1].indices),\n","    #       subsets[1].dataset.tensors[1][[i for i in subsets[1].indices]],\n","    #       )\n","    std = histo_pipe_dist(subsets[tr_or_ts].dataset.tensors[1][[i for i in subsets[tr_or_ts].indices]])\n","    if std \u003c min_std :\n","      min_std = std\n","      seed = i\n","  print(f'Seed Found: argmin_stdev {seed}\\n')\n","  return seed"]},{"cell_type":"markdown","metadata":{"id":"qysp3xV5W2dg"},"source":["#####MNISTfashion set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1652893943014,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"v2G9BTikRI0k","outputId":"7b730efb-fc61-4c99-e223-93b2f1bb2d50"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ntr_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=True,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nts_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=False,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nrows = 28\\ncols = 28\\ninput_dim = rows*cols\\noutput_dim = 10\\n\\nlearning_rate = 1e-3\\nepochs = 10\\nbatch_size = 64\\nmod = 100\\n#'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# mnist\n","\"\"\"\n","tr_dataset = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","\n","ts_dataset = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","\n","rows = 28\n","cols = 28\n","input_dim = rows*cols\n","output_dim = 10\n","\n","learning_rate = 1e-3\n","epochs = 10\n","batch_size = 64\n","mod = 100\n","#\"\"\""]},{"cell_type":"markdown","metadata":{"id":"uBUCPLjMROO0"},"source":["#####CSV data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9181,"status":"ok","timestamp":1652893952193,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"sPNhrjauC1OW","outputId":"730d8432-e4d1-4c65-a6e3-aea7f49be79c"},"outputs":[{"name":"stdout","output_type":"stream","text":["SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 52.067344665527344, min -12.905021667480469\n","cat_data(): X_raw range 64.97236633300781\n","cat_data(): X_raw torch.Size([5000, 840])\n","SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 43.62644958496094, min -23.13990020751953\n","cat_data(): X_raw range 66.76634979248047\n","cat_data(): X_raw torch.Size([5000, 840])\n","SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 31.60552215576172, min -16.47759246826172\n","cat_data(): X_raw range 48.08311462402344\n","cat_data(): X_raw torch.Size([5000, 840])\n","SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 32.65496063232422, min -27.48834991455078\n","cat_data(): X_raw range 60.143310546875\n","cat_data(): X_raw torch.Size([5000, 840])\n","X size: torch.Size([20000, 840])\n","Counter({8: 1672, 6: 1621, 10: 1533, 9: 1529, 5: 1470, 7: 1440, 3: 1158, 2: 1075, 4: 994, 1: 793, 0: 715})\n","stdev 3.4e+02\n","cols 840\n","Autoencoder(\n","  (encoder): Encoder(\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (binary_STE_stack): Sequential(\n","      (0): StraightThroughEstimator()\n","    )\n","  )\n","  (decoder): Decoder(\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (linear_lrelu_stack): Sequential(\n","      (0): Linear(in_features=840, out_features=512, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=512, out_features=11, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n",")\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 432x288 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# Network characterist options:\n","#  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n","#   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n","### Centralize file paths; update them all from here; pass them along through func calls.\n","###   Hunting for things to update is tedious.\n","residual = True  # Subtract measured from base_case; 0 if normal behavior, else non-zero.\n","norm_base = False\n","norm_feats = True   \n","mask = False\n","net_char = 0\n","# tmstp = 80   # Delete if necessary\n","tmstps = [78, 80, 82, 84]\n","cat_attrs = True\n","subsample = False\n","if cat_attrs :\n","  net_char = [0, 4]\n","# Training data is loaded from csv file constructed in SimData_to_cvs script.\n","# For single tmstp:\n","# X, y, pipIdx = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","# For mult tmstp (requires list of tmstps):\n","X, y, pipIdx = cat_data_mult_tmstps(residual, norm_base, norm_feats, mask, net_char, tmstps)\n","# print(y)\n","# print(pipIdx)\n","print(f'X size: {X.size()}')\n","# torch.set_printoptions(edgeitems=50)\n","# print(f'Training Set: X[0] {X[0]}')\n","\n","if subsample :\n","  perc_of_meas = 0.01\n","  size = int(X.size(1) * perc_of_meas)\n","  print('subsample: size ', size)\n","  seed = 1001\n","  reduced_meas_X, __ = rand_sub_dataset(X, size, seed)\n","  print('subsample:', reduced_meas_X.size())\n","  # print(X[0, 307], X[0, 536], X[0, 329])\n","  # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n","  X = reduced_meas_X\n","\n","torch.set_printoptions(edgeitems=3)\n","tr_size = int(len(X)*0.7)\n","ts_size = len(y) - tr_size\n","# tr_dataset = TensorDataset(X[:split_idx], y[:split_idx])\n","# ts_dataset = TensorDataset(X[split_idx:], y[split_idx:])\n","# Find a seed that creates a training set pipe distribution with smallest stdev.\n","seed = 83   # find_seed prints and returns best seed.\n","# seed = find_seed(X, y, tr_size, 0)   # Commented out to eliminate the overhead of looking for a seed everytime.\n","# assert False\n","# Two ways to deal w/ pipe labels\n","#  1. add a third element to the datasets (done here)\n","#  2. make y a tuple and decompose in training/test loop\n","tr_dataset, ts_dataset = torch.utils.data.random_split(TensorDataset(X, y, pipIdx),\n","                                        [tr_size, ts_size],\n","                                        generator=torch.Generator().manual_seed(seed),\n","                                        )\n","\n","# Visualize the dataset leak pipe distribution.\n","# X, y = randomize_dataset(X, y)\n","# histo_pipe_dist(y[:split_idx])\n","# histo_pipe_dist(y[split_idx:])\n","idx = tr_dataset.indices\n","histo_pipe_dist(tr_dataset.dataset.tensors[1][[i for i in idx]])\n","\n","# Determine learning rate and measurement vector size\n","if net_char == 0:\n","  # link_flowrate\n","  cols = 444   # links = 444; junctions = 396\n","  # learning_rate = 9e-3   # single layer\n","  learning_rate = 8e-2\n","if net_char == 2:\n","  # link_velocity\n","  cols = 444   # links = 444; junctions = 396\n","  learning_rate = 2e-7\n","elif net_char == 3:\n","  # node_demand\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 2e-6\n","  epochs = 2000\n","elif net_char == 4:\n","  # node_head\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 7e-2\n","  epochs = 2000\n","elif net_char == 5:\n","  # node_pressure\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 1e-3\n","elif isinstance(net_char, list) :\n","  if norm_feats and cat_attrs :\n","    learning_rate = 2e-2\n","  else :\n","    learning_rate = 8e-2   #(f, h, nf, nh, f+h)\n","\n","# Determine number of concatenated vectors. Used for determining input_dim\n","concats = 1\n","if mask :\n","  concats = 1\n","\n","elif norm_feats :\n","  pass\n","# Simplification adjustments\n","cols = X.size(1)\n","print(f'cols {cols}')\n","\n","# Use region_dict to assign output_dim (i.e. # of regions)\n","rows = 1\n","input_dim = rows*cols*concats\n","output_dim = 11\n","# epochs = 100000\n","epochs = 7000   # For testing.\n","batch_size = 128\n","mod = 5\n","\n","# Instantiate model framework\n","# model = Decoder(input_dim, output_dim).to(device)\n","model = Autoencoder(input_dim, output_dim).to(device)\n","print(model)\n","# print(*model.parameters())\n","# print(f\"mask_params {model.state_dict()['encoder.mask_params'].size()}\")\n","# assert False"]},{"cell_type":"markdown","metadata":{"id":"cQdyRZRYXiQJ"},"source":["#####Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LItPDTO9LCoY"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.392752 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.471828 \n","\n","Epoch 27377\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.387450 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.514047 \n","\n","Epoch 27378\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.388695 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.528168 \n","\n","Epoch 27379\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.406194 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.572341 \n","\n","Epoch 27380\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.401495 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.531987 \n","\n","Epoch 27381\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.371173 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.511823 \n","\n","Epoch 27382\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.387275 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.553471 \n","\n","Epoch 27383\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.402645 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.603539 \n","\n","Epoch 27384\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.408523 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.526559 \n","\n","Epoch 27385\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.409552 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.579665 \n","\n","Epoch 27386\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.417222 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.566969 \n","\n","Epoch 27387\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.407760 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.610658 \n","\n","Epoch 27388\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.427555 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.598714 \n","\n","Epoch 27389\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.398705 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.570567 \n","\n","Epoch 27390\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.386328 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.575905 \n","\n","Epoch 27391\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.403671 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.587037 \n","\n","Epoch 27392\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.413470 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.559055 \n","\n","Epoch 27393\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.407514 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.517753 \n","\n","Epoch 27394\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.398156 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.560926 \n","\n","Epoch 27395\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.365822 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.521190 \n","\n","Epoch 27396\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.383340 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.544581 \n","\n","Epoch 27397\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.386088 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.550079 \n","\n","Epoch 27398\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.390291 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.594900 \n","\n","Epoch 27399\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.393525 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.545412 \n","\n","Epoch 27400\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.394841 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.504317 \n","\n","Epoch 27401\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.379197 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.556652 \n","\n","Epoch 27402\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.390682 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.543182 \n","\n","Epoch 27403\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.386174 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.502421 \n","\n","Epoch 27404\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.393383 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.547448 \n","\n","Epoch 27405\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.376931 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.499750 \n","\n","Epoch 27406\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.367302 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.532361 \n","\n","Epoch 27407\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.382616 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.480605 \n","\n","Epoch 27408\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.371234 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.525086 \n","\n","Epoch 27409\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.382158 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.494675 \n","\n","Epoch 27410\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.366031 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.489748 \n","\n","Epoch 27411\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.381068 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.490283 \n","\n","Epoch 27412\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.391566 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.505876 \n","\n","Epoch 27413\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.401850 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.559357 \n","\n","Epoch 27414\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.387294 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.525804 \n","\n","Epoch 27415\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.390837 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.532770 \n","\n","Epoch 27416\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.381374 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.517250 \n","\n","Epoch 27417\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.380421 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.555077 \n","\n","Epoch 27418\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.380083 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.523300 \n","\n","Epoch 27419\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.389069 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.508105 \n","\n","Epoch 27420\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.393918 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.531061 \n","\n","Epoch 27421\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.381575 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.550455 \n","\n","Epoch 27422\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.395832 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.510241 \n","\n","Epoch 27423\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.388336 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.496747 \n","\n","Epoch 27424\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.382867 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.532649 \n","\n","Epoch 27425\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.391622 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.474325 \n","\n","Epoch 27426\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.382197 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.517703 \n","\n","Epoch 27427\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.395075 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.548503 \n","\n","Epoch 27428\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.414676 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.584021 \n","\n","Epoch 27429\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.4%, Avg loss: 0.787082 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.655692 \n","\n","Epoch 27430\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.463402 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.545627 \n","\n","Epoch 27431\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.415256 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.606279 \n","\n","Epoch 27432\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.414442 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.583004 \n","\n","Epoch 27433\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.411402 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.536473 \n","\n","Epoch 27434\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.404047 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.474151 \n","\n","Epoch 27435\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.382343 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.458957 \n","\n","Epoch 27436\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.377336 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.481690 \n","\n","Epoch 27437\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.346450 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.513107 \n","\n","Epoch 27438\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.355595 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.508667 \n","\n","Epoch 27439\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.362575 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.499466 \n","\n","Epoch 27440\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.346187 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.482334 \n","\n","Epoch 27441\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.371532 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.482572 \n","\n","Epoch 27442\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.366153 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.512044 \n","\n","Epoch 27443\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.355543 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.486178 \n","\n","Epoch 27444\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.344944 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.471557 \n","\n","Epoch 27445\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.357261 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.467059 \n","\n","Epoch 27446\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.366806 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.473126 \n","\n","Epoch 27447\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.357949 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.491260 \n","\n","Epoch 27448\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.384253 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.491944 \n","\n","Epoch 27449\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.396064 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.480541 \n","\n","Epoch 27450\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.368115 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.510195 \n","\n","Epoch 27451\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.375099 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.457054 \n","\n","Epoch 27452\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.385011 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.557299 \n","\n","Epoch 27453\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.395693 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.498652 \n","\n","Epoch 27454\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.387065 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.526708 \n","\n","Epoch 27455\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.405172 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.548540 \n","\n","Epoch 27456\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.382407 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.551681 \n","\n","Epoch 27457\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.404153 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.549130 \n","\n","Epoch 27458\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.392170 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.494608 \n","\n","Epoch 27459\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.379194 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.561410 \n","\n","Epoch 27460\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.366372 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.507010 \n","\n","Epoch 27461\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.379463 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.528262 \n","\n","Epoch 27462\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.385568 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.506851 \n","\n","Epoch 27463\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.354955 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.503869 \n","\n","Epoch 27464\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.358209 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.508514 \n","\n","Epoch 27465\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.344666 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.483836 \n","\n","Epoch 27466\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.377783 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.503219 \n","\n","Epoch 27467\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.364334 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.481809 \n","\n","Epoch 27468\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.355050 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.486913 \n","\n","Epoch 27469\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.350364 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.491109 \n","\n","Epoch 27470\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.358646 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.464297 \n","\n","Epoch 27471\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.358093 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.486313 \n","\n","Epoch 27472\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.376672 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.478013 \n","\n","Epoch 27473\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.366423 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.487906 \n","\n","Epoch 27474\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.345390 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.499252 \n","\n","Epoch 27475\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.347863 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.444745 \n","\n","Epoch 27476\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.349119 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.467814 \n","\n","Epoch 27477\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.344674 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.468010 \n","\n","Epoch 27478\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.362512 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.480609 \n","\n","Epoch 27479\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.362738 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.541482 \n","\n","Epoch 27480\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.389919 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.523852 \n","\n","Epoch 27481\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.384923 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.509339 \n","\n","Epoch 27482\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.372565 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.468025 \n","\n","Epoch 27483\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.371252 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.499884 \n","\n","Epoch 27484\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.354657 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.569497 \n","\n","Epoch 27485\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.369677 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.484912 \n","\n","Epoch 27486\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.344451 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.471452 \n","\n","Epoch 27487\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.340255 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.484013 \n","\n","Epoch 27488\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.355246 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.467390 \n","\n","Epoch 27489\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.346057 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.484832 \n","\n","Epoch 27490\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.381879 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.571689 \n","\n","Epoch 27491\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.360793 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.483978 \n","\n","Epoch 27492\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.358902 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.468435 \n","\n","Epoch 27493\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.350973 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.488583 \n","\n","Epoch 27494\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.370110 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.508340 \n","\n","Epoch 27495\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.357796 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.490930 \n","\n","Epoch 27496\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.357835 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.479218 \n","\n","Epoch 27497\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.356774 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.480098 \n","\n","Epoch 27498\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.354121 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.487882 \n","\n","Epoch 27499\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.360620 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.457896 \n","\n","Epoch 27500\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.373237 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.518315 \n","\n","Epoch 27501\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.352980 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.475248 \n","\n","Epoch 27502\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.356826 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.493881 \n","\n","Epoch 27503\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.354821 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.498390 \n","\n","Epoch 27504\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.356588 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.504254 \n","\n","Epoch 27505\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.721922 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.501953 \n","\n","Epoch 27506\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.381489 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.485627 \n","\n","Epoch 27507\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.356017 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.455435 \n","\n","Epoch 27508\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.363406 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.534521 \n","\n","Epoch 27509\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.379144 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.493368 \n","\n","Epoch 27510\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.372368 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.518307 \n","\n","Epoch 27511\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.374256 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.513857 \n","\n","Epoch 27512\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.378180 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.518826 \n","\n","Epoch 27513\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.375288 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.507070 \n","\n","Epoch 27514\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.363908 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.492147 \n","\n","Epoch 27515\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.380674 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.482203 \n","\n","Epoch 27516\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.396819 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.476504 \n","\n","Epoch 27517\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.361407 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.502176 \n","\n","Epoch 27518\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.360345 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.493666 \n","\n","Epoch 27519\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.352105 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.488912 \n","\n","Epoch 27520\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.383026 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.499529 \n","\n","Epoch 27521\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.363091 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.501290 \n","\n","Epoch 27522\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.372014 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.515539 \n","\n","Epoch 27523\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.359393 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.474336 \n","\n","Epoch 27524\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.365370 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.474005 \n","\n","Epoch 27525\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.344630 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.565921 \n","\n","Epoch 27526\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.401770 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.499357 \n","\n","Epoch 27527\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.377270 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.543751 \n","\n","Epoch 27528\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.396174 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.502673 \n","\n","Epoch 27529\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.364528 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.518210 \n","\n","Epoch 27530\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.370938 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.496798 \n","\n","Epoch 27531\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.366618 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.462862 \n","\n","Epoch 27532\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.372216 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.507694 \n","\n","Epoch 27533\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.362223 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.499529 \n","\n","Epoch 27534\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.353100 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.479868 \n","\n","Epoch 27535\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.348299 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.448049 \n","\n","Epoch 27536\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.360088 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.474799 \n","\n","Epoch 27537\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.380353 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.465836 \n","\n","Epoch 27538\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.354544 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.477797 \n","\n","Epoch 27539\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.347913 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.478184 \n","\n","Epoch 27540\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.347719 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.500929 \n","\n","Epoch 27541\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.357904 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.509872 \n","\n","Epoch 27542\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.780947 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.573792 \n","\n","Epoch 27543\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.495432 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.535802 \n","\n","Epoch 27544\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.450032 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.538972 \n","\n","Epoch 27545\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.431178 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.535624 \n","\n","Epoch 27546\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.414458 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.488145 \n","\n","Epoch 27547\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.406712 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.465481 \n","\n","Epoch 27548\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.397065 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.501977 \n","\n","Epoch 27549\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.390060 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.524132 \n","\n","Epoch 27550\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.413125 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.513856 \n","\n","Epoch 27551\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.383024 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.510061 \n","\n","Epoch 27552\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.368940 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.519969 \n","\n","Epoch 27553\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.381342 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.517285 \n","\n","Epoch 27554\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.366120 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.476199 \n","\n","Epoch 27555\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.379939 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.533559 \n","\n","Epoch 27556\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.363094 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.529780 \n","\n","Epoch 27557\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.356413 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.479766 \n","\n","Epoch 27558\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.359797 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.500426 \n","\n","Epoch 27559\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.359366 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.496010 \n","\n","Epoch 27560\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.361225 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.545937 \n","\n","Epoch 27561\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.379121 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.540205 \n","\n","Epoch 27562\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.364618 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.500989 \n","\n","Epoch 27563\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.372649 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.534712 \n","\n","Epoch 27564\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.376415 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.508149 \n","\n","Epoch 27565\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.381105 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.556032 \n","\n","Epoch 27566\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.369425 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.510722 \n","\n","Epoch 27567\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.360310 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.525423 \n","\n","Epoch 27568\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.395579 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.534562 \n","\n","Epoch 27569\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.722574 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.673536 \n","\n","Epoch 27570\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.446338 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.632931 \n","\n","Epoch 27571\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.366741 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.564382 \n","\n","Epoch 27572\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.358759 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.521172 \n","\n","Epoch 27573\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.359889 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.554203 \n","\n","Epoch 27574\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.357893 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.489792 \n","\n","Epoch 27575\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.361227 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.574271 \n","\n","Epoch 27576\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.656858 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.555583 \n","\n","Epoch 27577\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.450389 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.606580 \n","\n","Epoch 27578\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.432113 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.617123 \n","\n","Epoch 27579\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.404600 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.909600 \n","\n","Epoch 27580\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.415340 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.557566 \n","\n","Epoch 27581\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.808997 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.575725 \n","\n","Epoch 27582\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 1.332196 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.570370 \n","\n","Epoch 27583\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.465142 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.608455 \n","\n","Epoch 27584\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.7%, Avg loss: 0.463196 \n","\n","Test Error: \n"," Accuracy: 82.0%, Avg loss: 0.623217 \n","\n","Epoch 27585\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.428392 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.611130 \n","\n","Epoch 27586\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.1%, Avg loss: 0.489690 \n","\n","Test Error: \n"," Accuracy: 81.6%, Avg loss: 0.642420 \n","\n","Epoch 27587\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.442967 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.601078 \n","\n","Epoch 27588\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.448771 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.530739 \n","\n","Epoch 27589\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.422669 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.574363 \n","\n","Epoch 27590\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.434569 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.581978 \n","\n","Epoch 27591\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.455507 \n","\n","Test Error: \n"," Accuracy: 82.5%, Avg loss: 0.632306 \n","\n","Epoch 27592\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.5%, Avg loss: 0.435567 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.571228 \n","\n","Epoch 27593\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.1%, Avg loss: 0.427382 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.541958 \n","\n","Epoch 27594\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.428243 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.575329 \n","\n","Epoch 27595\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.428300 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.541542 \n","\n","Epoch 27596\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.436384 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.523123 \n","\n","Epoch 27597\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.792183 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.586019 \n","\n","Epoch 27598\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.1%, Avg loss: 0.456072 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.579210 \n","\n","Epoch 27599\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.449271 \n","\n","Test Error: \n"," Accuracy: 83.0%, Avg loss: 0.630490 \n","\n","Epoch 27600\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.421329 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.540122 \n","\n","Epoch 27601\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.402727 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.525444 \n","\n","Epoch 27602\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.411890 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.525264 \n","\n","Epoch 27603\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.399609 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.542083 \n","\n","Epoch 27604\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.410048 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.538050 \n","\n","Epoch 27605\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.389964 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.500098 \n","\n","Epoch 27606\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.381539 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.562515 \n","\n","Epoch 27607\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.392406 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.475168 \n","\n","Epoch 27608\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.382329 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.529219 \n","\n","Epoch 27609\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.385349 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.509257 \n","\n","Epoch 27610\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.374220 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.537838 \n","\n","Epoch 27611\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.395577 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.502247 \n","\n","Epoch 27612\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.393903 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.499109 \n","\n","Epoch 27613\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.382097 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.526959 \n","\n","Epoch 27614\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.405345 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.551309 \n","\n","Epoch 27615\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.392027 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.535357 \n","\n","Epoch 27616\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.397887 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.575092 \n","\n","Epoch 27617\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.391360 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.574715 \n","\n","Epoch 27618\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.397139 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.504986 \n","\n","Epoch 27619\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.369477 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.529620 \n","\n","Epoch 27620\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.374964 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.496209 \n","\n","Epoch 27621\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.396440 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.538609 \n","\n","Epoch 27622\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.404023 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.576335 \n","\n","Epoch 27623\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.391234 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.545201 \n","\n","Epoch 27624\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.386549 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.528382 \n","\n","Epoch 27625\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.398972 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.565915 \n","\n","Epoch 27626\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.390294 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.514261 \n","\n","Epoch 27627\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.378641 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.484814 \n","\n","Epoch 27628\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.380488 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.497582 \n","\n","Epoch 27629\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.378215 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.517517 \n","\n","Epoch 27630\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.397558 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.538480 \n","\n","Epoch 27631\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.402245 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.571306 \n","\n","Epoch 27632\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.420308 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.565551 \n","\n","Epoch 27633\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.415815 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.570444 \n","\n","Epoch 27634\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.388134 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.511766 \n","\n","Epoch 27635\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.384512 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.504549 \n","\n","Epoch 27636\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.391723 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.508371 \n","\n","Epoch 27637\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.385262 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.566750 \n","\n","Epoch 27638\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.394606 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.544379 \n","\n","Epoch 27639\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.397631 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.540480 \n","\n","Epoch 27640\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.385170 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.484822 \n","\n","Epoch 27641\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.371877 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.514391 \n","\n","Epoch 27642\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.381649 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.546827 \n","\n","Epoch 27643\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.381896 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.531164 \n","\n","Epoch 27644\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.364642 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.478074 \n","\n","Epoch 27645\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.393268 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.514562 \n","\n","Epoch 27646\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.389532 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.492379 \n","\n","Epoch 27647\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.361141 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.497553 \n","\n","Epoch 27648\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.363305 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.486895 \n","\n","Epoch 27649\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.385973 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.505573 \n","\n","Epoch 27650\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.364291 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.483860 \n","\n","Epoch 27651\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.366549 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.520745 \n","\n","Epoch 27652\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.386858 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.498126 \n","\n","Epoch 27653\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.365796 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.509752 \n","\n","Epoch 27654\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.651404 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.679175 \n","\n","Epoch 27655\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.468453 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.777715 \n","\n","Epoch 27656\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.430777 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.520106 \n","\n","Epoch 27657\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.447356 \n","\n","Test Error: \n"," Accuracy: 82.1%, Avg loss: 0.642922 \n","\n","Epoch 27658\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.468282 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.595686 \n","\n","Epoch 27659\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.411245 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.567005 \n","\n","Epoch 27660\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.400102 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.529723 \n","\n","Epoch 27661\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.400242 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.494746 \n","\n","Epoch 27662\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.390185 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.556790 \n","\n","Epoch 27663\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.416335 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.567492 \n","\n","Epoch 27664\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.408072 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.562509 \n","\n","Epoch 27665\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.420100 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.566435 \n","\n","Epoch 27666\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.417070 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.532560 \n","\n","Epoch 27667\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.371440 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.534517 \n","\n","Epoch 27668\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.384108 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.532930 \n","\n","Epoch 27669\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.372635 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.552685 \n","\n","Epoch 27670\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.378765 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.509504 \n","\n","Epoch 27671\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.374083 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.543829 \n","\n","Epoch 27672\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.379501 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.567448 \n","\n","Epoch 27673\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.376030 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.530727 \n","\n","Epoch 27674\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.366182 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.470109 \n","\n","Epoch 27675\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.347340 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.470305 \n","\n","Epoch 27676\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.339597 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.481397 \n","\n","Epoch 27677\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.356165 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.487396 \n","\n","Epoch 27678\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.345960 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.546005 \n","\n","Epoch 27679\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.352093 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.474208 \n","\n","Epoch 27680\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.338532 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.452322 \n","\n","Epoch 27681\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.364336 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.440741 \n","\n","Epoch 27682\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.367789 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.461312 \n","\n","Epoch 27683\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.353581 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.459449 \n","\n","Epoch 27684\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.369013 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.459134 \n","\n","Epoch 27685\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.373284 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.508752 \n","\n","Epoch 27686\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.368471 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.504647 \n","\n","Epoch 27687\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.392129 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.480412 \n","\n","Epoch 27688\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.381668 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.490010 \n","\n","Epoch 27689\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.362836 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.551888 \n","\n","Epoch 27690\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.360447 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.569985 \n","\n","Epoch 27691\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.369347 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.460150 \n","\n","Epoch 27692\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.350023 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.521696 \n","\n","Epoch 27693\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.335556 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.465023 \n","\n","Epoch 27694\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.329858 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.440065 \n","\n","Epoch 27695\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.335598 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.478396 \n","\n","Epoch 27696\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.341111 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.473604 \n","\n","Epoch 27697\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.322440 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.452364 \n","\n","Epoch 27698\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.350771 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.456004 \n","\n","Epoch 27699\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.339692 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.497401 \n","\n","Epoch 27700\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.337383 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.466611 \n","\n","Epoch 27701\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.335010 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.449773 \n","\n","Epoch 27702\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.330643 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.467398 \n","\n","Epoch 27703\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.342645 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.476313 \n","\n","Epoch 27704\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.362441 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.491153 \n","\n","Epoch 27705\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.348879 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.482672 \n","\n","Epoch 27706\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.358701 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.444029 \n","\n","Epoch 27707\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.359944 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.467348 \n","\n","Epoch 27708\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.355974 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.455237 \n","\n","Epoch 27709\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.347787 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.487366 \n","\n","Epoch 27710\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.345306 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.472363 \n","\n","Epoch 27711\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.351195 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.468077 \n","\n","Epoch 27712\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.347338 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.476601 \n","\n","Epoch 27713\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.347201 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.519875 \n","\n","Epoch 27714\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.376522 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.550950 \n","\n","Epoch 27715\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.378300 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.583070 \n","\n","Epoch 27716\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.388492 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 1.508001 \n","\n","Epoch 27717\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.391962 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.533229 \n","\n","Epoch 27718\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.372008 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.543133 \n","\n","Epoch 27719\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.387089 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.514604 \n","\n","Epoch 27720\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.376370 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.550702 \n","\n","Epoch 27721\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.376625 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.522657 \n","\n","Epoch 27722\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.360852 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.509650 \n","\n","Epoch 27723\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.358843 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.482931 \n","\n","Epoch 27724\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.504501 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.497137 \n","\n","Epoch 27725\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.348315 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.522688 \n","\n","Epoch 27726\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.337788 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.461788 \n","\n","Epoch 27727\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.336878 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.536741 \n","\n","Epoch 27728\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.354572 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.439475 \n","\n","Epoch 27729\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.330125 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.479389 \n","\n","Epoch 27730\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.333821 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.493242 \n","\n","Epoch 27731\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.341492 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.481806 \n","\n","Epoch 27732\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.341155 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.487926 \n","\n","Epoch 27733\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.351041 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.494291 \n","\n","Epoch 27734\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.354460 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.472582 \n","\n","Epoch 27735\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.360271 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.504462 \n","\n","Epoch 27736\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.347797 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.485206 \n","\n","Epoch 27737\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.353355 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.515153 \n","\n","Epoch 27738\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.336754 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.497818 \n","\n","Epoch 27739\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.330264 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.478469 \n","\n","Epoch 27740\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.360938 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.464609 \n","\n","Epoch 27741\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.337944 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.488237 \n","\n","Epoch 27742\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.352931 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.485662 \n","\n","Epoch 27743\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.353217 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.523695 \n","\n","Epoch 27744\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.357164 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.519967 \n","\n","Epoch 27745\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.365566 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.506957 \n","\n","Epoch 27746\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.348339 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.474316 \n","\n","Epoch 27747\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.341279 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.492462 \n","\n","Epoch 27748\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.339683 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.485871 \n","\n","Epoch 27749\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.336199 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.494617 \n","\n","Epoch 27750\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.332544 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.475235 \n","\n","Epoch 27751\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.334919 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.517297 \n","\n","Epoch 27752\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.341854 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.499382 \n","\n","Epoch 27753\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.346847 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.500046 \n","\n","Epoch 27754\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.330532 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.472245 \n","\n","Epoch 27755\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.352727 \n","\n","Test Error: \n"," Accuracy: 86.0%, Avg loss: 0.460557 \n","\n","Epoch 27756\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.339659 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.482591 \n","\n","Epoch 27757\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.344490 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.470235 \n","\n","Epoch 27758\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.338473 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.480509 \n","\n","Epoch 27759\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.345067 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.518752 \n","\n","Epoch 27760\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.352103 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.474393 \n","\n","Epoch 27761\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.340438 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.494707 \n","\n","Epoch 27762\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.353996 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.462760 \n","\n","Epoch 27763\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.348913 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.448575 \n","\n","Epoch 27764\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.343710 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.466053 \n","\n","Epoch 27765\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.365159 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.504113 \n","\n","Epoch 27766\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.347454 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.503091 \n","\n","Epoch 27767\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.363717 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.511118 \n","\n","Epoch 27768\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.366408 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.555734 \n","\n","Epoch 27769\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.361059 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.531950 \n","\n","Epoch 27770\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.359883 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.544521 \n","\n","Epoch 27771\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.355627 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.519979 \n","\n","Epoch 27772\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.358819 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.547763 \n","\n","Epoch 27773\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.373106 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.500259 \n","\n","Epoch 27774\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.361006 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.585867 \n","\n","Epoch 27775\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.375259 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.503357 \n","\n","Epoch 27776\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.376174 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.541844 \n","\n","Epoch 27777\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.377677 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 0.505142 \n","\n","Epoch 27778\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.387921 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.521076 \n","\n","Epoch 27779\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.373164 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.529740 \n","\n","Epoch 27780\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.379301 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.505436 \n","\n","Epoch 27781\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.374680 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.513873 \n","\n","Epoch 27782\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.352079 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.523827 \n","\n","Epoch 27783\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.368176 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.502038 \n","\n","Epoch 27784\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.359875 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.473027 \n","\n","Epoch 27785\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.369285 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.500816 \n","\n","Epoch 27786\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.393455 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.498293 \n","\n","Epoch 27787\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.372807 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.483379 \n","\n","Epoch 27788\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 1.278570 \n","\n","Test Error: \n"," Accuracy: 82.7%, Avg loss: 2.381187 \n","\n","Epoch 27789\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.620263 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.465062 \n","\n","Epoch 27790\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.1%, Avg loss: 0.324375 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.436971 \n","\n","Epoch 27791\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.0%, Avg loss: 0.317565 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.439081 \n","\n","Epoch 27792\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.308894 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.429181 \n","\n","Epoch 27793\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.1%, Avg loss: 0.309099 \n","\n","Test Error: \n"," Accuracy: 86.0%, Avg loss: 0.425193 \n","\n","Epoch 27794\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.390685 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.709919 \n","\n","Epoch 27795\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.312971 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.422459 \n","\n","Epoch 27796\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.321177 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.462048 \n","\n","Epoch 27797\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.316308 \n","\n","Test Error: \n"," Accuracy: 86.2%, Avg loss: 0.430063 \n","\n","Epoch 27798\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.308286 \n","\n","Test Error: \n"," Accuracy: 85.9%, Avg loss: 0.425348 \n","\n","Epoch 27799\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.303706 \n","\n","Test Error: \n"," Accuracy: 86.2%, Avg loss: 0.447124 \n","\n","Epoch 27800\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.307170 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.431304 \n","\n","Epoch 27801\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.306293 \n","\n","Test Error: \n"," Accuracy: 85.9%, Avg loss: 0.436426 \n","\n","Epoch 27802\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.305123 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.426754 \n","\n","Epoch 27803\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.308152 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.445517 \n","\n","Epoch 27804\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.301683 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.430858 \n","\n","Epoch 27805\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.306758 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.436834 \n","\n","Epoch 27806\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.5%, Avg loss: 0.307141 \n","\n","Test Error: \n"," Accuracy: 85.9%, Avg loss: 0.437127 \n","\n","Epoch 27807\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.4%, Avg loss: 0.303530 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.415583 \n","\n","Epoch 27808\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.304649 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.430541 \n","\n","Epoch 27809\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.308912 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.439020 \n","\n","Epoch 27810\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.304509 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.445773 \n","\n","Epoch 27811\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.312141 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.447935 \n","\n","Epoch 27812\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.4%, Avg loss: 0.307466 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.452394 \n","\n","Epoch 27813\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.1%, Avg loss: 0.306417 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.433537 \n","\n","Epoch 27814\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.311757 \n","\n","Test Error: \n"," Accuracy: 85.9%, Avg loss: 0.417353 \n","\n","Epoch 27815\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.4%, Avg loss: 0.306501 \n","\n","Test Error: \n"," Accuracy: 85.9%, Avg loss: 0.437192 \n","\n","Epoch 27816\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.2%, Avg loss: 0.314157 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.442481 \n","\n","Epoch 27817\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.312172 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.423684 \n","\n","Epoch 27818\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.310762 \n","\n","Test Error: \n"," Accuracy: 86.0%, Avg loss: 0.439834 \n","\n","Epoch 27819\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.0%, Avg loss: 0.314314 \n","\n","Test Error: \n"," Accuracy: 86.5%, Avg loss: 0.420718 \n","\n","Epoch 27820\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.3%, Avg loss: 0.309229 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.465224 \n","\n","Epoch 27821\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.316268 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.442100 \n","\n","Epoch 27822\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.0%, Avg loss: 0.313630 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.448081 \n","\n","Epoch 27823\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.326212 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.447662 \n","\n","Epoch 27824\n","-------------------------------\n","Training Error: \n"," Accuracy: 87.0%, Avg loss: 0.315993 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.435238 \n","\n","Epoch 27825\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.318295 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.429082 \n","\n","Epoch 27826\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.326218 \n","\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.443373 \n","\n","Epoch 27827\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.317109 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.438016 \n","\n","Epoch 27828\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.9%, Avg loss: 0.326110 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.453700 \n","\n","Epoch 27829\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.331495 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.447502 \n","\n","Epoch 27830\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.340863 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.468363 \n","\n","Epoch 27831\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.325732 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.454335 \n","\n","Epoch 27832\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.338404 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.447327 \n","\n","Epoch 27833\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.330276 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.458069 \n","\n","Epoch 27834\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.336774 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.475859 \n","\n","Epoch 27835\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.355066 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.468433 \n","\n","Epoch 27836\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.367558 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.515803 \n","\n","Epoch 27837\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.360586 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.522344 \n","\n","Epoch 27838\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.363239 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.519141 \n","\n","Epoch 27839\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.359264 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.522181 \n","\n","Epoch 27840\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.351026 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.480264 \n","\n","Epoch 27841\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.335984 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.492734 \n","\n","Epoch 27842\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.346514 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.480031 \n","\n","Epoch 27843\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.344150 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.456136 \n","\n","Epoch 27844\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.344850 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.471407 \n","\n","Epoch 27845\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.357440 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.484627 \n","\n","Epoch 27846\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.348329 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.461505 \n","\n","Epoch 27847\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.336681 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.491207 \n","\n","Epoch 27848\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.409698 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.505886 \n","\n","Epoch 27849\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.333630 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.499164 \n","\n","Epoch 27850\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.327224 \n","\n","Test Error: \n"," Accuracy: 85.7%, Avg loss: 0.441708 \n","\n","Epoch 27851\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.334259 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.481984 \n","\n","Epoch 27852\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.328046 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.460711 \n","\n","Epoch 27853\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.331580 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.498735 \n","\n","Epoch 27854\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.341502 \n","\n","Test Error: \n"," Accuracy: 85.9%, Avg loss: 0.478923 \n","\n","Epoch 27855\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.327699 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.478497 \n","\n","Epoch 27856\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.8%, Avg loss: 0.331547 \n","\n","Test Error: \n"," Accuracy: 85.6%, Avg loss: 0.445771 \n","\n","Epoch 27857\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.5%, Avg loss: 0.333378 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.452765 \n","\n","Epoch 27858\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.7%, Avg loss: 0.332898 \n","\n","Test Error: \n"," Accuracy: 85.3%, Avg loss: 0.483407 \n","\n","Epoch 27859\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.335510 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.463429 \n","\n","Epoch 27860\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.6%, Avg loss: 0.338757 \n","\n","Test Error: \n"," Accuracy: 85.5%, Avg loss: 0.478710 \n","\n","Epoch 27861\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.351597 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.470348 \n","\n","Epoch 27862\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.4%, Avg loss: 0.337795 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.482270 \n","\n","Epoch 27863\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.346566 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.474525 \n","\n","Epoch 27864\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.359140 \n","\n","Test Error: \n"," Accuracy: 85.4%, Avg loss: 0.475059 \n","\n","Epoch 27865\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.3%, Avg loss: 0.353597 \n","\n","Test Error: \n"," Accuracy: 85.0%, Avg loss: 0.499192 \n","\n","Epoch 27866\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.360302 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.480250 \n","\n","Epoch 27867\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.370058 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.497259 \n","\n","Epoch 27868\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.382946 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.515833 \n","\n","Epoch 27869\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.372546 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.514308 \n","\n","Epoch 27870\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.367970 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.510204 \n","\n","Epoch 27871\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.353591 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.467951 \n","\n","Epoch 27872\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.364815 \n","\n","Test Error: \n"," Accuracy: 85.2%, Avg loss: 0.514636 \n","\n","Epoch 27873\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.384439 \n","\n","Test Error: \n"," Accuracy: 84.3%, Avg loss: 1.174214 \n","\n","Epoch 27874\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.378032 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.534928 \n","\n","Epoch 27875\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.2%, Avg loss: 0.366204 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.471173 \n","\n","Epoch 27876\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.1%, Avg loss: 0.362476 \n","\n","Test Error: \n"," Accuracy: 84.9%, Avg loss: 0.482142 \n","\n","Epoch 27877\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.9%, Avg loss: 0.362285 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.519461 \n","\n","Epoch 27878\n","-------------------------------\n","Training Error: \n"," Accuracy: 86.0%, Avg loss: 0.348498 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.515491 \n","\n","Epoch 27879\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.390020 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.564192 \n","\n","Epoch 27880\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.401729 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.517321 \n","\n","Epoch 27881\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.374614 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.497490 \n","\n","Epoch 27882\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.7%, Avg loss: 0.377523 \n","\n","Test Error: \n"," Accuracy: 85.1%, Avg loss: 0.511412 \n","\n","Epoch 27883\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.366551 \n","\n","Test Error: \n"," Accuracy: 84.8%, Avg loss: 0.499393 \n","\n","Epoch 27884\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.385607 \n","\n","Test Error: \n"," Accuracy: 84.6%, Avg loss: 0.564980 \n","\n","Epoch 27885\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.403861 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.616569 \n","\n","Epoch 27886\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.433187 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.622609 \n","\n","Epoch 27887\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.439502 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.558830 \n","\n","Epoch 27888\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.454502 \n","\n","Test Error: \n"," Accuracy: 82.9%, Avg loss: 0.659998 \n","\n","Epoch 27889\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.439398 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.584457 \n","\n","Epoch 27890\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.427194 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.538535 \n","\n","Epoch 27891\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.5%, Avg loss: 0.435942 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.543195 \n","\n","Epoch 27892\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.428863 \n","\n","Test Error: \n"," Accuracy: 82.7%, Avg loss: 0.581030 \n","\n","Epoch 27893\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.434641 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.600714 \n","\n","Epoch 27894\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.425938 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.591423 \n","\n","Epoch 27895\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.422560 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.511300 \n","\n","Epoch 27896\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.406528 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.554630 \n","\n","Epoch 27897\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.5%, Avg loss: 0.429953 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.538444 \n","\n","Epoch 27898\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.5%, Avg loss: 0.684933 \n","\n","Test Error: \n"," Accuracy: 82.5%, Avg loss: 0.693404 \n","\n","Epoch 27899\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.436597 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.539691 \n","\n","Epoch 27900\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.5%, Avg loss: 0.449685 \n","\n","Test Error: \n"," Accuracy: 83.3%, Avg loss: 0.604582 \n","\n","Epoch 27901\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.447367 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.597785 \n","\n","Epoch 27902\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.9%, Avg loss: 0.452116 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.579543 \n","\n","Epoch 27903\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.456041 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.650529 \n","\n","Epoch 27904\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.424210 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.571828 \n","\n","Epoch 27905\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.420443 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.537220 \n","\n","Epoch 27906\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.3%, Avg loss: 0.408122 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.527560 \n","\n","Epoch 27907\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.408932 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.564544 \n","\n","Epoch 27908\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.400498 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.631499 \n","\n","Epoch 27909\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.420471 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.526770 \n","\n","Epoch 27910\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.403150 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.565413 \n","\n","Epoch 27911\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.8%, Avg loss: 0.391815 \n","\n","Test Error: \n"," Accuracy: 84.5%, Avg loss: 0.532840 \n","\n","Epoch 27912\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.396419 \n","\n","Test Error: \n"," Accuracy: 84.7%, Avg loss: 0.519060 \n","\n","Epoch 27913\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.392221 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.541695 \n","\n","Epoch 27914\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.395544 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.583054 \n","\n","Epoch 27915\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.9%, Avg loss: 0.403460 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.550236 \n","\n","Epoch 27916\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.396249 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.544777 \n","\n","Epoch 27917\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.398364 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.553565 \n","\n","Epoch 27918\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.419491 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.520880 \n","\n","Epoch 27919\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.396210 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.588184 \n","\n","Epoch 27920\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.390222 \n","\n","Test Error: \n"," Accuracy: 84.0%, Avg loss: 0.506820 \n","\n","Epoch 27921\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.426102 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.563381 \n","\n","Epoch 27922\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.418780 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.546656 \n","\n","Epoch 27923\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.394034 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.519954 \n","\n","Epoch 27924\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.384070 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.548439 \n","\n","Epoch 27925\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.4%, Avg loss: 0.393676 \n","\n","Test Error: \n"," Accuracy: 84.4%, Avg loss: 0.519063 \n","\n","Epoch 27926\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.399843 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.527734 \n","\n","Epoch 27927\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.5%, Avg loss: 0.380643 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.544968 \n","\n","Epoch 27928\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.399564 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.580301 \n","\n","Epoch 27929\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.6%, Avg loss: 0.384323 \n","\n","Test Error: \n"," Accuracy: 83.8%, Avg loss: 0.564952 \n","\n","Epoch 27930\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.422677 \n","\n","Test Error: \n"," Accuracy: 83.6%, Avg loss: 0.622690 \n","\n","Epoch 27931\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.400832 \n","\n","Test Error: \n"," Accuracy: 83.9%, Avg loss: 0.522597 \n","\n","Epoch 27932\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.410379 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.556759 \n","\n","Epoch 27933\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.401632 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.550282 \n","\n","Epoch 27934\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.404313 \n","\n","Test Error: \n"," Accuracy: 82.9%, Avg loss: 0.534738 \n","\n","Epoch 27935\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.401507 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.507120 \n","\n","Epoch 27936\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.412608 \n","\n","Test Error: \n"," Accuracy: 84.1%, Avg loss: 0.532665 \n","\n","Epoch 27937\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.385261 \n","\n","Test Error: \n"," Accuracy: 82.7%, Avg loss: 0.543080 \n","\n","Epoch 27938\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.2%, Avg loss: 0.401810 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.935733 \n","\n","Epoch 27939\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.5%, Avg loss: 0.411181 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.564112 \n","\n","Epoch 27940\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.412740 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.509506 \n","\n","Epoch 27941\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.419053 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.596112 \n","\n","Epoch 27942\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.428550 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.583131 \n","\n","Epoch 27943\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.427842 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.610016 \n","\n","Epoch 27944\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.427834 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.577451 \n","\n","Epoch 27945\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.434267 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.557614 \n","\n","Epoch 27946\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.5%, Avg loss: 0.441756 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.550266 \n","\n","Epoch 27947\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.2%, Avg loss: 0.478023 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.569917 \n","\n","Epoch 27948\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.7%, Avg loss: 0.465807 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.574090 \n","\n","Epoch 27949\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.7%, Avg loss: 0.445660 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.571583 \n","\n","Epoch 27950\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.5%, Avg loss: 0.477139 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.634131 \n","\n","Epoch 27951\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.8%, Avg loss: 0.478699 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.655428 \n","\n","Epoch 27952\n","-------------------------------\n","Training Error: \n"," Accuracy: 81.6%, Avg loss: 0.628179 \n","\n","Test Error: \n"," Accuracy: 81.0%, Avg loss: 0.730406 \n","\n","Epoch 27953\n","-------------------------------\n","Training Error: \n"," Accuracy: 81.1%, Avg loss: 0.606764 \n","\n","Test Error: \n"," Accuracy: 79.8%, Avg loss: 0.717999 \n","\n","Epoch 27954\n","-------------------------------\n","Training Error: \n"," Accuracy: 81.5%, Avg loss: 0.573756 \n","\n","Test Error: \n"," Accuracy: 79.9%, Avg loss: 0.737703 \n","\n","Epoch 27955\n","-------------------------------\n","Training Error: \n"," Accuracy: 81.9%, Avg loss: 0.549837 \n","\n","Test Error: \n"," Accuracy: 80.5%, Avg loss: 0.664109 \n","\n","Epoch 27956\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.1%, Avg loss: 0.538578 \n","\n","Test Error: \n"," Accuracy: 82.0%, Avg loss: 0.673608 \n","\n","Epoch 27957\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.7%, Avg loss: 0.520201 \n","\n","Test Error: \n"," Accuracy: 81.3%, Avg loss: 0.699636 \n","\n","Epoch 27958\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.3%, Avg loss: 0.518634 \n","\n","Test Error: \n"," Accuracy: 81.7%, Avg loss: 0.606990 \n","\n","Epoch 27959\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.1%, Avg loss: 0.536071 \n","\n","Test Error: \n"," Accuracy: 81.1%, Avg loss: 0.653830 \n","\n","Epoch 27960\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.8%, Avg loss: 0.488295 \n","\n","Test Error: \n"," Accuracy: 82.0%, Avg loss: 0.643159 \n","\n","Epoch 27961\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.3%, Avg loss: 0.492686 \n","\n","Test Error: \n"," Accuracy: 82.0%, Avg loss: 0.617886 \n","\n","Epoch 27962\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.0%, Avg loss: 0.482027 \n","\n","Test Error: \n"," Accuracy: 81.9%, Avg loss: 0.598371 \n","\n","Epoch 27963\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.3%, Avg loss: 0.638352 \n","\n","Test Error: \n"," Accuracy: 80.8%, Avg loss: 0.664459 \n","\n","Epoch 27964\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.0%, Avg loss: 0.538032 \n","\n","Test Error: \n"," Accuracy: 81.8%, Avg loss: 0.691782 \n","\n","Epoch 27965\n","-------------------------------\n","Training Error: \n"," Accuracy: 82.7%, Avg loss: 0.497826 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.606420 \n","\n","Epoch 27966\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.1%, Avg loss: 0.488609 \n","\n","Test Error: \n"," Accuracy: 81.8%, Avg loss: 0.655088 \n","\n","Epoch 27967\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.2%, Avg loss: 0.477333 \n","\n","Test Error: \n"," Accuracy: 82.1%, Avg loss: 0.634930 \n","\n","Epoch 27968\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.2%, Avg loss: 0.474042 \n","\n","Test Error: \n"," Accuracy: 82.1%, Avg loss: 0.665163 \n","\n","Epoch 27969\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.1%, Avg loss: 0.498168 \n","\n","Test Error: \n"," Accuracy: 82.5%, Avg loss: 0.678809 \n","\n","Epoch 27970\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.3%, Avg loss: 0.473213 \n","\n","Test Error: \n"," Accuracy: 83.7%, Avg loss: 0.618520 \n","\n","Epoch 27971\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.6%, Avg loss: 0.456240 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.628521 \n","\n","Epoch 27972\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.438769 \n","\n","Test Error: \n"," Accuracy: 83.0%, Avg loss: 0.625716 \n","\n","Epoch 27973\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.438928 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.679025 \n","\n","Epoch 27974\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.8%, Avg loss: 0.438768 \n","\n","Test Error: \n"," Accuracy: 82.7%, Avg loss: 0.597500 \n","\n","Epoch 27975\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.8%, Avg loss: 0.451654 \n","\n","Test Error: \n"," Accuracy: 82.7%, Avg loss: 0.578834 \n","\n","Epoch 27976\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.429315 \n","\n","Test Error: \n"," Accuracy: 82.3%, Avg loss: 0.596209 \n","\n","Epoch 27977\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.435526 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.559438 \n","\n","Epoch 27978\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.429954 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.593720 \n","\n","Epoch 27979\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.3%, Avg loss: 0.433859 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.583315 \n","\n","Epoch 27980\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.425552 \n","\n","Test Error: \n"," Accuracy: 82.9%, Avg loss: 0.587043 \n","\n","Epoch 27981\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.416068 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.544784 \n","\n","Epoch 27982\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.397247 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.610633 \n","\n","Epoch 27983\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.7%, Avg loss: 0.412516 \n","\n","Test Error: \n"," Accuracy: 82.9%, Avg loss: 0.615476 \n","\n","Epoch 27984\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.426326 \n","\n","Test Error: \n"," Accuracy: 83.0%, Avg loss: 0.607938 \n","\n","Epoch 27985\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.6%, Avg loss: 0.398554 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.590398 \n","\n","Epoch 27986\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.413100 \n","\n","Test Error: \n"," Accuracy: 83.2%, Avg loss: 0.578758 \n","\n","Epoch 27987\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.422624 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.558410 \n","\n","Epoch 27988\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.436669 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.599154 \n","\n","Epoch 27989\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.432982 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.571066 \n","\n","Epoch 27990\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.4%, Avg loss: 0.430708 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.639239 \n","\n","Epoch 27991\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.5%, Avg loss: 0.481216 \n","\n","Test Error: \n"," Accuracy: 83.1%, Avg loss: 0.571010 \n","\n","Epoch 27992\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.9%, Avg loss: 0.459961 \n","\n","Test Error: \n"," Accuracy: 82.8%, Avg loss: 0.613894 \n","\n","Epoch 27993\n","-------------------------------\n","Training Error: \n"," Accuracy: 83.9%, Avg loss: 0.453481 \n","\n","Test Error: \n"," Accuracy: 83.0%, Avg loss: 0.592215 \n","\n","Epoch 27994\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.440853 \n","\n","Test Error: \n"," Accuracy: 82.6%, Avg loss: 0.584258 \n","\n","Epoch 27995\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.0%, Avg loss: 0.424514 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.583109 \n","\n","Epoch 27996\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.422839 \n","\n","Test Error: \n"," Accuracy: 82.4%, Avg loss: 0.586375 \n","\n","Epoch 27997\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.2%, Avg loss: 0.423443 \n","\n","Test Error: \n"," Accuracy: 83.5%, Avg loss: 0.574159 \n","\n","Epoch 27998\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.0%, Avg loss: 0.394408 \n","\n","Test Error: \n"," Accuracy: 83.4%, Avg loss: 0.524116 \n","\n","Epoch 27999\n","-------------------------------\n","Training Error: \n"," Accuracy: 85.1%, Avg loss: 0.400746 \n","\n","Test Error: \n"," Accuracy: 83.0%, Avg loss: 0.539526 \n","\n","Epoch 28000\n","-------------------------------\n","Training Error: \n"," Accuracy: 84.8%, Avg loss: 0.392537 \n","\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 1.519396 \n","\n","Done!\n"]}],"source":["# Initialize the loss function\n","#  nn.CrossEntropyLoss() encapsulates nn.LogSoftmax and nn.NLLLoss\n","loss_fn = nn.CrossEntropyLoss()\n","# Parameter adjustment protocol\n","# He: always start w/ Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# For plotting results\n","animator = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0.0, 1.0],\n","                        legend=['train loss', 'train acc', 'test acc'])\n","binMaskSizeLs = []\n","\n","# Load pipe_to_pipeidx numpy file (for use w/ confussion matrix)\n","# dir_nm = 'drive/MyDrive/Colab Notebooks/Water Distribution Network/Samplers'\n","dir_nm = 'drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/leak_pipes_all/tmstp80/regionAndPipeLabels/11regions/00/'\n","file_nm = 'dictPipeToPipeIdx.npy'\n","load_loc = dir_nm + file_nm\n","pToPIdx_dic = np.load(load_loc, allow_pickle='TRUE').item()\n","# print(f'pToPIdx dict: {pToPIdx_dic}')\n","# print(len(pToPIdx_dic))   # dicts have lengths.\n","# assert False\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=False)\n","    # for x,y in train_dataloader:\n","    #   print(x,y)\n","    #   break\n","    tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, loss_fn, optimizer, pToPIdx_dic, epoch=t+1, mod=mod)\n","    # tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, loss_fn, optimizer, epoch=t+1, mod=mod)\n","    train_metrics = (tr_loss, __)\n","\n","    # Disaggregate data -- save models; note epoch\n","    # if t \u003e 2000 and tr_loss \u003e prevLoss * 10 :\n","    #   print(f'train_loop(): epoch {t} -- loss jumped from {prevLoss:.3} to {tr_loss:.3}')\n","    #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{t}.pt')\n","    # prevLoss = tr_loss\n","\n","    test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n","    test_acc, confusion_matrix = test_loop(test_dataloader, model, loss_fn, pToPIdx_dic, out_dim=output_dim)\n","    # animator\n","    animator.add(t + 1, train_metrics + (test_acc,))\n","torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Analysis/display_of_sensor_grid.pt')\n","# Not sure the following block is necessary\n","# train_loss, train_acc = train_metrics\n","# assert train_loss \u003c 0.5, train_loss\n","# assert train_acc \u003c= 1 and train_acc \u003e 0.7, train_acc\n","# assert test_acc \u003c= 1 and test_acc \u003e 0.7, test_acc\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1gq9jLK5xBSxk3G12Eub3ZJ-9LfqQR-dw"},"id":"2gTMxTXWcdMz","outputId":"d6d8587a-6f0e-41e1-8948-ce05f03493e2"},"outputs":[],"source":["# %matplotlib inline\n","\n","animator.display_plt()\n","# Save to file that is replaced on every run.\n","loss_filenm = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Analysis/loss.png'\n","animator.fig.savefig(loss_filenm, bbox_inches='tight')\n","# Automate improved filename description.\n","# animator.fig.savefig('loss.png', bbox_inches='tight')\n","# plt.savefig('loss.png', bbox_inches='tight')   # Less specific. Targets active figure.\n","\n","# Pipe labels (strings) are located in simdata_to_csv notebook\n","# predictions = [f'{i}' for i in range(output_dim)]\n","predictions = decode_labels()   # Update decode_labels list when leakpipe set has changed.\n","# print(predictions)\n","# labels = range(output_dim)\n","# labels = decode_labels()\n","# print(ts_dataset[:][2])\n","# print(len(ts_dataset[:][2]))\n","# temp = [x.item() for x in ts_dataset[:][2]]\n","# print(temp)\n","# print(len(temp))\n","# labels = sorted(set(temp))\n","labels = [i for i in range(len(pToPIdx_dic))]\n","# print(labels)\n","# print(len(labels))\n","fig, ax = plt.subplots(1, 1, figsize=(100,100))\n","# assert False\n","# fig.set_facecolor('#7d7f7c')\n","im = ax.imshow(confusion_matrix)\n","ax.set_xticks(np.arange(len(predictions)))\n","ax.set_yticks(np.arange(len(labels)))\n","ax.set_xticklabels(predictions)\n","ax.set_yticklabels(labels)\n","\n","# Set-up for white grid lines on minor ticks. Creates spacing effect.\n","ax.set_xticks(np.arange(len(predictions)+1) - 0.5, minor=True)\n","ax.set_yticks(np.arange(len(labels)+1) - 0.5, minor=True)\n","# Print white grid to space out the squares.\n","ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n","# Remove spines for clarity.\n","for k, v in ax.spines.items() :\n","  v.set_visible(False)\n","# ax.spines['top'].set_visible(False)   # Can't slice a dictionary.\n","ax.tick_params(which=\"minor\", bottom=False, left=False)\n","\n","# Horizontal labeling displays on top\n","ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n","# Rotate tick labels and set alignment.\n","plt.setp(ax.get_xticklabels(), rotation=-45, ha='right', rotation_mode='anchor')\n","plt.xlabel(f'Predictions -- {ts_size}')\n","# Move the x labels to the top\n","ax.xaxis.set_label_position('top')\n","plt.ylabel('Labels')\n","# Annotate matrix with values by looping over data dimensions\n","for i in range(len(labels)) :\n","  for j in range(len(predictions)) :\n","    text = ax.text(j, i, confusion_matrix[i, j].item(),\n","                   ha='center', va='center', color='white')\n","\n","ax.set_title(f'Confusion Matrix -- Epoch {t+1}')\n","fig.tight_layout()\n","# Save to file that is replaced on every run.\n","conf_filenm = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Analysis/conf.png'\n","fig.savefig(conf_filenm)\n","conf_mat_filenm = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Analysis/conf_mat.pt'\n","torch.save(confusion_matrix, conf_mat_filenm)\n","# plt.show()\n","# plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EcjWmtFdRdsZ"},"outputs":[],"source":["#torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/midTrainingModel.pt')"]},{"cell_type":"markdown","metadata":{"id":"4W6b5qaI8ZVj"},"source":["####Sanity Check: Pass a sample to the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Gp6zYEZN8mx5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Evaluation\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-28-56cc4abdd59a\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Scenario {samp_idx} (pred={preds.item()}, label={y.item()})'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m \u001b[0mpredict_ch3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamp_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-28-56cc4abdd59a\u003e\u001b[0m in \u001b[0;36mpredict_ch3\u001b[0;34m(net, sample, samp_idx)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Predict labels (redefined from d2l Chapter 3.6.7).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["def predict_ch3(net, sample, samp_idx=0):\n","    \"\"\"Predict labels (redefined from d2l Chapter 3.6.7).\"\"\"\n","    print('Model Evaluation')\n","    X, y = sample[samp_idx]\n","    X = X.reshape([1,-1])\n","    preds = net(X).argmax(axis=1)\n","    print(f'Scenario {samp_idx} (pred={preds.item()}, label={y.item()})')\n","\n","predict_ch3(model, tr_dataset, samp_idx=28)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rv4iOGftxLZi"},"outputs":[],"source":["# Test trained model on time stamps it hasn't seen before\n","# tmstp = 168\n","# X, y = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","# ts_dataset = TensorDataset(X, y)\n","\n","# test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n","# test_acc = test_loop(test_dataloader, model, loss_fn)"]},{"cell_type":"markdown","metadata":{"id":"k5xwqcQyNoBV"},"source":["####Scratch Work"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VeYlNlR6WH-P"},"outputs":[],"source":["# catting to an empty tensor -- doesn't work\n","ls = [1, 2, 3]\n","tsr = torch.Tensor(ls)\n","print(tsr)\n","tsr1 = torch.ones([2,2])\n","print(tsr1)\n","tsr = torch.concat([tsr1])\n","print(tsr)\n","test = None\n","if test is None :\n","  print('is None!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OCehwyn6imcp"},"outputs":[],"source":["print(0.991 \u003e 0.99)\n","x = torch.rand(5)\n","print(x)\n","y = torch.where(x \u003e 0.6, x, torch.tensor(0.))\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kGxSZV4cDLu7"},"outputs":[],"source":["import math\n","x = float('nan')\n","print(x)\n","print(not math.isnan(x))\n","print(1 + 0.0 * x)\n","print(torch.log(torch.tensor(0.0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-f8VELnBsjNg"},"outputs":[],"source":["x_tup = ([2], [3], [4])\n","# x_tup = (2, 3, 4)\n","x_tup[1][0] -= x_tup[0][0]\n","# x_tup[1] -= x_tup[0]\n","# print(x_tup[1] - x_tup[0])\n","print(x_tup[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HKcdDEqt7LVH"},"outputs":[],"source":["t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n","print(t)\n","t1 = torch.rand(10, generator=torch.Generator().manual_seed(11))\n","print(t1)\n","print(t[t1 \u003e 0.5])   # returns a tensor containing only those elems for which test returns true.\n","t[t1 \u003e 0.5] = 0   # Assigns zero to only those elems for which the test returns true.\n","print(t)\n","print(t[t1 \u003e 0.5])\n","print(t[0])   # Returns a zero dim tensor. (num)\n","print(t[0:1])   # Returns a 1-dim tensor. ([num])\n","print(t[0:1].new(1))\n","print(t[0:1].new_empty(1).uniform_())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j9aks4N-SHt5"},"outputs":[],"source":["t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n","print(' t:', t)\n","idxs = torch.randperm(5, generator=torch.Generator().manual_seed(10))\n","print(idxs)\n","tn = t[idxs]\n","print(' t:', t)\n","print('tn:', tn)\n","print(' t:', t[2])\n","print('tn:', tn[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kpjdHiXWtwOl"},"outputs":[],"source":["y_hat = torch.arange(20).reshape([2, -1])\n","print(y_hat)\n","len(y_hat)\n","print(y_hat.sum(1))\n","print(y_hat.argmax(dim=0))\n","print(y_hat.argmax(dim=1))\n","data = [[1.0, 1.0], [1.0, 1.0]] * 2   # multiplies the number of elements (like if you had 2 apples and then multiplied them by 2; you now have four apples)\n","print(data)\n","# print(data / data)   # dividing a list is not defined\n","(1,2) + (3,)   # cats the three elems\n","len((1,2))   # tuples have __len__ defined\n","[[] for _ in range(3)]\n","rows = [[1,1]]\n","print(rows)\n","[rows.append(i) for i in [[3,3],[4,4]]]\n","print(rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MPQBiMXCOahN"},"outputs":[],"source":["# X_masked = None\n","# masked_feats = torch.rand(15)\n","# print(masked_feats)\n","# mask = torch.randint(2, [15])\n","# print(mask)\n","# # Mask and masked features\n","# # May want sensing_mask_rand() to process batches of samples\n","# for i in range(5):\n","#   temp = torch.cat((masked_feats, mask)).reshape([1,-1])\n","#   print(temp)\n","\n","#   if X_masked is None:\n","#     X_masked = temp   \n","#     print(X_masked)\n","#   else:  \n","#     X_masked = torch.cat((X_masked, temp))\n","#     print(X_masked)\n","\n","# for i in X_masked:\n","#   print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i4kiAKeKfBIp"},"outputs":[],"source":["# size = [100,]\n","# K = 20\n","# tn = torch.zeros(size)\n","# mask = torch.zeros(tn.size())\n","# print(mask.size())\n","# print(mask)\n","# indices = torch.randint(len(tn), size=(K,))\n","# print(indices)\n","# for idx in indices:\n","#   mask[idx] = 1\n","# print(mask)\n","\n","# mask = torch.cuda.FloatTensor(3, 3).uniform_()\n","# # tensor of floats\n","# mask = torch.FloatTensor(3,3).uniform_()\n","# print(mask)\n","# # tensor of booleans (?? how ??)\n","# mask = torch.FloatTensor(3,3).uniform_() \u003e 0.8\n","# print(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NxvQSxfCAX_R"},"outputs":[],"source":["# converting string labs to labels ranging from 0 -\u003e num_of_classes (i.e. possible leak locations)\n","#  what if not all of the possible leak locations are used?\n","#  1) I can set the output dim to len of label_subset (easiser)\n","#  2) I can force the set to be all the possible fixed pipe locations (coordinating this will be tricky)\n","# labs = [1,2,2,3,1,4,4,3]\n","# lab_dict = {}\n","# encoded_labs = []\n","# label_subset = set(labs)\n","# print(label_subset)\n","# print(type(label_subset))\n","# print(len(label_subset))\n","# for i, key in enumerate(label_subset):\n","#   lab_dict[key] = i\n","# print(lab_dict)\n","# for key in labs:\n","#   encoded_labs.append(lab_dict[key])\n","# print(encoded_labs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"snpJEOWKoc5Y"},"outputs":[],"source":["# Reshaping practice\n","# base_file = 'simdata/_base_/node_demand.csv'\n","# data_file = base_file\n","# data = pd.read_csv(data_file)\n","# data_tn = torch.tensor(data.values, dtype=torch.float32)\n","# data_tn[:,1:].reshape([1,-1])\n","\n","# data_tn = torch.arange(20).reshape([4,5])\n","# print(data_tn)\n","# data_tn = data_tn[:,1:].reshape([1,-1])\n","# print(data_tn)\n","# data_tn1 = torch.arange(20).reshape([4,5])\n","# print(data_tn1)\n","# data_tn1 = data_tn1[:,1:].reshape([1, -1])\n","# print(data_tn1)\n","# torch.cat((data_tn1, data_tn))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2DGW06k6V-qI"},"outputs":[],"source":["# Extracting an intelligible answer from the model\n","# x = torch.arange(16, dtype=torch.float32).reshape((4,4))\n","# print(x)\n","# print(x.sum(axis=0))\n","# print(x.sum(axis=[0,1]))\n","# mean = x.sum() / x.numel()\n","# print(mean)\n","# # notice we keep all dims (tensor of a tensor ie. two brackets) vs above we lost one (just a tensor)\n","# print(x.sum(dim=0, keepdim=True))\n","\n","# y = torch.tensor([3,3,3,3])\n","# # x.argmax(1).type(y.dtype) == y\n","# correct = 0\n","# correct += (x.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","# correct"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kndCfp0DOku8"},"outputs":[],"source":["# Handy timer class\n","class Timer:\n","    \"\"\"Record multiple running times.\"\"\"\n","    def __init__(self):\n","        self.times = []\n","        self.start()\n","\n","    def start(self):\n","        \"\"\"Start the timer.\"\"\"\n","        self.tik = time.time()\n","\n","    def stop(self):\n","        \"\"\"Stop the timer and record the time in a list.\"\"\"\n","        self.times.append(time.time() - self.tik)\n","        return self.times[-1]\n","\n","    def avg(self):\n","        \"\"\"Return the average time.\"\"\"\n","        return sum(self.times) / len(self.times)\n","\n","    def sum(self):\n","        \"\"\"Return the sum of time.\"\"\"\n","        return sum(self.times)\n","\n","    def cumsum(self):\n","        \"\"\"Return the accumulated time.\"\"\"\n","        return np.array(self.times).cumsum().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"m3m7r6ps_rDM"},"outputs":[],"source":["# Target transform\n","from torchvision.transforms import Lambda\n","\n","train_size = 700\n","# target_transform = Lambda(lambda y: torch.zeros(\n","#     (train_size, output_dim), dtype=torch.float).scatter_(\n","#         dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))\n","\n","# one-hot classification label vector\n","target_transform = Lambda(lambda y: torch.scatter_(\n","        dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNJsrezgMpd4V6Kznm1hhkj","background_execution":"on","collapsed_sections":["4Epm5SJkrzLx","VQzqACaz6D71","ydc9rlm86Hmt","3ocO0Om3ofeX","RVZsPo1p2RoP","TmFOoA-k258k","5wCj-uKc561c","7KkFT5x93F5H","j_Zwmb6sYpbm","qysp3xV5W2dg"],"mount_file_id":"15D6yoW7Wd8ZvWBpPLw4BzhZN7jfMwatJ","name":"bernoulli5_11regions_mult_tmstps.ipynb","provenance":[{"file_id":"1YeSJUnijlqIt0y5eDayQvwNGRwSKSkYZ","timestamp":1649218750773},{"file_id":"13Yy2CsIHuUcYXGOkYgQOdktXEL2CHOFJ","timestamp":1639297478820},{"file_id":"1pSJ226BsrXMVoAQJ0BKrxwgrHyxGHvI5","timestamp":1636667778711},{"file_id":"1VHyXaGHoAm4NI3ahzbXhGxODiO4mg4AM","timestamp":1632939473633},{"file_id":"19ehUGgFEEdAcgFs-fy_SW0vkxJggJ5XW","timestamp":1631899911015},{"file_id":"1_vt4FQCGh7KSHkU7GKarP807bTNxTZV7","timestamp":1631190383013},{"file_id":"1Nf2Ay7YXjx6JBvs9gsySlwirTOO-Ownd","timestamp":1628629596966},{"file_id":"1pp1nL2XUKNxN0QscNo-VzAeWMS07sl53","timestamp":1627669468832},{"file_id":"1JCW99kbx6_NEJS5KKhCRGP9tz5YE3eCZ","timestamp":1627178038295},{"file_id":"1Hhtzwkvq30pxSybpIcmD4Wb4eRB0bVmO","timestamp":1626718176246},{"file_id":"1rFofuDkzfAOVxLTpBsYt28pthUMCMtOL","timestamp":1626651085984},{"file_id":"19eoMaxuZB18-ZWdJZdgDr1MD6qAJR0L3","timestamp":1626329222475}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}