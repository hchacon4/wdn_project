{"cells":[{"cell_type":"markdown","metadata":{"id":"dtgm9rRGYGRY"},"source":["###SimData to csv\n","Multiple timestamps, one stamp per traning sample\n","\n","row 0: expected basecase values (no label)  \n","row 1 -\u003e dset_size: residual values, one scenario per row"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5356,"status":"ok","timestamp":1654042168551,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"IBaskhZkY1lI","outputId":"0b4dabcf-fe5c-4b8e-b9c4-0cfcf2bb8652"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n"]}],"source":["# NOTE: no authentication needed for mount if only one person edits the notebook.\n","# I can read files directly from the drive locations (i.e. no need to copy all files to local setting)\n","\n","import pandas as pd\n","import torch\n","import pathlib\n","import numpy as np\n","\n","import pickle\n","\n","# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using {} device\".format(device))"]},{"cell_type":"markdown","metadata":{"id":"9bR7V14JV1Ng"},"source":["####Basecase and Observed Data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1654042168941,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"hnQJDki3YeZc"},"outputs":[],"source":["# Note whether sim has been run in debug mode before running this script.\n","def features_ds(base_file, simdata_dir, dset_size=150, net_charac=0, time_stamp=80, debug=False):\n","    \"\"\"\n","    Write desired features for all samples to csv file.\n","\n","    net_charac (int): network characteristic of interest.\n","        will determine simdata target file e.g. 0 -\u003e link_flowrate\n","    \"\"\"\n","    # Load _base_ data\n","    # Create dictionary of hdf file data keys\n","    charac_dict = {}\n","    with pd.HDFStore(base_file) as hdf:\n","      for key, value in enumerate(hdf.keys()):\n","        charac_dict[key] = value\n","    #   print(hdf.keys())\n","    #   assert False\n","    # output (debug=True):\n","    #  ['/link_flowrate', '/link_headloss', '/link_velocity', '/node_demand', '/node_head', '/node_pressure']\n","    # print(charac_dict)\n","    \n","    data = pd.read_hdf(base_file, charac_dict[net_charac])   # hdf using less memory; set sim file_format='hdf'\n","                                                      # hdf -\u003e Hierarchical Data Format; storage and manipulation of scientific data across diverse operating systems and machines\n","    print(charac_dict[net_charac])\n","    # print(data.head())\n","    # print(data.columns)\n","    # Trim time stamp labels in first column (Is this a good idea ?? ans: yes)\n","    basecase = torch.tensor(data.values, dtype=torch.float32)\n","    print(f'features_ds(): basecase {basecase.size()}')\n","    basecase = basecase[time_stamp:time_stamp+1].reshape([-1]).to(device)  # Wed, 8-9am (80hrs into sim) (assuming sim starts Sun, 12a)\n","    # print(f'features_ds(): basecase\\n{basecase}')\n","    # print(f'features_ds(): basecase -- max {max(basecase)}, min {min(basecase)}')\n","    print(f'features_ds(): basecase {basecase.size()}')\n","\n","    # Load leak scenarios\n","    p = pathlib.Path(simdata_dir)\n","    sim_dirs = sorted(x for x in p.iterdir() if x.is_dir())\n","    # print(sim_dirs)\n","\n","    # Construct hdf file list\n","    hdf_files = []\n","    for x in sim_dirs[:-1]:\n","        hdf_files += sorted(str(p) for p in pathlib.Path(x).glob(\"*.h5\"))\n","    # print(hdf_files)\n","\n","    observs = None\n","    # func? sets correct net_charac and inc conditioned on debug setting\n","    # start= net_charac   # targeting the head (pressure) file of each scenario\n","    # if debug:\n","    #   inc = 9\n","    # else:\n","    #   inc = 4   # I'd like to make this value dynamic according to file count in scenario folders.\n","    #             #  Either 9 or 4 depending on debug on/off, respectively.\n","\n","    # Tensor of sample tensors\n","    #  What if I construct a dict w/ keys: network characteristic, value: list files of charact from ea scenario?\n","    for i, h_file in enumerate(hdf_files[:dset_size]):\n","        if i % 100 == 0:\n","          print(h_file)\n","        # # Data file keys (debug=True):\n","        # #  ['/leak_demand', '/leak_head', '/leak_pressure', '/link_flowrate', \n","        # #   '/link_headloss', '/link_velocity', '/node_demand', '/node_head', '/node_pressure']\n","        # # NOTE: the first three are inaccessible using current design.\n","        data = pd.read_hdf(h_file, charac_dict[net_charac])\n","        meas_tn = torch.tensor(data.values, dtype=torch.float32)\n","        # print(meas_tn.size())\n","        # assert False\n","        # Trim time stamp label in first column and reshape\n","        #  Consider pushing time one day\n","        meas_tn = meas_tn[time_stamp:time_stamp+1].reshape([1,-1]).to(device)\n","        # print(meas_tn)\n","\n","        if observs is None:\n","            observs = torch.cat((meas_tn,))\n","            # print(observs)\n","        else:\n","            observs = torch.cat((observs, meas_tn))\n","            # print(observs)\n","\n","    # return only used for debug\n","    return observs, data.columns, basecase"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1654042168942,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"wOqY0JJigH3x"},"outputs":[],"source":["# Debug block\n","def debug_features_ds():\n","  # net_charac options:\n","  #  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n","  #   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n","  net_charac = 0\n","  base_file = 'data.h5'\n","  base_file = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/SimData/temp_test_10k_hdf/_base_/'+base_file\n","  simdata_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/SimData/temp_test_10k_hdf'\n","  # dest_dir = \n","  set_size = 500\n","  debug = False\n","  X, cols, basecase = features_ds(base_file, simdata_dir,set_size, net_charac, debug)\n","  print('END')\n","  print(basecase.size())\n","  # print(X)\n","  print(X.size())\n","# debug_features_ds()"]},{"cell_type":"markdown","metadata":{"id":"lTHhoweoV_SR"},"source":["####Labels"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1654042168942,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"B8miczvDTemh"},"outputs":[],"source":["# See scratch section for original four-region version of region_ls()\n","# Note: reg_dict is made in graph_partition notebook.\n","# def region_dict(regdict_dir=None) :   # where dir_path=dest_dir from main cell. This may help enforce the dict location.\n","def region_dict(regdict_filepath=None) :   # where dir_path=dest_dir from main cell. This may help enforce the dict location.\n","  # # Careful: the expected partition may have been overwritten by graph_partition notebook.\n","  # dir_path = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/'\n","  # regdict_dir = dir_path + 'Input Pipeline/Datasets/leak_pipes_all/39regions/00/'\n","  # partitions = 39\n","  # file_nm = f'region_dict_{partitions}.pickle'\n","  # # load_loc = dir_path + version + file_nm\n","  # load_loc = regdict_dir + file_nm\n","  load_loc = regdict_filepath\n","\n","  # For loading\n","  with open(load_loc, 'rb') as handle:\n","      reg_dict = pickle.load(handle)\n","  return reg_dict\n","\n","# dt = region_dict()\n","# print(dt)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":669,"status":"ok","timestamp":1654042169608,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"l5f4KYA5ZRoD"},"outputs":[],"source":["# need full list of labels (strings)\n","# encode labels\n","\n","def labels_ds(base_jfile, simdata_dir, regdict_dir, regdict_filepath, dset_size=150, debug=False):\n","    # Base network: get full set of pipes\n","    data_file = base_jfile\n","    data = pd.read_json(data_file)\n","    pipe_set = data['links']['pipes']   # returns a list of all pipes in network.\n","    # print(pipe_set)\n","\n","    # Encode full set of pipes (keys: strings (pipe names), values: ints (arbitrary assignment of unique int to pipe names))\n","    lab_dict = {}\n","    for i, key in enumerate(pipe_set):\n","        lab_dict[key] = i\n","    # print(lab_dict)\n","\n","    # Fetch label files\n","    info_jsons = []\n","    p = pathlib.Path(simdata_dir)\n","    sim_dirs = sorted(x for x in p.iterdir() if x.is_dir())\n","    # print(sim_dirs)\n","    # Construct json file list\n","    for x in sim_dirs[:-1]:\n","        info_jsons += sorted(str(p) for p in pathlib.Path(x).glob(\"*.json\"))\n","    print(info_jsons)\n","\n","    # Construct list of all training labels used in this simdata_dir collection (strings); labels not necessarily unique.\n","    labels = []\n","    for i, j_file in enumerate(info_jsons[:dset_size]):\n","        if i % 100 == 0:\n","          print(j_file)\n","        j_data = pd.read_json(j_file)\n","        labels.append(j_data['leak_pipes'][0])\n","    # print(labels)\n","\n","    # Encode training labels (restrict encoding to label subset)\n","    reg_dict = region_dict(regdict_filepath)   # Lists used for encoding pipes to respective regions.\n","    # regions = region_ls()   # tuple containing lists used for encoding pipes to respective regions.\n","    encoder = {}\n","    encoded_labels = []\n","    pipe_idxs = []\n","    labels_set = sorted(set(labels))   # sorted() returns a sorted list formed from the unordered elements of labels set.\n","    # labels_set.sort()\n","    # print(labels_set)\n","    # print(len(labels_set))\n","\n","    # Construct subset of region pipes containing only existing labels from that region\n","    # Do this for every region\n","    # Used to determine lengths when constructing the label encoder.\n","    # Strategy: construct a dict of lists -- a given list contains subset of pipes from a region\n","    reg_lab_subsets = {}\n","    regions = reg_dict['reg_partits']   # dict of lists containing the all pipes in a given region.\n","    # print(next(regions.values()))\n","    for i, reg in enumerate(regions.values()) :\n","      reg_lab_subsets[i] = []\n","      for label in labels_set :\n","        if label in reg :\n","          reg_lab_subsets[i].append(label)\n","    # print(reg_lab_subsets)\n","\n","    # Alt encoder construction\n","    # Construct dict from labels_set for encoding labels; key=Pipe, value=(region, assigned pipe int)\n","    pip_idx = 0\n","    encoder[f'reg_lens'] = []\n","    for reg_idx in reg_lab_subsets :   # reg_lab_subsets keys are ints (reg_idx)\n","      # reg_labels = reg_lab_subsets[reg_idx]\n","      for i, label in enumerate(reg_lab_subsets[reg_idx]) :\n","        encoder[label] = reg_idx, pip_idx\n","        pip_idx += 1\n","      encoder[f'reg_lens'].append(i+1)   # at this point, i+1 is the length of reg_idx region.\n","    encoder[f'lengths'] = (reg_idx+1, pip_idx)\n","    # Much cleaner. Also scales w/ num of regions.\n","    # Include keys for Key:'lengths', Val: (pip_ct, reg_ct), Key:'reg_lens', Val: list of region lens\n","    # print(encoder)\n","\n","    #  Construct list of encoded labels (in order of generated training data)\n","    for label in labels:\n","      encoded_labels.append(encoder[label][0])\n","      pipe_idxs.append(encoder[label][1])\n","    # print(encoded_labels)\n","    # print(pipe_idxs)\n","    print(f'lens should match: {len(encoded_labels)} =? {len(pipe_idxs)}')\n","    # assert False\n","\n","    # Save encoder (using numpy) for use in training model\n","    #  encoder is general to a given edge partition. (place in parent folder of any subordinate space e.g. tmstp)\n","    #  Key: pipe_nm (str), Val: (region_index, pipe_index)\n","    #  Add Key: 'lengths', Val: (pipe_ct, reg_ct) to encoder; for use w/ conf_mat in sampler models.\n","    dir_nm = regdict_dir   # place regdict_dir is as general a folder as possible -- edge partition level.\n","    file_nm = 'dictPipeToPipeIdx.npy'\n","    save_loc = dir_nm + file_nm\n","    np.save(save_loc, encoder)\n","\n","    return encoded_labels, pipe_idxs, labels_set"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1654042169609,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"t0nhsL3HhsoI"},"outputs":[],"source":["# Debug block\n","def debug_labels_ds():\n","  # This test function will overwrite the existing dictPipToPipeIdx.npy file. Otherwise, read only.\n","  simdata_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/SimData/leakpipesAll/temp_test_5k_unif_area0.01_0.1_hdf/'\n","  base_jfile = '_base_/info.json'\n","  base_jfile = simdata_dir + base_jfile \n","  set_size = 5000\n","  debug = True\n","  enc_labs, pipe_idxs, lab_dict = labels_ds(base_jfile, simdata_dir, set_size, debug)\n","  print(enc_labs)   #list of sample labels\n","  print(lab_dict)   #map of entire label set (all possible); key: label string, value: mapped integer\n","  assert False\n","\n","  # classes = []\n","  # for i, lab in enumerate(labs):\n","  #   classes.append(i)\n","  # classes = torch.arange(len(labs)).to(device)\n","  # print(classes)\n","# debug_labels_ds()\n","# labels from leakpipesAll: ['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P1022', 'P1023', 'P1024', 'P1025', 'P1026', 'P1027', 'P1028', 'P1029', 'P103', 'P1030', 'P1031', 'P1032', 'P1033', 'P1034', 'P1035', 'P1036', 'P1039', 'P104', 'P1040', 'P1041', 'P1042', 'P1044', 'P1045', 'P106', 'P107', 'P108', 'P109', 'P11', 'P110', 'P111', 'P112', 'P113', 'P115', 'P116', 'P117', 'P118', 'P119', 'P12', 'P120', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P129', 'P13', 'P130', 'P131', 'P132', 'P134', 'P136', 'P138', 'P139', 'P14', 'P140', 'P141', 'P142', 'P144', 'P147', 'P148', 'P15', 'P150', 'P154', 'P155', 'P156', 'P157', 'P158', 'P159', 'P16', 'P160', 'P161', 'P162', 'P163', 'P165', 'P166', 'P17', 'P174', 'P177', 'P18', 'P184', 'P19', 'P195', 'P2', 'P20', 'P201', 'P21', 'P211', 'P215', 'P218', 'P219', 'P22', 'P220', 'P223', 'P225', 'P228', 'P23', 'P230', 'P231', 'P233', 'P234', 'P235', 'P237', 'P238', 'P24', 'P241', 'P242', 'P243', 'P245', 'P246', 'P248', 'P249', 'P25', 'P251', 'P252', 'P255', 'P256', 'P258', 'P259', 'P26', 'P264', 'P266', 'P267', 'P268', 'P27', 'P270', 'P272', 'P275', 'P28', 'P280', 'P282', 'P284', 'P285', 'P286', 'P287', 'P288', 'P29', 'P290', 'P291', 'P292', 'P293', 'P294', 'P295', 'P296', 'P297', 'P298', 'P299', 'P3', 'P30', 'P301', 'P302', 'P303', 'P304', 'P305', 'P307', 'P308', 'P309', 'P31', 'P310', 'P316', 'P319', 'P32', 'P320', 'P322', 'P323', 'P329', 'P33', 'P330', 'P331', 'P336', 'P337', 'P338', 'P339', 'P34', 'P340', 'P341', 'P343', 'P344', 'P346', 'P347', 'P348', 'P349', 'P35', 'P350', 'P37', 'P372', 'P374', 'P375', 'P376', 'P378', 'P379', 'P38', 'P380', 'P381', 'P383', 'P384', 'P385', 'P386', 'P39', 'P397', 'P398', 'P399', 'P40', 'P402', 'P403', 'P409', 'P410', 'P42', 'P424', 'P43', 'P44', 'P443', 'P445', 'P446', 'P450', 'P46', 'P465', 'P467', 'P468', 'P48', 'P482', 'P484', 'P49', 'P492', 'P5', 'P500', 'P501', 'P502', 'P51', 'P510', 'P52', 'P524', 'P527', 'P529', 'P53', 'P54', 'P55', 'P57', 'P58', 'P596', 'P597', 'P6', 'P609', 'P610', 'P63', 'P633', 'P64', 'P65', 'P67', 'P670', 'P671', 'P68', 'P69', 'P697', 'P7', 'P70', 'P71', 'P72', 'P724', 'P725', 'P752', 'P753', 'P754', 'P755', 'P756', 'P757', 'P758', 'P759', 'P760', 'P761', 'P763', 'P766', 'P767', 'P768', 'P769', 'P771', 'P772', 'P775', 'P776', 'P777', 'P779', 'P780', 'P781', 'P783', 'P784', 'P785', 'P786', 'P787', 'P788', 'P789', 'P791', 'P794', 'P795', 'P796', 'P797', 'P798', 'P8', 'P800', 'P801', 'P804', 'P805', 'P806', 'P807', 'P808', 'P809', 'P810', 'P811', 'P813', 'P815', 'P817', 'P819', 'P821', 'P822', 'P823', 'P826', 'P827', 'P83', 'P830', 'P831', 'P84', 'P840', 'P841', 'P842', 'P844', 'P846', 'P847', 'P85', 'P850', 'P851', 'P852', 'P853', 'P855', 'P858', 'P859', 'P86', 'P861', 'P866', 'P87', 'P871', 'P880', 'P889', 'P89', 'P892', 'P9', 'P90', 'P91', 'P914', 'P915', 'P92', 'P924', 'P927', 'P929', 'P930', 'P931', 'P932', 'P933', 'P934', 'P935', 'P937', 'P938', 'P939', 'P94', 'P940', 'P941', 'P942', 'P943', 'P944', 'P946', 'P947', 'P948', 'P949', 'P95', 'P951', 'P953', 'P954', 'P955', 'P956', 'P957', 'P958', 'P959', 'P96', 'P961', 'P962', 'P963', 'P964', 'P965', 'P966', 'P967', 'P968', 'P969', 'P97', 'P970', 'P971', 'P972', 'P973', 'P974', 'P975', 'P976', 'P977', 'P978', 'P98', 'P981', 'P982', 'P983', 'P984', 'P986', 'P987', 'P988', 'P989', 'P99', 'P990', 'P991', 'P992', 'P993', 'P994', 'P995', 'P996', 'P997', 'P998', 'P999']"]},{"cell_type":"markdown","metadata":{"id":"MVC27tg63-zK"},"source":["####Subset"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1654042169609,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"P18-ojI14D9k"},"outputs":[],"source":["def subset(ds_file, pipe_count):\n","  # I want to be able to train on fewer than the total number of leak pipes\n","  pass\n","# subset()"]},{"cell_type":"markdown","metadata":{"id":"WAlYJ07_DuOB"},"source":["####SimData to csv"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":232,"status":"ok","timestamp":1654042169838,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"YH_44ECCvbhz"},"outputs":[],"source":["# File Paths -- Centralize all file paths (inputs and output) in this function.\n","def feat_lab_args(net_char, time_stamp) :\n","  ### !!! UPDATE when creating new csv file from SimData !!! ###\n","  # Network characterist options:\n","  #  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n","  #   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n","  net_charac = net_char   # Consider removing. Not used here.\n","\n","  ## Inputs\n","  ### NOTE: Update file paths for different SimData\n","  dir_path = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/'\n","  simdata_dir = 'SimData/leakpipesAll/'\n","  data_dir = 'temp_test_5k_unif_area0.01_0.1_hdf/'\n","  ## ---------------------------- ##\n","\n","  base_file = dir_path + simdata_dir + data_dir + '_base_/data.h5'\n","  base_jfile = dir_path + simdata_dir + data_dir  + '_base_/info.json'\n","  simdata_dir = dir_path + simdata_dir + data_dir\n","  \n","  ## Outputs\n","  ### UDATE ... partitions\n","  # Region dict path + filenm (for use in labels_ds() function)\n","  partitions = 30\n","  version_dir = f'Input Pipeline/Datasets/leak_pipes_all/{partitions}regions/00/'\n","  regdict_dir = dir_path + version_dir\n","  file_nm = f'region_dict_{partitions}.pickle'\n","  regdict_filepath = regdict_dir + file_nm\n","\n","  ### UPDATE ...\n","  dest_dir = dir_path + version_dir + f'tmstp{time_stamp}/'\n","  set_size = 5000\n","  # set_size = 100   # For testing.\n","  debug = True\n","  fname_note = f'_area0.01_0.1'\n","  ## ---------------------------- ##\n","\n","  # return net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir, dest_dir, set_size, time_stamps, debug, fname_note\n","  return (net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir,\n","          regdict_dir, regdict_filepath, dest_dir, set_size, time_stamp, debug, fname_note,)\n","# feat_lab_args(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1t7Vr4w2_hT7j0SVF63TkaLi0NH58BVNA"},"id":"QSICEBxxvlDP","outputId":"73de9dfd-f784-49af-8aaa-2400408fab92"},"outputs":[],"source":["# def SimData_to_csv(net_char):\n","def SimData_to_csv(net_char, tmstp):\n","  # Network characterist options:\n","  #  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n","  #   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n","\n","  # Update time_stamp to time_stamps list (start w/ three times stamps)\n","  # (net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir, dest_dir, set_size, time_stamps, debug, fname_note) = feat_lab_args(net_char)\n","  # (net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir, dest_dir, set_size, time_stamp, debug, fname_note) = feat_lab_args(net_char)\n","  (net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir,\n","   regdict_dir, regdict_filepath, dest_dir, set_size, time_stamp, debug, fname_note,) = feat_lab_args(net_char, tmstp)\n","\n","  # mult_tm_ds = pd.DataFrame()\n","  # for i in range(3) :\n","  #   fl_ds = pd.DataFrame({\"A\": range(3), \"B\": range(3)})\n","  #   # mult_tm_ds = pd.concat([mult_tm_ds, fl_ds], ignore_index=True)\n","  #   # Use ignore_index if repeat indices becomes a problem\n","  #   mult_tm_ds = pd.concat([mult_tm_ds, fl_ds], ignore_index=True)\n","  # print(mult_tm_ds)\n","\n","  # Note: Block moved here to support saving region dict and others to dest_dir\n","  # Create destination directory if does not exist.\n","  p = pathlib.Path(dest_dir)\n","  # print(dest_dir)\n","  # print(p)\n","  # print(p.exists())\n","  if not p.exists() :\n","    # Parent dir must exist. i.e. all but the new folder in the line of folders must already exist.\n","    # p.mkdir()\n","    p.mkdir(parents=True)\n","  # Think deeply about where the region dict should be placed. Currently located w/ the csv files, but notice the same region dict might be used by multiple csv training data as is the case here.\n","  #  idea: make the regionxx dir a parent of tmstp dir. place the region dict in the regionxx folder as that dict works for that specific instance of xx regions. Can see a problem in that there may exist multiple e.g. region11 arangements. Need to resolve this.\n","  # print(p)\n","  # assert False\n","  \n","  ##### Time stamp for loop code\n","  # mult_tm_ds = pd.DataFrame()\n","  # for time_stamp in time_stamps :\n","  #####\n","\n","  #### Logic block -- Construct data frame containing training set\n","  # one strat is to append subsequent dataframes to the previous\n","  file_ds = pd.DataFrame()\n","  observs, data_cols, basecase = features_ds(base_file, simdata_dir, set_size, net_charac, time_stamp, debug)\n","  print(f'SimData_to_csv(): observs {observs.size()}')\n","  # print(type(data_cols))\n","\n","  # Labels\n","  encoded_targets, pipe_idxs, _lab_set_ = labels_ds(base_jfile, simdata_dir, regdict_dir, regdict_filepath, set_size, debug)\n","  # encoded_targets contains a tuple of (region, pipe_idx)\n","\n","  # Write observs and basecase to csv file\n","  # Column labels\n","  col_labels = data_cols\n","  # row 0: basecase\n","  rows = basecase.reshape([1,-1]).tolist()\n","  # row 1-\u003edset_size: scenario observed measurements\n","  [rows.append(observed) for observed in observs.tolist()]\n","  # print(rows)\n","  file_ds = pd.DataFrame(rows, columns=col_labels)\n","  # print(file_ds.head())\n","  # print(*pipe_idxs)\n","  # Add label and pipeIdx columns to dataframe\n","  file_ds['PipeIdx'] = (-1, *pipe_idxs)\n","  file_ds['Label'] = (-1, *encoded_targets)\n","  #   Alt method:\n","  # label_column = {'Label': encoded_targets}   # First create dict.\n","  # print(label_column)\n","  # file_ds.append(label_column, ignore_index=True)   # Append dict to dataframe.\n","  # print(file_ds)   # Notice n + 1 rows  where n = training samples.\n","  ####\n","\n","  #####\n","    # mult_tm_ds = pd.concat([mult_tm_ds, file_ds])\n","  # Consider adding a col w/ tmstp info\n","  # print(mult_tm_ds)\n","  # assert False\n","  #####\n","\n","  # Write dataframe to csv file\n","  #  note: can use a dict as a switch statement proxy\n","  if   net_charac == 0: hdf_file = '_link_flowrate'\n","  elif net_charac == 1: hdf_file = '_link_headloss'\n","  elif net_charac == 2: hdf_file = '_link_velocity'\n","  elif net_charac == 3: hdf_file = '_node_demand'\n","  elif net_charac == 4: hdf_file = '_node_head'\n","  elif net_charac == 5: hdf_file = '_node_pressure'\n","  else                : hdf_file = '_error_no_such_net_charac'\n","  \n","  # # Create destination directory if does not exist.\n","  # p = pathlib.Path(dest_dir)\n","  # # print(dest_dir)\n","  # # print(p)\n","  # # print(p.exists())\n","  # if not p.exists() :\n","  #   p.mkdir()\n","\n","  file_ds.to_csv(path_or_buf= dest_dir\n","                 + 'dataset'\n","                 + str(set_size)\n","                 + hdf_file\n","                 + fname_note\n","                 + '.csv',\n","                 index=False)\n","  return\n","  \n","# Might be able to use an if __main__ type statement. Will need to determine\n","#  if %run command makes this a child process or runs as main.\n","tmstps = [78, 80, 82, 84]\n","# tmstps = [79, 81, 83, 85]\n","# tmstps = [80]\n","for tmstp in tmstps :\n","  print(f'tmstp: {tmstp}')\n","  for i in range(0, 6) :\n","    SimData_to_csv(i, tmstp)\n","  print(f'end tmstp {tmstp}')\n","print('Done!')"]},{"cell_type":"markdown","metadata":{"id":"OUuCEDzIb29E"},"source":["####Scratch"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1654045540649,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"BObYUjQI2-WZ","outputId":"ff9bbd8b-1fab-4758-9897-f9bf6edc26c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["   A  B\n","0  0  0\n","1  1  1\n","2  2  2\n","3  0  0\n","4  1  1\n","5  2  2\n","6  0  0\n","7  1  1\n","8  2  2\n","[1, 2, 3]\n"]}],"source":["mult_tm_ds = pd.DataFrame()\n","for i in range(3) :\n","  fl_ds = pd.DataFrame({\"A\": range(3), \"B\": range(3)})\n","  # mult_tm_ds = pd.concat([mult_tm_ds, fl_ds], ignore_index=True)\n","  # Use ignore_index if repeat indices becomes a problem\n","  mult_tm_ds = pd.concat([mult_tm_ds, fl_ds], ignore_index=True)\n","print(mult_tm_ds)\n","rg = [1, 2, 3]\n","print(f'{rg}')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1654045540649,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"ZvkjMF2JcEsH"},"outputs":[],"source":["# p = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/')\n","# print(p.is_dir())"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1654045540649,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"Vp7BColFclvr"},"outputs":[],"source":["# str1 = 'all'\n","# print(f'str {str1}')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1654045540649,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"NW33FINYLe1g"},"outputs":[],"source":["# older version of region_ls() used w/ first four region partition by hand.\n","def region_ls() :\n","  # boundary notation: from-region_to-region (e.g. r1_r2 means from region1 to region2)\n","  region0 = [# 2 boundary edges\n","            'P524', 'P237', 'P292', 'P291', 'P308', 'P293', 'P148', 'P268', 'P1036', 'P1026',\n","            'V47', 'V45', # valves\n","            'P386', 'P86', 'P165', # adjacent\n","            'P1042', 'P502', 'P977', # adjacent\n","            'P85', 'P238', 'P163', 'P938', 'P1035', 'P1034', 'P951', 'P252', 'P383', 'P501',\n","            'P267', 'P1039', 'P284', 'P1025', 'P344', 'P256', 'P933', 'P1024', 'P527', 'P1027',\n","            'P331', 'P54', 'P1028', 'P1029', 'P935', 'P937', 'P1032', 'P39', 'P384', 'P1033',\n","            'P1030', 'P1031', 'P340', 'P349', 'P347', 'P350', 'P914', 'P930', 'P929', 'P810',\n","            'P336', 'P343', 'P341', 'P337', 'P385', 'P338', 'P346', 'P330', 'P339', 'P329', 'P348',\n","            'P280', 'P305', 'P610', 'P510', 'P697', 'P670', 'P932', 'P671', 'P780', 'P754', 'P931',\n","            'P1016', 'P934', 'P940', 'P939', 'P942', 'P944', 'P941', 'P947', 'P946', 'P949', 'P948',\n","            'P959',  'P1023', 'P529', 'P1022', 'P597', 'P1040', 'P1041', 'P290', 'P304',\n","            'P955', 'P956', 'v1', 'P242', 'P243', 'P954', 'P142', 'P953', 'P957', 'P958', # adjacent\n","            'P943', 'P500', 'P270', 'P275', 'P285', 'P286', 'P288', 'P380', 'PU6', 'PU7',\n","            'P381',\n","            ]\n","  region1 = ['P297', # Boundary edge (r1_r2) # 3 total\n","            'P18', # r1_r2\n","            'P379', # r1_r0\n","            'V2', # valves\n","            'P892', 'P96', 'P445', 'P446', 'P450', # adjacent\n","            'P215', 'P287', 'P99', 'P468', 'P467', 'P465', 'P294', 'P303', 'P123', 'P880',\n","            'P118', 'P120', 'P296', 'P307', 'P301', 'P117', 'P282', 'P299', 'P298', 'P484',\n","            'P804', 'P889', 'P801', 'P241', 'P800', 'P112', 'P91', 'P97', 'P83', 'P98',\n","            'P100', 'P22', 'P111', 'P115', 'P121', 'P23', 'P68', 'P924', 'P25', 'P17',\n","            'P30', 'P125', 'P26', 'P101', 'P104', 'P769', 'P72', 'P63', 'P64', 'P67',\n","            'P33', 'P107', 'P31', 'P122', 'P1', 'P109', 'P2', 'P24', 'P110', 'P34',\n","            'P6', 'P3', 'P757', 'P84', 'P758', 'P116', 'P32', 'P87', 'P119', 'P71',\n","            'P108', 'P106', 'P102', 'P27', 'P28', 'P29', 'P103', 'P134', 'P136', 'P174',\n","            'P962', 'P195', 'P201', 'P272', 'P482', 'P295', 'P302', 'P443', 'P756', 'P94', # adjacent (5 rows)\n","            'P755', 'P95', 'P763', 'P767', 'P768', 'P771', 'P772', 'P775', 'P777', 'P776',\n","            'P779', 'P783', 'P785', 'P786', 'P784', 'P787', 'P788', 'P791', 'P797', 'P794',\n","            'P795', 'P798', 'P796', 'P807', 'P805', 'P806', 'P113', 'P92', 'P374', 'P375',\n","            'P310', 'P320', 'P319', 'PU3', 'P322', 'P323', 'PU2', 'PU1',\n","            'P378', 'PU4', 'P376', 'PU5', 'P316',\n","            ]\n","  region2 = [# 3 boundary edges # needs to be broken up into two regions\n","            'P19', # r2_r1\n","            'P996', # r2_r3\n","            'P53', # r2_r3\n","            'P218', 'P840', 'P219', 'P761', 'P220', 'P993', 'P127', 'P11', 'P223', 'P766',\n","            'P927', 'P9', 'P21', 'P991', 'P7', 'P8', 'P819', 'P989', 'P970', 'P990',\n","            'P789', 'P228', 'P813', 'P251', 'P815', 'P141', 'P759', 'P126', 'P132', 'P35',\n","            'P995', 'P817', 'P973', 'P811', 'P225', 'P809', 'P20', 'P230', 'P231', \n","            'P233', 'P10', 'P234', 'P235', 'P13', # adjacent\n","            'P150', 'P55',  'P781', 'P808', # adjacent\n","            'P258', 'P90', 'P129', 'P259', 'P89', 'P124', 'P130', # adjacent\n","            'P147', 'P988', 'P961', 'P1000', 'P37', 'P987', 'P983', 'P984', 'P52', 'P724',\n","            'P725', 'P982', 'P166', 'P65', 'P821', 'P978', 'P249', 'P976', 'P974', 'P963',\n","            'P16', 'P184', 'P915', 'P966', 'P971', 'P255', 'P975', 'P972', 'P986', 'P5',\n","            'P69', 'P144', 'P211', 'P752', 'P753', 'P841', 'P760', 'P177', 'P965', 'P139',\n","            'P998', 'P968', 'P969', 'P994', 'P997', 'P992', 'P138', 'P999', 'P159', 'P964',\n","            'P131', 'P967', 'P12', 'P14', 'P15', 'P128', 'P70',\n","            # region 5\n","            'P161', 'P981', 'P822', 'P309',\n","            ]\n","  region3 = [# 2 boudary edge\n","            'P397', # r3_r2\n","            'P424', # r3_r2\n","            'P140', 'P372', 'P42', 'P633', 'P38', 'P847', 'P596', 'P609', 'P846', 'P852',\n","            'P40', 'P844', 'P850', 'P158', 'P823', 'P826', 'P155', 'P851', 'P1045', 'P43',\n","            'P44', 'P853', 'P51', 'P866', 'P46', 'P157', 'P827', 'P154', 'P830', 'P160',\n","            'P831', 'P162', 'P246', 'P156', 'P248', 'P266', 'P57', 'P492', 'P58', 'P245',\n","            'P264', 'P859', 'P49', 'P48', 'P861', 'P398', 'PU8', 'P399', 'PU9', 'P403', 'P409', 'P402', # adjacent\n","            'P871', 'P855', 'P858', 'P842', 'PU10', 'PU11', 'P410',  'P1044'\n","            ]\n","  return region0, region1, region2, region3"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOZw851uv5y7bFk3IIVkoo2","background_execution":"on","collapsed_sections":["9bR7V14JV1Ng","MVC27tg63-zK"],"mount_file_id":"1Xdjeafh1wUHgg3IZQFo7vO2f2ejDZigF","name":"SimData_to_csv(hdf)_v4.ipynb","provenance":[{"file_id":"1F30YjffWODEFkpvP8LBzkmAMCXKJULam","timestamp":1648667802968},{"file_id":"1P03o2VMexrJWf7e9p2no_n3IjPCiCkYX","timestamp":1628662595668},{"file_id":"1cqHlT2HFA8WJm8zAkxLp7YDSwEK7tcuN","timestamp":1628618688217},{"file_id":"1Ucc3VGdDpJWbujMBbWvOrm3hRXL8mR3k","timestamp":1627242959728}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
