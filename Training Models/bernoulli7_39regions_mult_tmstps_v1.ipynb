{"cells":[{"cell_type":"markdown","metadata":{"id":"j1q0Q-CJEFf5"},"source":["####Notes and Imports"]},{"cell_type":"markdown","metadata":{"id":"4Epm5SJkrzLx"},"source":["#####Notes\n","- Using SimData_to_csv notebook to create dataset csv\n","- I may want a labels map (dict) for pipe labels like the one here\n","  - https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterating-and-visualizing-the-dataset\n","- Unclear what a one-hot encoded tensor might be used for\n","  - used here as a target label transformation\n","    - https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#lambda-transforms\n","\n","- How do I handle two labels?\n","- Is normalization required?\n"," - ans: desirable when features have different ranges. https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=The%20goal%20of%20normalization%20is,when%20features%20have%20different%20ranges.\n","- Complete a panda tutorial\n","- He: Classifer tutorial; CIFAR10 dataset (3 channel images)\n"," - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","- Colab Pro\n"," - One important caveat to remember while using Colab is that the files you upload to it wonâ€™t be available forever. Colab is a temporary environment with an idle timeout of 90 minutes and an absolute timeout of 12 hours. This means that the runtime will disconnect if it has remained idle for 90 minutes, or if it has been in use for 12 hours [longer for Colab Pro]. On disconnection, you lose all your variables, states, installed packages, and files and will be connected to an entirely new and clean environment on reconnecting. source:https://neptune.ai/blog/google-colab-dealing-with-files"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":501,"status":"ok","timestamp":1655287019128,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"CZl-N7iuq70P"},"outputs":[],"source":["# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)\n","\n","# from psutil import virtual_memory\n","# ram_gb = virtual_memory().total / 1e9\n","# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","# if ram_gb < 20:\n","#   print('Not using a high-RAM runtime')\n","# else:\n","#   print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"VQzqACaz6D71"},"source":["####Model framework"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3809,"status":"ok","timestamp":1655287022923,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"YPV_XlfZFlI-","outputId":"b117194c-ee2a-4b02-8cdd-402de7d8299c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["%matplotlib inline\n","\n","import os\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","\n","import pandas as pd   # For loading csv file dataset\n","import random\n","\n","import math   # Used for nan checking (math.isnan())\n","import numpy as np   # Used w/ pipe_to_pipeIdx file and graphics.\n","import pickle   # Used for loading reg_dict (stored w/ training data csv's; constructed in graph_partition notebook)\n","\n","# Straight-Through Estimator\n","# https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html\n","\n","# Autograd\n","# https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n","\n","# Define model\n","#  -- when called, model returns a output_dim dimensional tensor\n","# def sparseProbMap(batch_probMap, sparsity) :\n","def sparseProbMap(probMap, sparsity) :\n","  \"\"\"Rescale probability map (batch_probMap) to obtain desired sparsity in\n","  measuremets that are turned on.\n","  sparsity = budget (int) / training sample length (meas)\n","    budget -- number of measurement to turn on/ sensor to deploy\n","  \"\"\"\n","  mean = torch.mean(probMap, dim=0, keepdim=True)\n","  scalar = sparsity / mean\n","  beta_scalar = (1 - sparsity) / (1 - mean)\n","  toggle = torch.le(scalar, 1).float()\n","  sparse_probMap = ( toggle * scalar * probMap\n","                      + (1 - toggle) * (1 - (1 - probMap) * beta_scalar) )\n","  # print(f'sparseProbMap(): samp_means {samp_means}')\n","  # print(f'sparseProbMap(): samp_means size {samp_means.size()}')\n","  # print(f'sparseProbMap(): samp_scalars {samp_scalars}')\n","  # print(f'sparseProbMap(): samp_beta_scalars {samp_beta_scalars}')\n","  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n","  return sparse_probMap\n","  \n","  # Alt approaches\n","  # torch method -- with prob_map repeat before this func call (i.e. batch_probMap)\n","  # Notice batch_probMap contains batch_size copies of one prob_map\n","  #  Might be able to make this even faster by running the calc once, then repeating after this call.\n","  # Tensor version -- might speed up training time.\n","  # samp_means = torch.mean(batch_probMap, dim=1, keepdim=True)\n","  # samp_scalars = sparsity / samp_means\n","  # samp_beta_scalars = (1 - sparsity) / (1 - samp_means)\n","  # toggles = torch.le(samp_scalars, 1).float()\n","  # sparse_probMap = ( toggles * samp_scalars * batch_probMap\n","  #                     + (1 - toggles) * (1 - (1 - batch_probMap) * samp_beta_scalars) )\n","\n","  # for samp_probMap in batch_probMap :\n","  #   mean_sampProbMap = torch.mean(samp_probMap)\n","  #   scalar_sampProbMap = sparsity / mean_sampProbMap\n","  #   beta_scalar = (1 - sparsity) / (1 - mean_sampProbMap) # Rename variable to something more descriptive\n","  #   # print(f'sparseProbMap(): samp_probMap {samp_probMap}')\n","  #   # print(f'sparseProbMap(): samp_probMap size {samp_probMap.size()}')\n","  #   # print(f'sparseProbMap(): mean_sampProbMap {mean_sampProbMap}')\n","  #   # assert False\n","  #   toggle = torch.le(scalar_sampProbMap, 1).float()\n","  #   scaled_sampMap = ( toggle * scalar_sampProbMap * samp_probMap\n","  #                     + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar) ).reshape([1, -1])\n","  #   # print(f'sparseProbMap(): scaled_sampMap type {type(scaled_sampMap)}')\n","  #   if scaled_sampMaps == None :\n","  #     scaled_sampMaps = torch.cat((scaled_sampMap,), dim=0).to(device)\n","  #   else :\n","  #     scaled_sampMaps = torch.cat((scaled_sampMaps, scaled_sampMap,), dim=0).to(device)\n","  #   # scaled_sampMaps.append(toggle * scalar_sampProbMap * samp_probMap + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar))\n","  # # print(f'sparseProbMap(): scaled_sampMaps size {scaled_sampMaps.size()}')\n","\n","  # NOTE: Dramatic slowdown after incorporating this function.\n","  #  May be due to use of for loop.\n","  # Use with scaled_sampMaps list version.\n","  # One big tensor. I'd prefer rows. Try adding brackets to append statement.\n","  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n","  # sparse_probMap = torch.cat((scaled_sampMaps,), dim=0)\n","  # sparse_probMap  = sparse_probMap.reshape([batch_size, -1])\n","  # assert False\n","  # return scaled_sampMaps   # Equivalent to sparse_probMap for a batch.\n","\n","class STEFunction(torch.autograd.Function) :\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, probability_mask) :\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input (prob_mask) and return\n","        a Tensor containing the output (binary_mask).\n","        ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        # Push probability map through a bernoulli sampling to create 0/1 mask.\n","        # Return mask.\n","\n","        prob_mask_size = probability_mask.size()\n","        # print(f'STEFunc() forward(): prob_mask_size {prob_mask_size}')\n","        # print(f'STEFunc() forward(): batch_size {batch_size}')\n","\n","        # Bernoulli sampling.\n","        # Sample from a uniform distribution.\n","        # uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n","        uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n","        # Bernoulli sampled binary mask.\n","        binary_mask = (probability_mask > uni_samples).float()\n","        # Note the different sizes of probability_mask and uni_sample. The\n","        #  following operation returns a tensor shaped like uni_sample.\n","        # Note: torch.bernoulli() is not usable because func return only the prob mask,\n","        #  but the uniform distributions samples are needed for gradient calcs.\n","        # bin_mask = torch.bernoulli(probablility_mask)\n","        # print(f'STEFunc() forward(): bin_mask_size {binary_mask.size()}')\n","        # print(f'STEFunc() forward(): probability_mask {probability_mask},\\n\\tuni_samples {uni_samples}')\n","        # assert False\n","\n","        ctx.save_for_backward(probability_mask, uni_samples)\n","        return binary_mask\n","    \n","    @staticmethod\n","    def backward(ctx, grad_output) :\n","      # return F.hardtanh(grad_output)\n","      # SigDeriv graph: https://www.desmos.com/calculator/icbxupp3dh\n","      alpha = 1\n","      prob_mask, uni_samples = ctx.saved_tensors\n","\n","      # Sigmoid function derivative\n","      grad_est = (alpha * torch.exp(-alpha * (prob_mask - uni_samples))\n","          / (1 + torch.exp(-alpha * (prob_mask - uni_samples))) ** 2)\n","          # / torch.exp(1 + -alpha * (prob_mask - uni_samples)) ** 2)\n","      return grad_est * grad_output\n","\n","class StraightThroughEstimator(nn.Module) :\n","  def __init__(self) :\n","    # Consider moving probability map parameters here.\n","    #  If so, change STEFunction call.\n","    super(StraightThroughEstimator, self).__init__()\n","  \n","  def forward(self, probability_mask) :\n","    binary_mask = STEFunction.apply(probability_mask)\n","  # def forward(self, probability_mask, batch_size) :\n","    # binary_mask = STEFunction.apply(probability_mask, batch_size)\n","    return binary_mask\n","\n","class Encoder(nn.Module):\n","    # Consider removing the default parameter values.\n","    def __init__(self, input_dim=0, output_dim=0):\n","        super(Encoder, self).__init__()\n","        self.flatten = nn.Flatten()\n","        # Note: nn.Sequential may take only a single argument. A tuple may work,\n","        #  but unclear if this is stable.\n","        self.binary_STE_stack = nn.Sequential(\n","            # Define the input dimensions\n","            # STE is placed at the bottleneck of the autoencoder.\n","            StraightThroughEstimator(),\n","        )\n","        # self.ste = StraightThroughEstimator()\n","        # self.steFunc = STEFunction().apply\n","        self.sparProbMapFunc = sparseProbMap\n","        unif_samp_tn = torch.zeros([input_dim]).uniform_()\n","        self.mask_params = nn.Parameter(unif_samp_tn)\n","        # fill_val_tn = torch.zeros([input_dim]).fill_(0.5)\n","        # self.mask_params = nn.Parameter(fill_val_tn)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        prob_mask = torch.sigmoid(self.mask_params)   # Ensures probabilities lie btwn (0, 1)\n","        batch_size = x.size()[0]\n","        # Used to create bin_mask for ea training sample.\n","        #  To switch to one bin_mask per batch, comment out the following line.\n","        # prob_mask = prob_mask.repeat(batch_size, 1)\n","        sparse_probMask = self.sparProbMapFunc(prob_mask, sparsity=0.012)\n","        sparse_probMask = sparse_probMask.repeat(batch_size, 1)\n","        # sparse_probMask = prob_mask.repeat(batch_size, 1)\n","        # Using a tuple input is problematic because the model expects a grad for all inputs to nn.Sequential.\n","        binary_mask = self.binary_STE_stack(sparse_probMask)\n","        # binary_mask = self.binary_STE_stack(prob_mask)\n","        # binary_mask = self.ste(prob_mask)\n","        # binary_mask = self.steFunc(prob_mask)\n","        # print(f'Encoder.forward(): prob_mask size {prob_mask.size()}')\n","        # print(f'Encoder.forward(): prob_mask size {prob_mask}')\n","        # assert False\n","        # print(x.size())\n","        # print(f'Encoder.forward(): mask_params {self.mask_params}')\n","        # print('Encoder.forward(): batch_size', batch_size)\n","        # print('Encoder.forward(): binary_mask', binary_mask)\n","        return x * binary_mask, binary_mask, self.mask_params\n","\n","    # May or may not need to define the backward behavior of this class.\n","    # def backward(self, x):\n","    #     pass\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, input_dim, output_dim):   # output_dim is not yet used.\n","        super(Decoder, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_lrelu_stack = nn.Sequential(\n","            # Define the input dimensions\n","            nn.Linear(input_dim, 512),\n","            # nn.Linear(input_dim, output_dim),\n","            nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # # Define the output dimensions\n","            nn.Linear(512, output_dim),\n","            nn.LeakyReLU(),\n","        )\n","        # Yisong: initialize the weights in the first layer, and the following layers will follow suit.\n","        # self.linear_lrelu_stack[0].weight.data /= 100.\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        # print(x.size())\n","        logits = self.linear_lrelu_stack(x)\n","        return logits\n","\n","class Autoencoder(nn.Module) :\n","    def __init__(self, input_dim, output_dim) :\n","        super(Autoencoder, self).__init__()\n","        # self.auto_stack = nn.Sequential(\n","        #     Encoder(input_dim, output_dim),\n","        #     Decoder(),\n","        # )\n","        self.encoder = Encoder(input_dim=input_dim)\n","        self.decoder = Decoder(input_dim, output_dim)\n","\n","    def forward(self, x, binMaskSizeLs=[], encode=False, decode=False) :\n","        if encode :\n","            pass\n","        elif decode :\n","            pass\n","        else :\n","            # x = self.auto_stack(x)\n","            under_samp_meas, binary_mask, mask_params = self.encoder(x)\n","            # Working out what to do with the binary_mask -- i.e. if it will be fed to decoder or not.\n","            bin_mask_size = binary_mask.sum()\n","            batch_size = x.size()[0]\n","            # binMaskSizeLs.append(bin_mask_size)\n","            # print(f'AE forward(): avg binary_mask size {int(bin_mask_size / batch_size)}')\n","            # print(f'Auto.forward(): mask_params {mask_params}')\n","            x = self.decoder(under_samp_meas)\n","        return x, mask_params\n","\n","# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using {} device\".format(device))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1655287022923,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"_Bs_956ARP3c"},"outputs":[],"source":["# input_dim = 10\n","# init_tensor = torch.zeros([input_dim]).uniform_()\n","# print(init_tensor)\n","# p_mask = nn.Parameter(init_tensor)\n","# print(p_mask)\n","# ste = STEFunction()\n","# print(ste)\n","# masks = ste.forward(init_tensor, p_mask)\n","# a = torch.randn(1, 2, 3, 4)\n","# b = torch.randn(2, 2)\n","# print(a)\n","# print(a.size())\n","# print(a.view(1, 1, 2, 3, 2, 2))\n","# print(b)\n","# print(b * a.view(1, 1, 2, 3, 2, 2))\n","\n","# Create a mask for each sample by first creating a uniform value tn of size\n","#  [batch_size, prob_mask_size], then \n","# c = torch.zeros([10]).uniform_()\n","# print(c)\n","# tn = c.new_empty([3, 10]).uniform_()\n","# print(tn)\n","# bin_mask = (c > tn).float()\n","# print(bin_mask)\n","# End \"Create a mask ...\"\n","\n","# print(c)\n","# mask = STEFunction.forward(c, c)   # Comment out ctx line.\n","# print(mask)   # OK\n","# module = StraightThroughEstimator()\n","# mask = module(c)\n","# print(mask)\n","# module1 = Encoder(input_dim=10)\n","# meas = torch.randint(100, (10,))\n","# print(meas)\n","# mask = module1(meas)\n","# print(mask)   # OK"]},{"cell_type":"markdown","metadata":{"id":"BTY1vNInM3RI"},"source":["####Training and Testing"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1655287023332,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"Fi2L7VdNGqNb"},"outputs":[],"source":["def train_loop(dataloader, model, entrop_decay, loss_fn, optimizer, pToPIdxDict=None, epoch=0, mod=100):\n","    \"\"\"\n","    pipIdx (for loop) is a tensor containing the pipe indices associated with a training leak scenario.\n","    pToPIdxDict is a dict that maps pipe names (str) to a tuple of (region, pipe_idx)\n","      used to determine number of rows.\n","    \"\"\"\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)   # For avg training loss\n","    # print(f'train_loop(): dataset size: {size}')\n","    # print(f'train_loop(): num_batches: {num_batches}')\n","    train_loss, train_accuracy = 0, 0\n","    confusion_matrix = torch.zeros(len(pToPIdxDict), 4, dtype=torch.int16)   # Think of way to make # of regions automatic.\n","                                                                             # Maybe add a 'lengths' key, value: (pipe_ct, reg_ct)\n","    prevLoss = 100.\n","    for batch, (X, y, pipIdx) in enumerate(dataloader):\n","        # print(batch, X)\n","        # print(y)\n","        # print(pipIdx)\n","        X = X.to(device)\n","        y = y.to(device)\n","        # Compute prediction and loss\n","        pred, prob_params = model(X)  # params for flexibility; update sanity check\n","        # print(y.size())\n","        # print(pred.size())\n","        # print(pred)\n","        # print(prob_params)\n","        \n","        if (True) :\n","        # if (epoch < 100) :\n","          # prob_mask * X -> classifier -> pred\n","          # # print(f\"mask_params {model.state_dict()['encoder.mask_params']}\")\n","          # prob_params = model.state_dict()['encoder.mask_params']  # could be slower than passing the params out\n","          prob_map = torch.sigmoid(prob_params)   # Ensures values (probabilities) lie btwn (0, 1)\n","          splus = nn.functional.softplus(prob_params, -1)   # -log(1 + e^(-x)); careful with the sign when using in expressions.\n","          # print(splus[0], prob_params[0])   # Sanity check\n","          # print(torch.max(splus))\n","          # assert torch.max(splus) < 0\n","          # print(epoch)\n","          # loss_KL = ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) - torch.log(torch.tensor(0.5)) ).sum()\n","          # p_m = torch.where(prob_map > 0, prob_map, torch.tensor(0.001).to(device))\n","          # p_m = torch.where(prob_map < 1, prob_map, torch.tensor(0.999).to(device))\n","          # # print(f'p_m max {torch.max(p_m)}, min {torch.min(p_m)}')\n","          # # print(f'p_m range {torch.max(p_m) + abs(torch.min(p_m))}')\n","          # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) - torch.log(torch.tensor(0.5)) ).sum()\n","          # # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) ).sum()\n","          # loss_KL = lamb * ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) ).sum()\n","          # Natural log is the result of torch.log(...)\n","          loss_KL = ( (1 - prob_map) * ((-1)*prob_params + splus) + prob_map * splus - torch.log(torch.tensor(0.5)) ).sum()   # includes constant term\n","          # loss_KL = ( (1 - prob_map) * ((-1)*prob_params + splus) + prob_map * splus ).sum()\n","          # Dramatic change between including lamb and not may affect learning.\n","          lamb = torch.exp(torch.tensor(entrop_decay * epoch))   #\n","          # lamb = torch.exp(torch.tensor(-1/1*epoch))   # zeros around (very early) epochs;\n","          # lamb = torch.exp(torch.tensor(-1/25*epoch))   # zeros around 250 epochs; https://www.desmos.com/calculator/miayf0qrbe\n","          # lamb = torch.exp(torch.tensor(-1/100*epoch))   # zeros around 1000 epochs; https://www.desmos.com/calculator/dem8eqibe6\n","          # lamb = torch.exp(torch.tensor(-1/4000*epoch))   # zeros around 28k epochs; https://www.desmos.com/calculator/6fi4dfzuji\n","          # lamb = torch.exp(torch.tensor(-1/6000*epoch))   # zeros around 44k epochs; https://www.desmos.com/calculator/cwlhq5xe4n\n","          # lamb = torch.exp(torch.tensor(-1/10000*epoch))   # zeros around 70k epochs \n","          # lamb = torch.exp(torch.tensor(-1/14000*epoch))   # zeros around 100k epochs # Try 1 / sqrt(epoch)   # Was using 1 / e^(-alpha*x) => e^(alpha*x) i.e. not what I wanted.\n","          # lamb = 1 / epoch\n","          loss_KL = lamb * loss_KL   # Anneal diversity loss\n","        else :\n","          loss_KL = 0.0\n","        # # print(f'loss_KL {loss_KL}')\n","        # # assert loss_KL != float('nan')\n","        # assert not math.isnan(loss_KL)\n","\n","        # **** Notice pred is formed on the batch level, but loss_KL is a one shot ****\n","        loss = loss_fn(pred, y) + loss_KL  # returns single value; avg loss across batch\n","        # loss = loss_fn(pred, y)   # returns single value; avg loss across batch\n","        assert loss != float('nan')\n","        # loss = loss_fn(pred, y)  # returns single value; avg loss across batch\n","\n","        # print(loss * len(y))\n","        # for i in range(len(y)) :\n","        #   print(pred[i])\n","        #   print(y[i])\n","        #   ls = loss_fn(pred[i], y[i])\n","        #   print(f'loss {i} {ls}')\n","        # print(y)\n","        # print(pred.argmax(1).type(y.dtype))\n","\n","        # Confusion Matrix\n","        # (pipe label vs region) i.e. the resolution of the row is pipe name (not just region)\n","        # m(n, r)  where n is the cardinality of the leakpipe set, r is the number of regions\n","        # At the time of assignment,\n","        #  need a dict that maps pipe str to pipe idx\n","        #  the pipe label (str) (not the region label) for the training sample\n","        #  the model predicted region\n","        # preds = pred.argmax(1).type(y.dtype)\n","        # for i in range(len(y)) :\n","        #   confusion_matrix[pipIdx[i]][preds[i]] += 1\n","        # print(f'train_loop(): preds {preds}')\n","        # print(confusion_matrix)\n","        # print(f'train_loop(): conf_mat {confusion_matrix.size()}')\n","        # assert False\n","        # Older version of conf mat.\n","        # preds = pred.argmax(1).type(y.dtype)\n","        # for i in range(len(y)) :\n","        #   confusion_matrix[y[i]][preds[i]] += 1\n","        \n","        # Top-k predictions\n","        # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n","        # k = 1\n","        # top_k = torch.topk(input=pred, k=k, dim=1,)\n","        # print(top_k)\n","\n","        # Disaggregate performance -- save model\n","        #  goal: extract outliers (in another notebook)\n","        #  Make into a function.\n","        # if epoch > 2000 and loss.item() > (prevLoss * 15) :\n","        # if epoch == 10000 :\n","        #   print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n","        #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n","        # # print(prevLoss)\n","        # prevLoss = loss.item()\n","        # print(prevLoss > loss * 20)\n","        # print(epoch)\n","        # assert False\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # train_loss += loss_fn(pred, y).item() * len(y)   # I think this second call to loss_fn runs the grad twice. ???\n","        train_loss += loss.item() * len(y)\n","        # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n","        train_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","        # print(loss)\n","\n","        if batch % mod == 0:\n","            # print(batch)\n","            # print(y)\n","            # print(pred)\n","            loss, current = loss.item(), batch * len(X)\n","            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","    # Save last model\n","    # if epoch == 10000 :\n","    #     print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n","    #     torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n","\n","    train_loss /= size    # weighted avg training loss\n","    train_accuracy /= size\n","    print(f\"Training Error: \\n Accuracy: {(100*train_accuracy):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","    return train_loss, train_accuracy, confusion_matrix\n","\n","\n","def test_loop(dataloader, model, loss_fn, pToPIdxDict=None, out_dim=10):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    # print(f'test dataset size: {size}')\n","    # print(f'test num_batches: {num_batches}')\n","    test_loss, test_accuracy = 0, 0\n","    confusion_matrix = torch.zeros(len(pToPIdxDict), out_dim, dtype=torch.int32)\n","\n","    with torch.no_grad():\n","        for X, y, pipIdx in dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","\n","            pred, _ = model(X)\n","            # print(pred)\n","            # print(y)\n","            test_loss += loss_fn(pred, y).item()\n","            # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n","            test_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","            # Confusion matrix\n","            preds = pred.argmax(1).type(y.dtype)\n","            for i in range(len(y)) :\n","              # confusion_matrix[y[i]][preds[i]] += 1\n","              confusion_matrix[pipIdx[i]][preds[i]] += 1\n","            # print(f'test_loop(): conf mat {confusion_matrix}')\n","\n","    test_loss /= num_batches\n","    test_accuracy /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","    # return test accuracy percentage for epoch\n","    return test_accuracy, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"ydc9rlm86Hmt"},"source":["####Animator (d2l)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655287023333,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"9KnHcBLdIJYH"},"outputs":[],"source":["def use_svg_display():\n","    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n","    display.set_matplotlib_formats('svg')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655287023333,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"XKViWEWBIK2E"},"outputs":[],"source":["def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n","    \"\"\"Set the axes for matplotlib.\"\"\"\n","    axes.set_xlabel(xlabel)\n","    axes.set_ylabel(ylabel)\n","    axes.set_xscale(xscale)\n","    axes.set_yscale(yscale)\n","    axes.set_xlim(xlim)\n","    axes.set_ylim(ylim)\n","    if legend:\n","        axes.legend(legend)\n","    axes.grid()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655287023333,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"xM5q0mh96LMw"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","from IPython import display   # Try commenting out. Maybe I'll be able to save image from print out (rather than files)\n","\n","class Animator:\n","    \"\"\"For plotting data in animation.\"\"\"\n","    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n","                 ylim=None, xscale='linear', yscale='linear',\n","                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n","                 figsize=(3.5, 2.5)):\n","        # Incrementally plot multiple lines\n","        if legend is None:\n","            legend = []\n","      \n","        self.nrows = nrows\n","        self.ncols = ncols\n","        self.figsize = figsize\n","\n","        self.xlabel = xlabel\n","        self.ylabel = ylabel\n","        self.xlim = xlim\n","        self.ylim = ylim\n","        self.xscale = xscale\n","        self.yscale = yscale\n","        self.legend = legend\n","\n","        self.X, self.Y, self.fmts = None, None, fmts\n","\n","    def add(self, x, y):\n","        # Add multiple data points into the figure\n","        if not hasattr(y, \"__len__\"):\n","            y = [y]\n","        n = len(y)\n","        if not hasattr(x, \"__len__\"):\n","            x = [x] * n\n","        if not self.X:\n","            self.X = [[] for _ in range(n)]\n","        if not self.Y:\n","            self.Y = [[] for _ in range(n)]\n","        for i, (a, b) in enumerate(zip(x, y)):\n","            if a is not None and b is not None:\n","                self.X[i].append(a)\n","                self.Y[i].append(b)\n","\n","    \n","    def display_plt(self):\n","        # Borrowed use_svg_display() implementation from d2l\n","        use_svg_display()\n","        # matplot function\n","        self.fig, self.axes = plt.subplots(self.nrows, self.ncols, figsize=self.figsize)\n","        if self.nrows * self.ncols == 1:\n","            self.axes = [self.axes,]\n","        # Use a lambda function to capture arguments; set_axes in d2l API\n","        self.config_axes = lambda: set_axes(self.axes[0],\n","                                            self.xlabel,\n","                                            self.ylabel,\n","                                            self.xlim,\n","                                            self.ylim,\n","                                            self.xscale, self.yscale, self.legend)\n","        self.axes[0].cla()\n","        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n","            self.axes[0].plot(x, y, fmt)\n","        self.config_axes()\n","        display.display(self.fig)\n","        display.clear_output(wait=True)\n","\n","        # return self.fig"]},{"cell_type":"markdown","metadata":{"id":"3ocO0Om3ofeX"},"source":["####Confusion Matrix"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":453,"status":"ok","timestamp":1655287023782,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"AxqpHHcoomED"},"outputs":[],"source":["# %run /content/drive/MyDrive/'Colab Notebooks'/'Water Distribution Network'/'Input Pipeline'/'Data To File'/SimData_to_csv(hdf)_v1.ipynb\n","\n","class Conf_Mat() :\n","  def __init__(self, classes=10) :\n","    \"\"\"classes (int): the number of classes in the classifier.\"\"\"\n","    self.confusion_matrix = torch.zeros(classes, classes, dtype=torch.int32)\n","    # Load labels via JSON or csv file.\n","    self.labels_str = ['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024']\n","  \n","  def addValues(self, pred, y) :\n","    \"\"\"Add values to the confusion matrix.\n","    pred (tensor): tensor containing model predictions.\n","    y (tensor): tensor containing ground truth labels.\n","    return None\n","    \"\"\"\n","    # Confusion Matrix\n","    for i in range(len(y)) :\n","      preds = pred.argmax(1).type(y.dtype)\n","      confusion_matrix[y[i]][preds[i]] += 1\n","    print(confusion_matrix)\n","  \n","  def displayConfMat(self) :\n","    pass\n","\n","def decode_labels(regdict_filenm) :\n","  # Might be easier to place the encoder in a file and read it here.\n","  # WARNING: lab_subset order is not consistent.\n","  # NOTE: sets are not subscriptable\n","  # return sorted(['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024'])\n","  # regdict_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/leak_pipes_all/39regions/00/'\n","  # file_nm = f'region_dict_{partitions}.pickle'\n","  load_loc = regdict_filenm\n","\n","  # For loading\n","  with open(load_loc, 'rb') as handle:\n","      reg_dict = pickle.load(handle)\n","  \n","  prediction_labels = []\n","  for reg_nm in reg_dict['reg_partits'] :   # reg_partits is a dict; key: reg_nm (str), value: ls of pipe in region\n","    prediction_labels.append(reg_nm)\n","  return sorted(prediction_labels)   # sorted in lexiconic order. Not cosistent w/ conf_mat pred col order.\n","# print(decode_labels())   # For testing.\n","\n","def leak_pipe_strs() :\n","  # Would like to develop an automatic method to transfer pipe names of labels (rather copy paste).\n","  #  In this noteboook, labels have been encoded (mapped) to a unique int in \n","  # Copy-paste list of pipe names for labels here from simdata_to_csv notebook.\n","  return sorted({'P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P1022',\n","                 'P1023', 'P1024', 'P1025', 'P1026', 'P1027', 'P1028', 'P1029', \n","                 'P103', 'P1030', 'P1031', 'P1032', 'P1033', 'P1034', 'P1035', \n","                 'P1036', 'P1039', 'P104', 'P1040', 'P1041', 'P1042', 'P1044', \n","                 'P1045', 'P106', 'P107', 'P108', 'P109', 'P11', 'P110', 'P111', \n","                 'P112', 'P113', 'P115', 'P116', 'P117', 'P118', 'P119', 'P12', \n","                 'P120', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', \n","                 'P128', 'P129', 'P13', 'P130', 'P131', 'P132', 'P134', 'P136', \n","                 'P138', 'P139', 'P14', 'P140', 'P141', 'P142', 'P144', 'P147', \n","                 'P148', 'P15', 'P150', 'P154', 'P155', 'P156', 'P157', 'P158', \n","                 'P159', 'P16', 'P160', 'P161', 'P162', 'P163', 'P165', 'P166', \n","                 'P17', 'P174', 'P177', 'P18', 'P184', 'P19', 'P195', 'P2', 'P20', \n","                 'P201', 'P21', 'P211', 'P215', 'P218', 'P219', 'P22', 'P220', 'P223', \n","                 'P225', 'P228', 'P23', 'P230', 'P231', 'P233', 'P234', 'P235', 'P237', \n","                 'P238', 'P24', 'P241', 'P242', 'P243', 'P245', 'P246', 'P248', 'P249', \n","                 'P25', 'P251', 'P252', 'P255', 'P256', 'P258', 'P259', 'P26', 'P264', \n","                 'P266', 'P267', 'P268', 'P27', 'P270', 'P272', 'P275', 'P28', 'P280', \n","                 'P282', 'P284', 'P285', 'P286', 'P287', 'P288', 'P29', 'P290', 'P291', \n","                 'P292', 'P293', 'P294', 'P295', 'P296', 'P297', 'P298', 'P299', 'P3', \n","                 'P30', 'P301', 'P302', 'P303', 'P304', 'P305', 'P307', 'P308', 'P309', 'P31', 'P310', 'P316', 'P319', 'P32', 'P320', 'P322', 'P323', 'P329', 'P33', 'P330', 'P331', 'P336', 'P337', 'P338', 'P339', 'P34', 'P340', 'P341', 'P343', 'P344', 'P346', 'P347', 'P348', 'P349', 'P35', 'P350', 'P37', 'P372', 'P374', 'P375', 'P376', 'P378', 'P379', 'P38', 'P380', 'P381', 'P383', 'P384', 'P385', 'P386', 'P39', 'P397', 'P398', 'P399', 'P40', 'P402', 'P403', 'P409', 'P410', 'P42', 'P424', 'P43', 'P44', 'P443', 'P445', 'P446', 'P450', 'P46', 'P465', 'P467', 'P468', 'P48', 'P482', 'P484', 'P49', 'P492', 'P5', 'P500', 'P501', 'P502', 'P51', 'P510', 'P52', 'P524', 'P527', 'P529', 'P53', 'P54', 'P55', 'P57', 'P58', 'P596', 'P597', 'P6', 'P609', 'P610', 'P63', 'P633', 'P64', 'P65', 'P67', 'P670', 'P671', 'P68', 'P69', 'P697', 'P7', 'P70', 'P71', 'P72', 'P724', 'P725', 'P752', 'P753', 'P754', 'P755', 'P756', 'P757', 'P758', 'P759', 'P760', 'P761', 'P763', 'P766', 'P767', 'P768', 'P769', 'P771', 'P772', 'P775', 'P776', 'P777', 'P779', 'P780', 'P781', 'P783', 'P784', 'P785', 'P786', 'P787', 'P788', 'P789', 'P791', 'P794', 'P795', 'P796', 'P797', 'P798', 'P8', 'P800', 'P801', 'P804', 'P805', 'P806', 'P807', 'P808', 'P809', 'P810', 'P811', 'P813', 'P815', 'P817', 'P819', 'P821', 'P822', 'P823', 'P826', 'P827', 'P83', 'P830', 'P831', 'P84', 'P840', 'P841', 'P842', 'P844', 'P846', 'P847', 'P85', 'P850', 'P851', 'P852', 'P853', 'P855', 'P858', 'P859', 'P86', 'P861', 'P866', 'P87', 'P871', 'P880', 'P889', 'P89', 'P892', 'P9', 'P90', 'P91', 'P914', 'P915', 'P92', 'P924', 'P927', 'P929', 'P930', 'P931', 'P932', 'P933', 'P934', 'P935', 'P937', 'P938', 'P939', 'P94', 'P940', 'P941', 'P942', 'P943', 'P944', 'P946', 'P947', 'P948', 'P949', 'P95', 'P951', 'P953', 'P954', 'P955', 'P956', 'P957', 'P958', 'P959', 'P96', 'P961', 'P962', 'P963', 'P964', 'P965', 'P966', 'P967', 'P968', 'P969', 'P97', 'P970', 'P971', 'P972', 'P973', 'P974', 'P975', 'P976', 'P977', 'P978', 'P98', 'P981', 'P982', 'P983', 'P984', 'P986', 'P987', 'P988', 'P989', 'P99', 'P990', 'P991', 'P992', 'P993', 'P994', 'P995', 'P996', 'P997', 'P998', 'P999'})"]},{"cell_type":"markdown","metadata":{"id":"RVZsPo1p2RoP"},"source":["####Normalization"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"dDXCstVB2XIb"},"outputs":[],"source":["def norm(features) :\n","  # print(f'norm(): {features} size {features.size()}')\n","  sq_feat = features ** 2\n","  # print(f'norm(): {sq_feat} size {sq_feat.size()}')\n","  sum_feat = sq_feat.sum(1)\n","  # print(f'norm(): {sum_feat} size {sum_feat.size()}')\n","  norm_feat = torch.sqrt(sum_feat)\n","  # print(f'norm(): {norm_feat} size {norm_feat.size()}')\n","  # print(norm_feat.view(features.size(0), 1))\n","  unit_feat = features / norm_feat.view(features.size(0), 1)\n","  # print(f'norm(): {unit_feat} size {unit_feat.size()}')\n","  return unit_feat\n","# _, observed, __ = SimData(net_char=4, tmstp=80)\n","# norm(observed)"]},{"cell_type":"markdown","metadata":{"id":"TmFOoA-k258k"},"source":["####Subsampling"]},{"cell_type":"markdown","metadata":{"id":"5wCj-uKc561c"},"source":["#####Mask Generation\n","- (1/0) Sensing Mask Vector"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"QZnE0eEEGO88"},"outputs":[],"source":["def sensing_mask_rand(feature_vec, max_sense = 30):\n","  \"\"\"\n","  preconditions: feature tensor must be flat.\n","\n","  Parameters\n","  feature_vec: feature tensor to be masked\n","   shape: list of dimensions\n","  max_sense: randomly choose 1 to max_sense to be 1, rest 0\n","\n","  returns 0/1 sensing mask tensor of shape feature_vec w/ max_sense elems set to 1 (rest 0)\n","  \"\"\"\n","\n","  # Create (0/1) mask populated w/ max_sense ones\n","  mask = torch.zeros(feature_vec.size()).to(device)\n","  # print(mask)\n","  # indices = torch.randint(len(feature_vec), size=(max_sense,))   # may contain fewer than max_sense unique indices\n","  # Katie: Randomize indices and choose the first max_sense\n","  indices = [*range(len(feature_vec))]\n","  random.shuffle(indices)\n","  # print(indices)\n","  # for idx in indices:   # for use with line 16\n","  for idx in indices[:max_sense]:\n","    mask[idx] = 1\n","  # print(mask)\n","\n","  # Pass features through the mask\n","  masked_features = feature_vec * mask\n","\n","  return mask, masked_features.to(device)\n","# feature_vec = torch.rand([100])\n","# print(feature_vec)\n","# mask, masked_feats = sensing_mask_rand(feature_vec)\n","# print(mask, '\\n', masked_feats)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"jaJ04dzFpbzL"},"outputs":[],"source":["def sensing_mask_alternate(feature_vec):\n","  \"\"\"\n","  preconditions: feature tensor must be flat.\n","\n","  Parameters\n","  feature_vec: feature tensor to be masked\n","\n","  returns 0/1 sensing mask tensor of shape feature_vec w/ even indexed elems set to 1 (rest 0)\n","  \"\"\"\n","\n","  # Create (0/1) mask populated w/ max_sense ones\n","  mask = torch.zeros(feature_vec.size()).to(device)\n","  # print(mask)\n","  for idx in range(0, feature_vec.size()[0], 2) :\n","    mask[idx] = 1\n","  # print(mask)\n","\n","  # Pass features through the mask\n","  # masked_features = feature_vec * mask\n","\n","  # Simplification: This version reduces the size of the feature\n","  #  vector by half (mask not concat).\n","  masked_features = feature_vec[0].reshape([1])\n","  for idx in range(2, feature_vec.size()[0], 2) :\n","    masked_features = torch.cat((masked_features, feature_vec[idx].reshape([1])))\n","\n","  return mask, masked_features.to(device)\n","# feature_vec = torch.rand([100]).to(device)\n","# print(feature_vec)\n","# mask, masked_feats = sensing_mask_alternate(feature_vec)\n","# print(mask, '\\n', masked_feats.size(), '\\n', masked_feats)"]},{"cell_type":"markdown","metadata":{"id":"7KkFT5x93F5H"},"source":["#####Random subsamples"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"xfeD3clN3QKu"},"outputs":[],"source":["def random_subset(meas_tensor, size, seed=None) :\n","  \"\"\"Random subset of measurements (fixed seed).\n","  Must be able to change the cardinality of the subset.\n","  Algorithm\n","    create list of indices\n","    randomize indices\n","    select first x number of indices (where x is the subset cardinality)\n","    form a tensor containing the the sample measurements matching those indices.\n","    return this tensor\n","  \"\"\"\n","  gen = None\n","  if seed :\n","    gen = torch.Generator().manual_seed(seed)\n","\n","  rand_idxs = torch.randperm(meas_tensor.size()[0], generator=gen)\n","  sub_idxs = rand_idxs[:size]\n","  # print('random_subset():', sub_idxs, sub_idxs.size())\n","  subset = meas_tensor[sub_idxs]\n","  # print(meas_tensor.size())\n","  return subset.to(device), sub_idxs\n","\n","# Test code\n","# X = torch.rand(100)\n","# rand_sub = random_subset(X, 50, 1000)\n","# print('random_subset results:', rand_sub)\n","# print(rand_sub.size())\n","# assert False"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"Di_I5E4g4PDR"},"outputs":[],"source":["def rand_sub_dataset(X, size, seed=None) :\n","  reduced_meas_X = torch.zeros([X.size(0), size]).to(device)\n","  # print(X.size(0))\n","  for i in range(len(X)) :\n","    rand_sub, sub_idxs = random_subset(X[i], size, seed)\n","    # print('random_subset results:', rand_sub)\n","    # print(rand_sub.size())\n","    reduced_meas_X[i] = rand_sub\n","  return reduced_meas_X, sub_idxs\n","\n","# Test Code\n","# X = torch.rand([3, 20])\n","# size, seed = 10, 1000\n","# reduced_meas_X, sub_idxs = rand_sub_dataset(X, size, seed)\n","# # print(sub_idxs)\n","# for i, idx in enumerate(sub_idxs) :\n","#   # print(reduced_meas_X[0, i], X[0, idx])\n","#   if reduced_meas_X[0, i].item() != X[0, idx].item() :\n","#     raise Exception('rand_sub_dataset(): Error: Value doesnt match.')\n","# # print(X[0, 307], X[0, 536], X[0, 329])\n","# # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n","# print('Done!')"]},{"cell_type":"markdown","metadata":{"id":"if63qNQuS0HA"},"source":["####Load CSV (SimData) and form concat dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"uY7IINhjLGSQ"},"outputs":[],"source":["def SimData(net_char, dset_size, tmstp_path, tmstp) :\n","  \"\"\"\n","  Note: Dataset file generated in SimData_to_csv notebook\n","   row 0: base_case\n","   rows 1->set_size: observed; one leak scenario per row; 1hr (out of one week)\n","    Labels included (last column); pipe index (second to last column) for use w/ conf mat\n","  \"\"\"\n","\n","  # Load dataframe\n","  # Moved file path assembly to filepaths(); centralizing input/output path update\n","  if tmstp :\n","    ts_dir = f'tmstp{tmstp}/'\n","  else :\n","    ts_dir = ''\n","  # NOTE: double check dataset file name\n","  dataset_file = ts_dir + f'dataset{dset_size}'\n","  areaLo, areaHi = 0.01, 0.1\n","  leak_area = f'area{areaLo}_{areaHi}'\n","  if net_char == 0:   dataset_file += f'_link_flowrate_{leak_area}.csv'\n","  elif net_char == 1: dataset_file += f'_link_headloss_{leak_area}.csv'\n","  elif net_char == 2: dataset_file += f'_link_velocity_{leak_area}.csv'\n","  elif net_char == 3: dataset_file += f'_node_demand_{leak_area}.csv'\n","  elif net_char == 4: dataset_file += f'_node_head_{leak_area}.csv'\n","  elif net_char == 5: dataset_file += f'_node_pressure_{leak_area}.csv'\n","  \n","  data_file = tmstp_path + dataset_file\n","  # print(data_file)\n","  # data_file = src_dir + leak_pipes + dataset_file\n","  datast_ds = pd.read_csv(data_file)\n","  # print(datast_ds.head())   # View csv data.\n","  \n","  # Separate base_case, raw_data, and encoded labels\n","  # Base Case\n","  # print(datast_ds.values[0, :-1].astype(np.float32))\n","  # print(type(datast_ds.values[0, :-1].astype(np.float32)))\n","  # base_case = torch.tensor(datast_ds.values[0, :-1], dtype=torch.float32).to(device)\n","  # base_case = torch.tensor(datast_ds.values[0, :-1].astype(np.float32), dtype=torch.float32).to(device)\n","  base_case = torch.tensor(datast_ds.values[0, :-2], dtype=torch.float32).to(device)   # Used w/ separate cols for pipeIdx and Label (no tuple)\n","  # print(base_case)\n","  # raw_data\n","  # raw_data = torch.tensor(datast_ds.values[1: , :-1], dtype=torch.float32).to(device)\n","  # raw_data = torch.tensor(datast_ds.values[1: , :-1].astype(np.float32), dtype=torch.float32).to(device)  # Had to add astype() when I added tuples in label col\n","  raw_data = torch.tensor(datast_ds.values[1: , :-2], dtype=torch.float32).to(device)   # Used w/ separate cols for pipeIdx and Label (no tuple)\n","  print(f'SimData(): raw_data {raw_data.size()}')\n","  # encoded labels\n","  # print(datast_ds['Label'][1])\n","  # print(type(datast_ds['Label'].astype('int')[1]))\n","  pipe_idxs = torch.tensor(datast_ds['PipeIdx'], dtype=torch.long)[1:].to(device)\n","  labels = torch.tensor(datast_ds['Label'], dtype=torch.long)[1:].to(device)\n","  # Possible implementation for pipe_idx (mapping of pipe_nms to pipe_idx done in simdata_to_csv notebook)\n","  #  Change base_case and raw_data to [1: , :-2]; the last two rows would contain pipe_idx and label values respectively.\n","  # pipe_idx = torch.tensor(datast_ds['Pipe_idx'], dtype=torch.long)[1:].to(device)\n","  print(f'SimData(): labels {labels.size()}')\n","\n","  return base_case, raw_data, labels, pipe_idxs\n","# _, __, ___ = SimData(0, 80)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"A6Xn9Due5kYf"},"outputs":[],"source":["def leakpipe_subset(pipe_ct, raw, enc_labs):\n","  \"Used to limit the number of pipes in the dataset\"\n","  tmp_raw = torch.tensor([[0]]).to(device)   # dim trouble w/ cat\n","  tmp_labs = torch.tensor([0]).to(device)\n","  print(tmp_labs.size())\n","\n","  for i in range(len(enc_labs)):\n","    if enc_labs[i] < pipe_ct:\n","      tmp_raw = torch.cat((tmp_raw, raw[i].reshape([444])))\n","      tmp_labs = torch.cat((tmp_labs, enc_labs[i].reshape([1])))\n","\n","  return tmp_raw[1:], tmp_labs[1:]"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1655287023783,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"alN3uGMtzKbn"},"outputs":[],"source":["def cat_net_attr(net_ls, dset_size, ts_path, tmstp, residual=False, norm_feats=False) :\n","  \"\"\"Creates three flat vectors containing concatinations of each attribute's\n","       (1) base_case, (2) observed measurements, and (3) a single copy of label data (leak labels for all attrs are the same).\n","     **If residual is true, don't use norm here.**\n","     \"\"\"\n","  cat_attrs = None\n","  for net_char in net_ls :\n","    # base_case, X_raw, encoded_labels\n","    data = SimData(net_char, dset_size, ts_path, tmstp)\n","    data_ls = list(data)   # Creates 3 elem list from data 3-tuple returned by SimData.\n","\n","    if residual :   # Residual before normalization of features.\n","      data_ls[1] -= data_ls[0]\n","    # print(data_ls[1])\n","    # assert False\n","    if norm_feats :   # Normalize each attribute data separtely before cat.\n","      data_ls[1] = norm(data_ls[1])\n","\n","    if not cat_attrs :\n","      cat_attrs = data_ls   # Includes labels. Only needed once.\n","    else :\n","      cat_attrs[0] = torch.cat((cat_attrs[0], data_ls[0]))  # Update base_case by cat'ing next net_attr.\n","      cat_attrs[1] = torch.cat((cat_attrs[1], data_ls[1]), dim=1)   # Update features by cat'ing next net_attr.\n","      # only need one set of label; no need to cat those again.\n","    # Normalize Features -- all together (i.e. if more than one attribute is cat'ed, all will be norm'ed together)\n","    # if norm_feats:\n","      # Raw data normed across all samples\n","      #  Would it make more sense to normalize ea sample individually?\n","      # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n","      # cat_attrs[1] = norm(cat_attrs[1])\n","  print(f'cat_net_attr(): Base {cat_attrs[0].size()}, X {cat_attrs[1].size()}, Labels {cat_attrs[2].size()}')\n","  return cat_attrs   # cat_attrs is a list (of tensors), but can be expanded by the caller.\n","# _, __, ___ = cat_net_attr([0, 4], 80, True)\n","# print(_.size(), __.size(), ___.size())\n","# assert False"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"WWwFaq5FMiPh"},"outputs":[],"source":["def cat_data(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], dset_size=-1, ts_path=None, tmstp=None) :\n","  \"\"\"Idea: pass list of tmstps to this function, for loop the list to create a training set w/ training samples from mult tmstps.\n","  \"\"\"\n","  if isinstance(net_char, int) :\n","    net_char = [net_char]\n","  \n","  n_fs = norm_feats\n","  if residual :\n","    n_fs = False\n","  \n","  # Consider writing a script that automates both pulling mult tmstps of sim data together, but also cats features\n","  #  or perhaps just a dataset constructor script where multiple tmstps can be assembled into a single training set.\n","  base_case, X_raw, encoded_labels, pipIdxs = cat_net_attr(net_char, dset_size, ts_path, tmstp, residual, norm_feats=n_fs)   # Don't norm_feat in cat_net_attr if residual is true.\n","  # base_case, X_raw, encoded_labels = SimData(net_char, tmstp)\n","  print(f'cat_data(): X_raw max {torch.max(X_raw)}, min {torch.min(X_raw)}')\n","  print(f'cat_data(): X_raw range {torch.max(X_raw) + abs(torch.min(X_raw))}')\n","  print(f'cat_data(): X_raw {X_raw.size()}')\n","  # print(f'cat_data(): X_raw {X_raw[0]}')\n","  # print(f'cat_data(): base_case {base_case}')\n","  # print(f'cat_data(): pipe_idxs {encoded_labels}')\n","  \n","  # work in progress\n","  # X_raw, encoded_labels = leakpipe_subset(10, X_raw, encoded_labels)\n","  # print(f'cat_data: encoded labels \\n{encoded_labels}')\n","  # print(f'cat_data: encoded labels {encoded_labels.size()}')\n","\n","  # X_cat = None\n","  # norm_str = ''   # Used for debug output\n","  # Residual -- handled in cat_net_data\n","  # div_by_base = base_case   # What is this used for?\n","  # if residual:\n","    # Avoid dividing by zero\n","    # for idx in range(len(base_case)):\n","      # if base_case[idx] < 1.0e-12:\n","      #   div_by_base[idx] = 1.0e-10\n","    # X_raw = X_raw / div_by_base\n","    # X_raw -= base_case\n","    # base_case /= base_case   # might divide by zero\n","    # pass\n","  # print(f'cat_data(): X_raw {X_raw[0]}')\n","  \n","  # Normalizations\n","  #  Not sure this code is doing anything as X_raw has already been formed.\n","  if norm_base:\n","    # Normalize base_case\n","    #  orders of magnitude larger than residual data\n","    #  v = v / max(||v||_p, epsilon)  where epsilon is a small value that void dividing by zero\n","    # norm_base_case = torch.nn.functional.normalize(base_case.reshape([1,-1]))\n","    base_case = torch.nn.functional.normalize(base_case, dim=0) * 10.0   # Out-dated, use with caution.\n","    norm_str = 'norm_'\n","  # if norm_feats:\n","  #   # Raw data normed across all samples\n","  #   #  Would it make more sense to normalize ea sample individually?\n","  #   # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n","  #   X_raw = norm(X_raw)\n","  # print(f'cat_data(): {norm_str}base_case {base_case.size()}')\n","  # # print(f'cat_data(): {norm_str}base_case {base_case}')\n","\n","  # print(X_raw[0])   # X_raw elements are already a flat tensor.\n","  # I think I can move if mask outer and elim the for loop.\n","  # for feature_vec in X_raw:\n","  #   if mask :\n","  #     # Mask and masked features\n","  #     #  May want sensing_mask_rand() that can process batches of samples\n","  #     # mask_tn, masked_feats = sensing_mask_rand(feature_vec)\n","  #     # Construct feature set from concatination of base_case, mask, and masked measuremnts.\n","  #     # temp = torch.cat((masked_feats.to(device), mask.to(device), base_case.to(device))).reshape([1,-1])\n","  #     # base_case measurements not included\n","  #     # temp = torch.cat((masked_feats.to(device), mask_tn.to(device))).reshape([1, -1])\n","  #     # Simplification: notice feature vector size is halved. Update col variable accordingly.\n","  #     mask_tn, masked_feats = sensing_mask_alternate(feature_vec)\n","  #     # Not Needed. Adjust size of label vector.\n","  #     # masked_labels = encoded_labels[0].reshape([1])\n","  #     # print(encoded_labels.size())\n","  #     # assert False\n","  #     # for idx in range(2, encoded_labels.size()[0], 2) :\n","  #     #   masked_labels = torch.cat((masked_labels, encoded_labels[idx].reshape([1])))\n","  #     temp = masked_feats.reshape([1,-1])\n","  #     # encoded_labels = masked_labels\n","  #     # print(f'cat_data(), mask: enc_lab size {encoded_labels.size()}')\n","\n","  #   else :\n","  #     # Features (no mask)\n","  #     # Construct feature set from concatination of base_case and observed measuremnts.\n","  #     # temp = torch.cat((feature_vec.to(device), base_case.to(device))).reshape([1,-1])\n","  #     # I don't think this is doing anything.  Check if X_raw is changed by this. May already be a flat tensor after cat_net_attrs call.\n","  #     temp = feature_vec.reshape([1,-1])   # Tensor containing a tensor. Not sure that's needed.\n","  #   # print(f'cat_data(): temp {temp}')\n","\n","    # if X_cat is None:\n","    #   X_cat = temp\n","    #   # print(X_cat)\n","    # else:  \n","    #   X_cat = torch.cat((X_cat, temp))\n","    #   # print(X_cat)\n","\n","  # print(f'cat_data(): X_cat {X_cat.size()}')\n","  # print(f'cat_data(): X_cat {X_cat[0]}')\n","  # assert False\n","  # return X_cat, encoded_labels   # Not returning base_case at this time.\n","  return X_raw, encoded_labels, pipIdxs   # Not returning base_case at this time.\n","# print(f'output: {cat_data(residual=True, norm_base=False, norm_feats=False, mask=False)}')\n","# assert False"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"8lxQz9XbUBgO"},"outputs":[],"source":["def cat_data_mult_tmstps(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], dset_size=-1, ts_path=None, tmstps=None) :\n","  \"\"\"This func wraps cat_data() to concat traning samples from multiple time stamps.\n","  Notice tmstps is a list of time stamps to be included in the training set\n","      tmstps can include one time stamp to mimic previouis version, but must be in a list.\n","  \"\"\"\n","  X_raw, encoded_labels, pipIdxs = None, None, None\n","\n","  for tmstp in tmstps :\n","    X_r, enc_labs, pipIds = cat_data(residual, norm_base, norm_feats, mask, net_char, dset_size, ts_path, tmstp)\n","    if X_raw is None :\n","      X_raw = torch.cat([X_r])\n","      encoded_labels = torch.cat([enc_labs])\n","      pipIdxs = torch.cat([pipIds])\n","    else :\n","      X_raw = torch.cat([X_raw, X_r])\n","      encoded_labels = torch.cat([encoded_labels, enc_labs])\n","      pipIdxs = torch.cat([pipIdxs, pipIds])\n","  # print(f'X_raw {X_raw.size()}')\n","  # assert False\n","\n","  return X_raw, encoded_labels, pipIdxs"]},{"cell_type":"markdown","metadata":{"id":"CdawEj-SS9hP"},"source":["####Train the model"]},{"cell_type":"markdown","metadata":{"id":"j_Zwmb6sYpbm"},"source":["#####Set-up Training"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"tcdigZ4CO-pn"},"outputs":[],"source":["def filepaths() :\n","  \"\"\"Consider splitting input and output paths into two separate functions.\n","  \"\"\"\n","  ## Inputs\n","  partitions = 39\n","  leak_pip_ct = 'all'\n","  leak_pipes = f'leak_pipes_{leak_pip_ct}/'\n","\n","  src_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/'\n","  reg_dir = src_dir + f'Input Pipeline/Datasets/' + leak_pipes + f'{partitions}regions/'\n","  version_dir = reg_dir + '00/'\n","\n","  #  file path in Confusion Matrix cell; decode_labels(); region_dict_xx.pickle\n","  file_nm = f'region_dict_{partitions}.pickle'\n","  regdict_filenm = version_dir + file_nm\n","\n","  #  file path in SimData(); dest path from step 3\n","  tmstp_path = version_dir\n","  # NOTE: double check dataset file name\n","  dset_size = 5000\n","\n","  #  file path in Training setup and loop cell; pipe to pipe_idx.npy\n","  pToPId_dir = version_dir\n","  file_nm = 'dictPipeToPipeIdx.npy'\n","  pToPIdx_filenm = pToPId_dir + file_nm\n","  ## -------------- ##\n","\n","  ## Outputs\n","  analy_dir = src_dir + f'Analysis/{partitions}regions/'\n","  download_dir = analy_dir + f'download_{partitions}regs/'\n","\n","  #  file path in Training setup and loop cell; display_of_sensor_grid.pt\n","  sensgrid_filenm = analy_dir + f'display_of_sensor_grid.pt'\n","\n","  #  file paths (3) in Graphics cell; loss.png, conf.png, conf_mat.pt\n","  conf_mat_filenm = analy_dir + 'conf_mat.pt'\n","  loss_filenm = download_dir + 'loss.png'\n","  conf_filenm = download_dir + 'conf.png'\n","  ## -------------- ##\n","\n","  return partitions, regdict_filenm, tmstp_path, dset_size, pToPIdx_filenm, sensgrid_filenm, loss_filenm, conf_filenm, conf_mat_filenm"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"yLehjrJ5SV8M"},"outputs":[],"source":["rows = 0\n","cols = 0\n","input_dim = 0\n","output_dim = 0\n","\n","tr_dataset = None\n","ts_dataset = None"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"gJJN6lECYvM_"},"outputs":[],"source":["from collections import Counter\n","import statistics as stats\n","\n","def histo_pipe_dist(labels) :\n","  \"\"\"Display histogram of leakpipe distribution.\n","  labels (tensor) : pytorch tensor of labels (ints).\n","  Note: requires matplotlib and Pandas.\n","  \"\"\"\n","  # Histo\n","  Y = pd.Series(labels.cpu())\n","  recounted = Counter(Y)\n","  print(recounted)\n","  std = stats.stdev(recounted.values())\n","  print(f'stdev {std:.2}')\n","  Y.plot.hist(grid=True, bins=10, alpha=0.7, rwidth=0.8, color='#607c8e', align='mid')\n","  plt.title(f'Label Frequency for {len(Y)} Samples')\n","  plt.xlabel('Label')\n","  plt.grid(axis='x')\n","  # plt.text(6, 200, r'class 5 = 229 (46%)')\n","  # assert False\n","  return std"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"PpisWMQEGjxL"},"outputs":[],"source":["def randomize_dataset(X, y) :\n","  random.seed(10343)\n","  ls = []\n","  for i in range(len(y)) :\n","    zipped = X[i], y[i]\n","    # print(zipped, end=\" \")\n","    ls.append(zipped)\n","  # print('randomize_dataset(): ls', len(ls))\n","  random.shuffle(ls)\n","  shuf_X = torch.empty([1, len(X[0])]).to(device)\n","  # print(shuf_X.size(), shuf_X)\n","  shuf_y = torch.empty([1], dtype=torch.long).to(device)\n","  # print(shuf_X.size(), shuf_X)\n","  for feat, label in ls :\n","    # print('randomize_dataset(): feat', feat.reshape([1, -1]).size())\n","    shuf_X = torch.cat((shuf_X, feat.reshape([1, -1])))\n","    # print('randomize_dataset(): label', label.reshape([1]).size())\n","    # label = torch.tensor(label, dtype=torch.long).to(device)\n","    shuf_y = torch.cat( (shuf_y, label.reshape([1])) )\n","  # print(shuf_X)\n","  # print(shuf_y)\n","  # print('randomize_dataset(): shuf_X', shuf_X.size())\n","  # print('randomize_dataset(): shuf_y', shuf_y.size())\n","  return shuf_X[1:], shuf_y[1:]"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"WfhI0Do7Ad_o"},"outputs":[],"source":["def find_seed(X, y, tr_size, tr_or_ts) :\n","  min_std = 1e9\n","  # tr_or_ts = 0  # tr = 0, ts = 1\n","  for i in range(20, 150) :\n","    print(i)\n","    subsets = torch.utils.data.random_split(TensorDataset(X, y),\n","                                            [tr_size, len(y) - tr_size],\n","                                            generator=torch.Generator().manual_seed(i),\n","                                            )\n","    # print(len(subsets),\n","    #       subsets[1].dataset.tensors[1].size(),\n","    #       type(subsets[1].indices),\n","    #       len(subsets[1].indices),\n","    #       subsets[1].dataset.tensors[1][[i for i in subsets[1].indices]],\n","    #       )\n","    std = histo_pipe_dist(subsets[tr_or_ts].dataset.tensors[1][[i for i in subsets[tr_or_ts].indices]])\n","    if std < min_std :\n","      min_std = std\n","      seed = i\n","  print(f'Seed Found: argmin_stdev {seed}\\n')\n","  return seed"]},{"cell_type":"markdown","metadata":{"id":"qysp3xV5W2dg"},"source":["#####MNISTfashion set"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655287024087,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"v2G9BTikRI0k","outputId":"f70617b2-416c-4c48-e177-764c9b4a6afe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntr_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=True,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nts_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=False,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nrows = 28\\ncols = 28\\ninput_dim = rows*cols\\noutput_dim = 10\\n\\nlearning_rate = 1e-3\\nepochs = 10\\nbatch_size = 64\\nmod = 100\\n#'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["# mnist\n","\"\"\"\n","tr_dataset = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","\n","ts_dataset = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","\n","rows = 28\n","cols = 28\n","input_dim = rows*cols\n","output_dim = 10\n","\n","learning_rate = 1e-3\n","epochs = 10\n","batch_size = 64\n","mod = 100\n","#\"\"\""]},{"cell_type":"markdown","metadata":{"id":"uBUCPLjMROO0"},"source":["#####CSV data Loading"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":974},"executionInfo":{"elapsed":25806,"status":"ok","timestamp":1655287049891,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"sPNhrjauC1OW","outputId":"33d43fe5-c005-45ae-d777-96bd5e55dd7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 43.62644958496094, min -23.13990020751953\n","cat_data(): X_raw range 66.76634979248047\n","cat_data(): X_raw torch.Size([5000, 840])\n","SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 41.52079772949219, min -26.361465454101562\n","cat_data(): X_raw range 67.88226318359375\n","cat_data(): X_raw torch.Size([5000, 840])\n","X size: torch.Size([10000, 840])\n","Counter({36: 294, 25: 247, 30: 230, 27: 224, 24: 222, 38: 219, 35: 219, 19: 215, 33: 213, 34: 209, 37: 204, 29: 195, 20: 195, 6: 191, 28: 186, 8: 184, 32: 184, 23: 181, 9: 179, 15: 178, 31: 178, 14: 177, 18: 172, 7: 170, 5: 165, 1: 164, 21: 160, 3: 159, 16: 151, 22: 149, 2: 143, 26: 141, 17: 140, 12: 133, 4: 132, 10: 131, 13: 131, 11: 121, 0: 114})\n","stdev 3.8e+01\n","cols 840\n","Autoencoder(\n","  (encoder): Encoder(\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (binary_STE_stack): Sequential(\n","      (0): StraightThroughEstimator()\n","    )\n","  )\n","  (decoder): Decoder(\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (linear_lrelu_stack): Sequential(\n","      (0): Linear(in_features=840, out_features=512, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=512, out_features=39, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n",")\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Network characterist options:\n","#  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n","#   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n","### Centralize file paths; update them all from here; pass them along through func calls.\n","### Update seed when changing meta parameters outside notebook (e.g. partitions i.e. region count)\n","residual = True  # Subtract measured from base_case; 0 if normal behavior, else non-zero.\n","norm_base = False\n","norm_feats = True   \n","mask = False\n","net_char = 0\n","# tmstp = 80   # Delete if necessary\n","tmstps = [80, 81]\n","# tmstps = [78, 79, 80, 81, 82, 83, 84, 85]\n","entrop_decay = -1/1\n","cat_attrs = True\n","subsample = False\n","if cat_attrs :\n","  net_char = [0, 4]\n","# Update filepaths() when changing meta parameters outside this notebook (e.g. number of regions has changed)\n","(partit_cnt, regdict_filenm, ts_path, dset_size, pToPIdx_filenm,\n","    sensgrid_filenm, loss_filenm, conf_filenm, conf_mat_filenm) = filepaths()\n","\n","# print(regdict_filenm, ts_path, dset_size, pToPIdx_filenm,\n","#     sensgrid_filenm, loss_filenm, conf_filenm, conf_mat_filenm)\n","# assert False\n","\n","# Training data is loaded from csv file constructed in SimData_to_cvs script.\n","# For single tmstp:\n","# X, y, pipIdx = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","# For mult tmstp (requires list of tmstps):\n","X, y, pipIdx = cat_data_mult_tmstps(residual, norm_base, norm_feats, mask, net_char, dset_size, ts_path, tmstps)\n","# print(y)\n","# print(pipIdx)\n","print(f'X size: {X.size()}')\n","# torch.set_printoptions(edgeitems=50)\n","# print(f'Training Set: X[0] {X[0]}')\n","\n","if subsample :\n","  perc_of_meas = 0.01\n","  size = int(X.size(1) * perc_of_meas)\n","  print('subsample: size ', size)\n","  seed = 1001\n","  reduced_meas_X, __ = rand_sub_dataset(X, size, seed)\n","  print('subsample:', reduced_meas_X.size())\n","  # print(X[0, 307], X[0, 536], X[0, 329])\n","  # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n","  X = reduced_meas_X\n","\n","torch.set_printoptions(edgeitems=3)\n","tr_size = int(len(X)*0.7)\n","ts_size = len(y) - tr_size\n","# tr_dataset = TensorDataset(X[:split_idx], y[:split_idx])\n","# ts_dataset = TensorDataset(X[split_idx:], y[split_idx:])\n","# Find a seed that creates a training set pipe distribution with smallest stdev.\n","seed = 87   # find_seed prints and returns best seed.\n","# seed = find_seed(X, y, tr_size, 0)   # Commented out to eliminate the overhead of looking for a seed everytime.\n","# assert False\n","# Two ways to deal w/ pipe labels\n","#  1. add a third element to the datasets (done here)\n","#  2. make y a tuple and decompose in training/test loop\n","tr_dataset, ts_dataset = torch.utils.data.random_split(TensorDataset(X, y, pipIdx),\n","                                        [tr_size, ts_size],\n","                                        generator=torch.Generator().manual_seed(seed),\n","                                        )\n","\n","# Visualize the dataset leak pipe distribution.\n","# X, y = randomize_dataset(X, y)\n","# histo_pipe_dist(y[:split_idx])\n","# histo_pipe_dist(y[split_idx:])\n","idx = tr_dataset.indices\n","histo_pipe_dist(tr_dataset.dataset.tensors[1][[i for i in idx]])\n","\n","# Determine learning rate and measurement vector size\n","if net_char == 0:\n","  # link_flowrate\n","  cols = 444   # links = 444; junctions = 396\n","  # learning_rate = 9e-3   # single layer\n","  learning_rate = 8e-2\n","if net_char == 2:\n","  # link_velocity\n","  cols = 444   # links = 444; junctions = 396\n","  learning_rate = 2e-7\n","elif net_char == 3:\n","  # node_demand\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 2e-6\n","  epochs = 2000\n","elif net_char == 4:\n","  # node_head\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 7e-2\n","  epochs = 2000\n","elif net_char == 5:\n","  # node_pressure\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 1e-3\n","elif isinstance(net_char, list) :\n","  if norm_feats and cat_attrs :\n","    learning_rate = 2e-2\n","  else :\n","    learning_rate = 8e-2   #(f, h, nf, nh, f+h)\n","\n","# Determine number of concatenated vectors. Used for determining input_dim\n","concats = 1\n","if mask :\n","  concats = 1\n","\n","elif norm_feats :\n","  pass\n","# Simplification adjustments\n","cols = X.size(1)\n","print(f'cols {cols}')\n","\n","# Use region_dict to assign output_dim (i.e. # of regions)\n","rows = 1\n","input_dim = rows*cols*concats\n","output_dim = partit_cnt\n","epochs = 10000\n","# epochs = 50   # For testing.\n","batch_size = 128\n","mod = 5\n","\n","# Instantiate model framework\n","# model = Decoder(input_dim, output_dim).to(device)\n","model = Autoencoder(input_dim, output_dim).to(device)\n","print(model)\n","# print(*model.parameters())\n","# print(f\"mask_params {model.state_dict()['encoder.mask_params'].size()}\")\n","# assert False"]},{"cell_type":"markdown","metadata":{"id":"cQdyRZRYXiQJ"},"source":["#####Training"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LItPDTO9LCoY","executionInfo":{"status":"ok","timestamp":1655288808015,"user_tz":420,"elapsed":1758145,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"9bdf24f2-b05c-4e88-c432-2788b9c3c266"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.207621 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.718374 \n","\n","Epoch 3377\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.205823 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.777897 \n","\n","Epoch 3378\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.166208 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.713843 \n","\n","Epoch 3379\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.140581 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.706705 \n","\n","Epoch 3380\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.177108 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.793858 \n","\n","Epoch 3381\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.188494 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.765009 \n","\n","Epoch 3382\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.179058 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.780065 \n","\n","Epoch 3383\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.150871 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.798095 \n","\n","Epoch 3384\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.140946 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.768225 \n","\n","Epoch 3385\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.150371 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.764740 \n","\n","Epoch 3386\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.114992 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.733837 \n","\n","Epoch 3387\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.121689 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.746680 \n","\n","Epoch 3388\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.122141 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.679985 \n","\n","Epoch 3389\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.133976 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.699590 \n","\n","Epoch 3390\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.139769 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.661427 \n","\n","Epoch 3391\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.100061 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.672894 \n","\n","Epoch 3392\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.096528 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.660582 \n","\n","Epoch 3393\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.157227 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.663210 \n","\n","Epoch 3394\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.122247 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.671992 \n","\n","Epoch 3395\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.103724 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.665501 \n","\n","Epoch 3396\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.104526 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.686662 \n","\n","Epoch 3397\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.126071 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.700783 \n","\n","Epoch 3398\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.101969 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.702531 \n","\n","Epoch 3399\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.092828 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.790957 \n","\n","Epoch 3400\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.107454 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.739624 \n","\n","Epoch 3401\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.107361 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.684828 \n","\n","Epoch 3402\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.140226 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.716025 \n","\n","Epoch 3403\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.123750 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.700388 \n","\n","Epoch 3404\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.149242 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.747044 \n","\n","Epoch 3405\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.107511 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.649412 \n","\n","Epoch 3406\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.074566 \n","\n","Test Error: \n"," Accuracy: 51.8%, Avg loss: 1.684294 \n","\n","Epoch 3407\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.092263 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.656104 \n","\n","Epoch 3408\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.097433 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.710495 \n","\n","Epoch 3409\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.066586 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.696379 \n","\n","Epoch 3410\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.091944 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.656817 \n","\n","Epoch 3411\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.091647 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.711962 \n","\n","Epoch 3412\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.124617 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.734166 \n","\n","Epoch 3413\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.106001 \n","\n","Test Error: \n"," Accuracy: 51.5%, Avg loss: 1.687660 \n","\n","Epoch 3414\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.131321 \n","\n","Test Error: \n"," Accuracy: 51.5%, Avg loss: 1.721639 \n","\n","Epoch 3415\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.097776 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.700981 \n","\n","Epoch 3416\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.101811 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.703496 \n","\n","Epoch 3417\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.110003 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.775147 \n","\n","Epoch 3418\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.135356 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.698174 \n","\n","Epoch 3419\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.155991 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.734793 \n","\n","Epoch 3420\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.130511 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.708698 \n","\n","Epoch 3421\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.130103 \n","\n","Test Error: \n"," Accuracy: 52.0%, Avg loss: 1.652243 \n","\n","Epoch 3422\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.158159 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.720303 \n","\n","Epoch 3423\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.125373 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.731716 \n","\n","Epoch 3424\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.149205 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.717102 \n","\n","Epoch 3425\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.139622 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.677373 \n","\n","Epoch 3426\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.173276 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.699896 \n","\n","Epoch 3427\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.116844 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.686226 \n","\n","Epoch 3428\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.149081 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.679933 \n","\n","Epoch 3429\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.094185 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.697917 \n","\n","Epoch 3430\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.109027 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.696043 \n","\n","Epoch 3431\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.138203 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.696253 \n","\n","Epoch 3432\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.142574 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.654484 \n","\n","Epoch 3433\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.097933 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.640332 \n","\n","Epoch 3434\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.094204 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.725800 \n","\n","Epoch 3435\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.108546 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.674621 \n","\n","Epoch 3436\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.107635 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.664229 \n","\n","Epoch 3437\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.085420 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.733811 \n","\n","Epoch 3438\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.154626 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.692277 \n","\n","Epoch 3439\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.131370 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.771286 \n","\n","Epoch 3440\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.146685 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.717393 \n","\n","Epoch 3441\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.116293 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.706631 \n","\n","Epoch 3442\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.136697 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.678114 \n","\n","Epoch 3443\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.142886 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.684979 \n","\n","Epoch 3444\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.133149 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.648425 \n","\n","Epoch 3445\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.115204 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.671257 \n","\n","Epoch 3446\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.104593 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.652592 \n","\n","Epoch 3447\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.103980 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.665107 \n","\n","Epoch 3448\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.095484 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.671596 \n","\n","Epoch 3449\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.084680 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.787087 \n","\n","Epoch 3450\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.085703 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.682533 \n","\n","Epoch 3451\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.104450 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.725132 \n","\n","Epoch 3452\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.133153 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.700658 \n","\n","Epoch 3453\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.126476 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.670788 \n","\n","Epoch 3454\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.185461 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.687248 \n","\n","Epoch 3455\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.160836 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.763784 \n","\n","Epoch 3456\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.165649 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.676124 \n","\n","Epoch 3457\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.131186 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.754926 \n","\n","Epoch 3458\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.147226 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.753211 \n","\n","Epoch 3459\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.100967 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.738457 \n","\n","Epoch 3460\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.107747 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.772858 \n","\n","Epoch 3461\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.192302 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.690262 \n","\n","Epoch 3462\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.199142 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.813266 \n","\n","Epoch 3463\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.200543 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.848233 \n","\n","Epoch 3464\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.202313 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.811628 \n","\n","Epoch 3465\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.239938 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.814871 \n","\n","Epoch 3466\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.169555 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.817776 \n","\n","Epoch 3467\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.229501 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.777474 \n","\n","Epoch 3468\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.206106 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.772044 \n","\n","Epoch 3469\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.184049 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.828604 \n","\n","Epoch 3470\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.147741 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.800274 \n","\n","Epoch 3471\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.169953 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.794288 \n","\n","Epoch 3472\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.209251 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.727003 \n","\n","Epoch 3473\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.124647 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.738436 \n","\n","Epoch 3474\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.108341 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.741011 \n","\n","Epoch 3475\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.151426 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.703276 \n","\n","Epoch 3476\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.100078 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.686281 \n","\n","Epoch 3477\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.095484 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.697595 \n","\n","Epoch 3478\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.077409 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.672100 \n","\n","Epoch 3479\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.082205 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.678434 \n","\n","Epoch 3480\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.074385 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.670476 \n","\n","Epoch 3481\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.094446 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.650381 \n","\n","Epoch 3482\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.075738 \n","\n","Test Error: \n"," Accuracy: 51.8%, Avg loss: 1.642131 \n","\n","Epoch 3483\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.7%, Avg loss: 1.064091 \n","\n","Test Error: \n"," Accuracy: 52.2%, Avg loss: 1.669743 \n","\n","Epoch 3484\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.055914 \n","\n","Test Error: \n"," Accuracy: 51.9%, Avg loss: 1.694531 \n","\n","Epoch 3485\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.092944 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 2.041036 \n","\n","Epoch 3486\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.107136 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.717323 \n","\n","Epoch 3487\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.082626 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.764561 \n","\n","Epoch 3488\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.108271 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.648116 \n","\n","Epoch 3489\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.082332 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.683918 \n","\n","Epoch 3490\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.091163 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.675565 \n","\n","Epoch 3491\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.117116 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.742242 \n","\n","Epoch 3492\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.106561 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.730291 \n","\n","Epoch 3493\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.103406 \n","\n","Test Error: \n"," Accuracy: 51.9%, Avg loss: 1.673856 \n","\n","Epoch 3494\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.155494 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.742555 \n","\n","Epoch 3495\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.101879 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.721528 \n","\n","Epoch 3496\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.134758 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.750881 \n","\n","Epoch 3497\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.111804 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.742239 \n","\n","Epoch 3498\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.122667 \n","\n","Test Error: \n"," Accuracy: 51.9%, Avg loss: 1.699752 \n","\n","Epoch 3499\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.146295 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.750130 \n","\n","Epoch 3500\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.148526 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.755705 \n","\n","Epoch 3501\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.134662 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.752326 \n","\n","Epoch 3502\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.188549 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.703204 \n","\n","Epoch 3503\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.159042 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.736051 \n","\n","Epoch 3504\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.154897 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.719240 \n","\n","Epoch 3505\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.172284 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.759474 \n","\n","Epoch 3506\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.148818 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.708557 \n","\n","Epoch 3507\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.147334 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.822323 \n","\n","Epoch 3508\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.200110 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.744832 \n","\n","Epoch 3509\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.181089 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.719317 \n","\n","Epoch 3510\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.133827 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.716827 \n","\n","Epoch 3511\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.126035 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.724752 \n","\n","Epoch 3512\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.100211 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.700350 \n","\n","Epoch 3513\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.077514 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.647380 \n","\n","Epoch 3514\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.107183 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.709705 \n","\n","Epoch 3515\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.108875 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.743727 \n","\n","Epoch 3516\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.138527 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.749325 \n","\n","Epoch 3517\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.165229 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.777511 \n","\n","Epoch 3518\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.124833 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.733059 \n","\n","Epoch 3519\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.114992 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.705424 \n","\n","Epoch 3520\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.102718 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.751394 \n","\n","Epoch 3521\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.118295 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.695104 \n","\n","Epoch 3522\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.112706 \n","\n","Test Error: \n"," Accuracy: 51.5%, Avg loss: 1.731722 \n","\n","Epoch 3523\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.106116 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.694724 \n","\n","Epoch 3524\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.125710 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.745434 \n","\n","Epoch 3525\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.109726 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.722531 \n","\n","Epoch 3526\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.088440 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.704883 \n","\n","Epoch 3527\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.084068 \n","\n","Test Error: \n"," Accuracy: 51.8%, Avg loss: 1.696665 \n","\n","Epoch 3528\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.102669 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.747427 \n","\n","Epoch 3529\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.077434 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.765725 \n","\n","Epoch 3530\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.125576 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.709642 \n","\n","Epoch 3531\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.099491 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.774915 \n","\n","Epoch 3532\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.087422 \n","\n","Test Error: \n"," Accuracy: 52.0%, Avg loss: 1.688003 \n","\n","Epoch 3533\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.095596 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.719431 \n","\n","Epoch 3534\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.094778 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.746246 \n","\n","Epoch 3535\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.099775 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.728399 \n","\n","Epoch 3536\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.084165 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.718805 \n","\n","Epoch 3537\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.111755 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.694351 \n","\n","Epoch 3538\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.096671 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.749376 \n","\n","Epoch 3539\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.098630 \n","\n","Test Error: \n"," Accuracy: 52.0%, Avg loss: 1.632558 \n","\n","Epoch 3540\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.045566 \n","\n","Test Error: \n"," Accuracy: 52.2%, Avg loss: 1.634139 \n","\n","Epoch 3541\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.031668 \n","\n","Test Error: \n"," Accuracy: 52.2%, Avg loss: 1.635648 \n","\n","Epoch 3542\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.7%, Avg loss: 1.377056 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.667770 \n","\n","Epoch 3543\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.093714 \n","\n","Test Error: \n"," Accuracy: 51.5%, Avg loss: 1.718843 \n","\n","Epoch 3544\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.045406 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.632483 \n","\n","Epoch 3545\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.031012 \n","\n","Test Error: \n"," Accuracy: 52.1%, Avg loss: 1.613927 \n","\n","Epoch 3546\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.040102 \n","\n","Test Error: \n"," Accuracy: 52.5%, Avg loss: 1.626991 \n","\n","Epoch 3547\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.8%, Avg loss: 1.045192 \n","\n","Test Error: \n"," Accuracy: 52.0%, Avg loss: 1.635809 \n","\n","Epoch 3548\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.039528 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.689262 \n","\n","Epoch 3549\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.8%, Avg loss: 1.036892 \n","\n","Test Error: \n"," Accuracy: 52.1%, Avg loss: 1.635134 \n","\n","Epoch 3550\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.8%, Avg loss: 1.052496 \n","\n","Test Error: \n"," Accuracy: 51.8%, Avg loss: 1.647201 \n","\n","Epoch 3551\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.055383 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.672199 \n","\n","Epoch 3552\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.8%, Avg loss: 1.061410 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.677712 \n","\n","Epoch 3553\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.065652 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.674439 \n","\n","Epoch 3554\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.056773 \n","\n","Test Error: \n"," Accuracy: 51.9%, Avg loss: 1.684972 \n","\n","Epoch 3555\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.069686 \n","\n","Test Error: \n"," Accuracy: 52.1%, Avg loss: 1.664299 \n","\n","Epoch 3556\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.059745 \n","\n","Test Error: \n"," Accuracy: 51.5%, Avg loss: 1.656936 \n","\n","Epoch 3557\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.077822 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.675094 \n","\n","Epoch 3558\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.069381 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.679804 \n","\n","Epoch 3559\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.052427 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.655393 \n","\n","Epoch 3560\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.084966 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.666685 \n","\n","Epoch 3561\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.074010 \n","\n","Test Error: \n"," Accuracy: 52.3%, Avg loss: 1.728673 \n","\n","Epoch 3562\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.5%, Avg loss: 1.090088 \n","\n","Test Error: \n"," Accuracy: 51.8%, Avg loss: 1.683055 \n","\n","Epoch 3563\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.084268 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.751824 \n","\n","Epoch 3564\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.4%, Avg loss: 1.092737 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.788930 \n","\n","Epoch 3565\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.3%, Avg loss: 1.071709 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.734685 \n","\n","Epoch 3566\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.6%, Avg loss: 1.095469 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.686793 \n","\n","Epoch 3567\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.102793 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.677401 \n","\n","Epoch 3568\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.085349 \n","\n","Test Error: \n"," Accuracy: 51.3%, Avg loss: 1.692858 \n","\n","Epoch 3569\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.127069 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.698373 \n","\n","Epoch 3570\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.082643 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.691796 \n","\n","Epoch 3571\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.2%, Avg loss: 1.103059 \n","\n","Test Error: \n"," Accuracy: 51.7%, Avg loss: 1.630750 \n","\n","Epoch 3572\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.081442 \n","\n","Test Error: \n"," Accuracy: 52.2%, Avg loss: 1.662275 \n","\n","Epoch 3573\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.097004 \n","\n","Test Error: \n"," Accuracy: 51.5%, Avg loss: 1.644565 \n","\n","Epoch 3574\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.321658 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.817329 \n","\n","Epoch 3575\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.241688 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.839922 \n","\n","Epoch 3576\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.177046 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.759692 \n","\n","Epoch 3577\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.160641 \n","\n","Test Error: \n"," Accuracy: 52.0%, Avg loss: 1.698981 \n","\n","Epoch 3578\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.151824 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.701191 \n","\n","Epoch 3579\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.117525 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.744510 \n","\n","Epoch 3580\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.8%, Avg loss: 1.134289 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.732679 \n","\n","Epoch 3581\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.158398 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.756808 \n","\n","Epoch 3582\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.221926 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.764023 \n","\n","Epoch 3583\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.204589 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.900744 \n","\n","Epoch 3584\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.278506 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.748614 \n","\n","Epoch 3585\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.226889 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.834183 \n","\n","Epoch 3586\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.167580 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.871153 \n","\n","Epoch 3587\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.176643 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.764694 \n","\n","Epoch 3588\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.171612 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.758682 \n","\n","Epoch 3589\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.159634 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.796801 \n","\n","Epoch 3590\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.226645 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.798075 \n","\n","Epoch 3591\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.243249 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.849702 \n","\n","Epoch 3592\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.242650 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.924285 \n","\n","Epoch 3593\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.212429 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.785608 \n","\n","Epoch 3594\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.236719 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.775582 \n","\n","Epoch 3595\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.313052 \n","\n","Test Error: \n"," Accuracy: 48.4%, Avg loss: 1.969303 \n","\n","Epoch 3596\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.409678 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.977554 \n","\n","Epoch 3597\n","-------------------------------\n","Training Error: \n"," Accuracy: 50.1%, Avg loss: 1.757290 \n","\n","Test Error: \n"," Accuracy: 46.8%, Avg loss: 2.126805 \n","\n","Epoch 3598\n","-------------------------------\n","Training Error: \n"," Accuracy: 51.6%, Avg loss: 1.577271 \n","\n","Test Error: \n"," Accuracy: 46.8%, Avg loss: 2.112069 \n","\n","Epoch 3599\n","-------------------------------\n","Training Error: \n"," Accuracy: 51.7%, Avg loss: 1.579514 \n","\n","Test Error: \n"," Accuracy: 45.4%, Avg loss: 2.241370 \n","\n","Epoch 3600\n","-------------------------------\n","Training Error: \n"," Accuracy: 51.9%, Avg loss: 1.479652 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 2.156819 \n","\n","Epoch 3601\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.4%, Avg loss: 1.457055 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 2.016139 \n","\n","Epoch 3602\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.441806 \n","\n","Test Error: \n"," Accuracy: 46.4%, Avg loss: 2.025246 \n","\n","Epoch 3603\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.1%, Avg loss: 1.442936 \n","\n","Test Error: \n"," Accuracy: 46.3%, Avg loss: 2.022799 \n","\n","Epoch 3604\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.9%, Avg loss: 1.436769 \n","\n","Test Error: \n"," Accuracy: 47.7%, Avg loss: 2.048430 \n","\n","Epoch 3605\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.369638 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.956721 \n","\n","Epoch 3606\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.361413 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 2.018916 \n","\n","Epoch 3607\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.330772 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 2.135211 \n","\n","Epoch 3608\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.352280 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.937022 \n","\n","Epoch 3609\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.321615 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.955952 \n","\n","Epoch 3610\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.356932 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.785401 \n","\n","Epoch 3611\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.296013 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.847893 \n","\n","Epoch 3612\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.362523 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.901207 \n","\n","Epoch 3613\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.324033 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.927793 \n","\n","Epoch 3614\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.316816 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.946650 \n","\n","Epoch 3615\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.314188 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.937491 \n","\n","Epoch 3616\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.283917 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.863670 \n","\n","Epoch 3617\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.288734 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.835794 \n","\n","Epoch 3618\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.311778 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 2.023103 \n","\n","Epoch 3619\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.343841 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.927563 \n","\n","Epoch 3620\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.337626 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.884623 \n","\n","Epoch 3621\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.227796 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.901149 \n","\n","Epoch 3622\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.257453 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.899057 \n","\n","Epoch 3623\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.290196 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.836113 \n","\n","Epoch 3624\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.284631 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.920055 \n","\n","Epoch 3625\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.335942 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.877320 \n","\n","Epoch 3626\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.270979 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.798911 \n","\n","Epoch 3627\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.232845 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.857592 \n","\n","Epoch 3628\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.165880 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.816909 \n","\n","Epoch 3629\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.209273 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.771672 \n","\n","Epoch 3630\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.188905 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.830374 \n","\n","Epoch 3631\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.195679 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.861797 \n","\n","Epoch 3632\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.186333 \n","\n","Test Error: \n"," Accuracy: 48.4%, Avg loss: 1.805045 \n","\n","Epoch 3633\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.272391 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.865862 \n","\n","Epoch 3634\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.200119 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.878457 \n","\n","Epoch 3635\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.189070 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.838446 \n","\n","Epoch 3636\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.208419 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.709341 \n","\n","Epoch 3637\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.176580 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.801711 \n","\n","Epoch 3638\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.218108 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.789842 \n","\n","Epoch 3639\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.180054 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.741084 \n","\n","Epoch 3640\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.218368 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.821897 \n","\n","Epoch 3641\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.220161 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.769030 \n","\n","Epoch 3642\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.169528 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.830190 \n","\n","Epoch 3643\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.165106 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.768105 \n","\n","Epoch 3644\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.221214 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.780808 \n","\n","Epoch 3645\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.175233 \n","\n","Test Error: \n"," Accuracy: 51.1%, Avg loss: 1.753652 \n","\n","Epoch 3646\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.214532 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.829273 \n","\n","Epoch 3647\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.234922 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 2.143564 \n","\n","Epoch 3648\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.201668 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.783575 \n","\n","Epoch 3649\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.269703 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.794379 \n","\n","Epoch 3650\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.199865 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.850629 \n","\n","Epoch 3651\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.248166 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.868527 \n","\n","Epoch 3652\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.192878 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.866947 \n","\n","Epoch 3653\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.249591 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.811470 \n","\n","Epoch 3654\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.224343 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.857709 \n","\n","Epoch 3655\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.271096 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.922098 \n","\n","Epoch 3656\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.226144 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.789323 \n","\n","Epoch 3657\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.227540 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.819474 \n","\n","Epoch 3658\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.249821 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.888273 \n","\n","Epoch 3659\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.248451 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.848982 \n","\n","Epoch 3660\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.269792 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.909972 \n","\n","Epoch 3661\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.236650 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.909366 \n","\n","Epoch 3662\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.262850 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.794388 \n","\n","Epoch 3663\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.218282 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.795209 \n","\n","Epoch 3664\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.226013 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.834189 \n","\n","Epoch 3665\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.188003 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.803564 \n","\n","Epoch 3666\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.192219 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.771906 \n","\n","Epoch 3667\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.162872 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.801657 \n","\n","Epoch 3668\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.171979 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.848347 \n","\n","Epoch 3669\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.158824 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.838177 \n","\n","Epoch 3670\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.166482 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.737343 \n","\n","Epoch 3671\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.149652 \n","\n","Test Error: \n"," Accuracy: 51.0%, Avg loss: 1.701752 \n","\n","Epoch 3672\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.146743 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.755560 \n","\n","Epoch 3673\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.144797 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.744717 \n","\n","Epoch 3674\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.134481 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.751453 \n","\n","Epoch 3675\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.124032 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.733080 \n","\n","Epoch 3676\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.104633 \n","\n","Test Error: \n"," Accuracy: 51.2%, Avg loss: 1.683315 \n","\n","Epoch 3677\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.9%, Avg loss: 1.136085 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.762389 \n","\n","Epoch 3678\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.1%, Avg loss: 1.129354 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.722981 \n","\n","Epoch 3679\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.110567 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.714371 \n","\n","Epoch 3680\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.119795 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.706323 \n","\n","Epoch 3681\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.149617 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.643448 \n","\n","Epoch 3682\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.141322 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.770069 \n","\n","Epoch 3683\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.156892 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.734237 \n","\n","Epoch 3684\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.211393 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.881730 \n","\n","Epoch 3685\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.261900 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.832301 \n","\n","Epoch 3686\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.170846 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.831984 \n","\n","Epoch 3687\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.195661 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.774675 \n","\n","Epoch 3688\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.214425 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.773180 \n","\n","Epoch 3689\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.145118 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.784093 \n","\n","Epoch 3690\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.149986 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.784272 \n","\n","Epoch 3691\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.171960 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.741653 \n","\n","Epoch 3692\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.143740 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.731849 \n","\n","Epoch 3693\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.148270 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.751577 \n","\n","Epoch 3694\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.135069 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.767398 \n","\n","Epoch 3695\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.7%, Avg loss: 1.113860 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.752926 \n","\n","Epoch 3696\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.116358 \n","\n","Test Error: \n"," Accuracy: 51.8%, Avg loss: 1.715748 \n","\n","Epoch 3697\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.6%, Avg loss: 1.146431 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.695706 \n","\n","Epoch 3698\n","-------------------------------\n","Training Error: \n"," Accuracy: 57.0%, Avg loss: 1.107791 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.704589 \n","\n","Epoch 3699\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.135399 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.788612 \n","\n","Epoch 3700\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.119084 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.770327 \n","\n","Epoch 3701\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.158460 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.761445 \n","\n","Epoch 3702\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.180846 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.764270 \n","\n","Epoch 3703\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.148421 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.792441 \n","\n","Epoch 3704\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.229487 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.715122 \n","\n","Epoch 3705\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.170697 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.695680 \n","\n","Epoch 3706\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.175709 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.774897 \n","\n","Epoch 3707\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.133302 \n","\n","Test Error: \n"," Accuracy: 50.8%, Avg loss: 1.710127 \n","\n","Epoch 3708\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.135588 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.700973 \n","\n","Epoch 3709\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.2%, Avg loss: 1.170489 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.780707 \n","\n","Epoch 3710\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.4%, Avg loss: 1.135796 \n","\n","Test Error: \n"," Accuracy: 51.6%, Avg loss: 1.708557 \n","\n","Epoch 3711\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.144919 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.725372 \n","\n","Epoch 3712\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.129777 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.705362 \n","\n","Epoch 3713\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.122741 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.676881 \n","\n","Epoch 3714\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.136192 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.743877 \n","\n","Epoch 3715\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.248616 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.807228 \n","\n","Epoch 3716\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.218911 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.793441 \n","\n","Epoch 3717\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.259449 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.816257 \n","\n","Epoch 3718\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.271756 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.870989 \n","\n","Epoch 3719\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.235977 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.818000 \n","\n","Epoch 3720\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.264651 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.662738 \n","\n","Epoch 3721\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.297771 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.834648 \n","\n","Epoch 3722\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.306601 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.972240 \n","\n","Epoch 3723\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.290182 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 2.105862 \n","\n","Epoch 3724\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.276728 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.873629 \n","\n","Epoch 3725\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.243172 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.886156 \n","\n","Epoch 3726\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.280113 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.905542 \n","\n","Epoch 3727\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.292817 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.887293 \n","\n","Epoch 3728\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.283410 \n","\n","Test Error: \n"," Accuracy: 47.7%, Avg loss: 1.828565 \n","\n","Epoch 3729\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.304169 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.906599 \n","\n","Epoch 3730\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.345471 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.876364 \n","\n","Epoch 3731\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.270443 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.929140 \n","\n","Epoch 3732\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.274465 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.787554 \n","\n","Epoch 3733\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.245699 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.893006 \n","\n","Epoch 3734\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.229099 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.803681 \n","\n","Epoch 3735\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.268465 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.904729 \n","\n","Epoch 3736\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.264636 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.864309 \n","\n","Epoch 3737\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.404316 \n","\n","Test Error: \n"," Accuracy: 47.5%, Avg loss: 1.956729 \n","\n","Epoch 3738\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.334648 \n","\n","Test Error: \n"," Accuracy: 46.7%, Avg loss: 2.078840 \n","\n","Epoch 3739\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.9%, Avg loss: 1.343141 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 2.001894 \n","\n","Epoch 3740\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.295358 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.910234 \n","\n","Epoch 3741\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.376146 \n","\n","Test Error: \n"," Accuracy: 46.8%, Avg loss: 2.010881 \n","\n","Epoch 3742\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.5%, Avg loss: 1.342727 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.918712 \n","\n","Epoch 3743\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.344827 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.883199 \n","\n","Epoch 3744\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.275274 \n","\n","Test Error: \n"," Accuracy: 47.7%, Avg loss: 2.002823 \n","\n","Epoch 3745\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.286655 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.951980 \n","\n","Epoch 3746\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.246569 \n","\n","Test Error: \n"," Accuracy: 47.4%, Avg loss: 1.897986 \n","\n","Epoch 3747\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.278296 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.956574 \n","\n","Epoch 3748\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.245046 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.910557 \n","\n","Epoch 3749\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.255557 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.816983 \n","\n","Epoch 3750\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.247534 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.893628 \n","\n","Epoch 3751\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.228792 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.887763 \n","\n","Epoch 3752\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.217726 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.902866 \n","\n","Epoch 3753\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.245451 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.817627 \n","\n","Epoch 3754\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.261508 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.840935 \n","\n","Epoch 3755\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.234923 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.767754 \n","\n","Epoch 3756\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.243052 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.817027 \n","\n","Epoch 3757\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.249949 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.837056 \n","\n","Epoch 3758\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.195298 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.758728 \n","\n","Epoch 3759\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.222845 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.834418 \n","\n","Epoch 3760\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.178822 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.839197 \n","\n","Epoch 3761\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.218774 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.850037 \n","\n","Epoch 3762\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.396173 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.783091 \n","\n","Epoch 3763\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.153407 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.770685 \n","\n","Epoch 3764\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.165395 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.771237 \n","\n","Epoch 3765\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.143924 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.792971 \n","\n","Epoch 3766\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.188414 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.775845 \n","\n","Epoch 3767\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.185462 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.755824 \n","\n","Epoch 3768\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.208451 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.792589 \n","\n","Epoch 3769\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.205558 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.864821 \n","\n","Epoch 3770\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.212574 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.841423 \n","\n","Epoch 3771\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.168321 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.892306 \n","\n","Epoch 3772\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.186906 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.812547 \n","\n","Epoch 3773\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.169403 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.834831 \n","\n","Epoch 3774\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.194657 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.775437 \n","\n","Epoch 3775\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.164838 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.824461 \n","\n","Epoch 3776\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.207925 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.812574 \n","\n","Epoch 3777\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.216323 \n","\n","Test Error: \n"," Accuracy: 50.9%, Avg loss: 1.780972 \n","\n","Epoch 3778\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.173201 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.803549 \n","\n","Epoch 3779\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.282242 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.851712 \n","\n","Epoch 3780\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.262068 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.871530 \n","\n","Epoch 3781\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.225088 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.817625 \n","\n","Epoch 3782\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.252718 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.799409 \n","\n","Epoch 3783\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.294708 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.930484 \n","\n","Epoch 3784\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.291618 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.968923 \n","\n","Epoch 3785\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.323681 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 2.016077 \n","\n","Epoch 3786\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.2%, Avg loss: 1.445185 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.983801 \n","\n","Epoch 3787\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.7%, Avg loss: 1.372952 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 2.008334 \n","\n","Epoch 3788\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.4%, Avg loss: 1.367868 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.961353 \n","\n","Epoch 3789\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.297262 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.948124 \n","\n","Epoch 3790\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.353514 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.916828 \n","\n","Epoch 3791\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.293365 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 1.935664 \n","\n","Epoch 3792\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.360941 \n","\n","Test Error: \n"," Accuracy: 47.5%, Avg loss: 1.871035 \n","\n","Epoch 3793\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.365400 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.932026 \n","\n","Epoch 3794\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.9%, Avg loss: 1.329368 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.922826 \n","\n","Epoch 3795\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.3%, Avg loss: 1.383289 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.944125 \n","\n","Epoch 3796\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.2%, Avg loss: 1.363226 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.963009 \n","\n","Epoch 3797\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.300748 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.863027 \n","\n","Epoch 3798\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.363504 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.887350 \n","\n","Epoch 3799\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.7%, Avg loss: 1.364510 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.850903 \n","\n","Epoch 3800\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.273568 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.936016 \n","\n","Epoch 3801\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.334518 \n","\n","Test Error: \n"," Accuracy: 48.4%, Avg loss: 1.886804 \n","\n","Epoch 3802\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.348276 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 2.032478 \n","\n","Epoch 3803\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.9%, Avg loss: 1.355055 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.920388 \n","\n","Epoch 3804\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.351028 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 2.044066 \n","\n","Epoch 3805\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.7%, Avg loss: 1.322454 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 1.891678 \n","\n","Epoch 3806\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.6%, Avg loss: 1.341487 \n","\n","Test Error: \n"," Accuracy: 47.5%, Avg loss: 1.896158 \n","\n","Epoch 3807\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.346191 \n","\n","Test Error: \n"," Accuracy: 47.7%, Avg loss: 1.934994 \n","\n","Epoch 3808\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.3%, Avg loss: 1.435202 \n","\n","Test Error: \n"," Accuracy: 47.4%, Avg loss: 1.897867 \n","\n","Epoch 3809\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.6%, Avg loss: 1.330137 \n","\n","Test Error: \n"," Accuracy: 46.9%, Avg loss: 1.931148 \n","\n","Epoch 3810\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.1%, Avg loss: 1.372058 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.975035 \n","\n","Epoch 3811\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.371053 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.894065 \n","\n","Epoch 3812\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.330215 \n","\n","Test Error: \n"," Accuracy: 46.9%, Avg loss: 1.909551 \n","\n","Epoch 3813\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.323509 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.869137 \n","\n","Epoch 3814\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.263821 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.853650 \n","\n","Epoch 3815\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.333478 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 2.079967 \n","\n","Epoch 3816\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.280822 \n","\n","Test Error: \n"," Accuracy: 47.0%, Avg loss: 1.866658 \n","\n","Epoch 3817\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.345385 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.874595 \n","\n","Epoch 3818\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.1%, Avg loss: 1.366021 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.893599 \n","\n","Epoch 3819\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.342985 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.946065 \n","\n","Epoch 3820\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.7%, Avg loss: 1.362211 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.860557 \n","\n","Epoch 3821\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.355632 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.898505 \n","\n","Epoch 3822\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.291365 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.890165 \n","\n","Epoch 3823\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.324941 \n","\n","Test Error: \n"," Accuracy: 47.7%, Avg loss: 1.932148 \n","\n","Epoch 3824\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.252827 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.882979 \n","\n","Epoch 3825\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.312581 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.882692 \n","\n","Epoch 3826\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.271739 \n","\n","Test Error: \n"," Accuracy: 46.6%, Avg loss: 1.992110 \n","\n","Epoch 3827\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.3%, Avg loss: 1.370270 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.920358 \n","\n","Epoch 3828\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.6%, Avg loss: 1.406507 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.911537 \n","\n","Epoch 3829\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.342769 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.926205 \n","\n","Epoch 3830\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.315945 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.956455 \n","\n","Epoch 3831\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.4%, Avg loss: 1.360947 \n","\n","Test Error: \n"," Accuracy: 47.3%, Avg loss: 1.893768 \n","\n","Epoch 3832\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.6%, Avg loss: 1.331061 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.844657 \n","\n","Epoch 3833\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.1%, Avg loss: 1.352004 \n","\n","Test Error: \n"," Accuracy: 48.4%, Avg loss: 1.755298 \n","\n","Epoch 3834\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.2%, Avg loss: 1.328235 \n","\n","Test Error: \n"," Accuracy: 47.0%, Avg loss: 1.898283 \n","\n","Epoch 3835\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.336852 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.968524 \n","\n","Epoch 3836\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.316475 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 1.885430 \n","\n","Epoch 3837\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.7%, Avg loss: 1.327048 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.750722 \n","\n","Epoch 3838\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.309351 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.802303 \n","\n","Epoch 3839\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.277791 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.824664 \n","\n","Epoch 3840\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.278814 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.895083 \n","\n","Epoch 3841\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.267146 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.876574 \n","\n","Epoch 3842\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.237750 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.806289 \n","\n","Epoch 3843\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.271310 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.838713 \n","\n","Epoch 3844\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.236911 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.817444 \n","\n","Epoch 3845\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.247232 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.886813 \n","\n","Epoch 3846\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.297874 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.787184 \n","\n","Epoch 3847\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.264321 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.857573 \n","\n","Epoch 3848\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.249953 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.838698 \n","\n","Epoch 3849\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.271976 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.765730 \n","\n","Epoch 3850\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.270939 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.848233 \n","\n","Epoch 3851\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.223961 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.926528 \n","\n","Epoch 3852\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.252055 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.758854 \n","\n","Epoch 3853\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.249324 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.892192 \n","\n","Epoch 3854\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.210717 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.813654 \n","\n","Epoch 3855\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.257552 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.871727 \n","\n","Epoch 3856\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.299643 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.782479 \n","\n","Epoch 3857\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.292332 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.849376 \n","\n","Epoch 3858\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.268807 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.835792 \n","\n","Epoch 3859\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.276469 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.863251 \n","\n","Epoch 3860\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.9%, Avg loss: 1.292368 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.865324 \n","\n","Epoch 3861\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.295060 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.801467 \n","\n","Epoch 3862\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.287364 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 1.880428 \n","\n","Epoch 3863\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.252844 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.822974 \n","\n","Epoch 3864\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.218661 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.816666 \n","\n","Epoch 3865\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.288492 \n","\n","Test Error: \n"," Accuracy: 47.4%, Avg loss: 1.919275 \n","\n","Epoch 3866\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.5%, Avg loss: 1.352074 \n","\n","Test Error: \n"," Accuracy: 47.3%, Avg loss: 1.859408 \n","\n","Epoch 3867\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.8%, Avg loss: 1.354605 \n","\n","Test Error: \n"," Accuracy: 46.8%, Avg loss: 1.955588 \n","\n","Epoch 3868\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.307334 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.850804 \n","\n","Epoch 3869\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.319857 \n","\n","Test Error: \n"," Accuracy: 47.1%, Avg loss: 1.885130 \n","\n","Epoch 3870\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.298330 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.879522 \n","\n","Epoch 3871\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.5%, Avg loss: 1.280337 \n","\n","Test Error: \n"," Accuracy: 47.2%, Avg loss: 1.888978 \n","\n","Epoch 3872\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.299523 \n","\n","Test Error: \n"," Accuracy: 47.8%, Avg loss: 1.861388 \n","\n","Epoch 3873\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.236681 \n","\n","Test Error: \n"," Accuracy: 47.7%, Avg loss: 1.867698 \n","\n","Epoch 3874\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.245103 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.808569 \n","\n","Epoch 3875\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.310712 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.796467 \n","\n","Epoch 3876\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.298934 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.968962 \n","\n","Epoch 3877\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.287241 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.811218 \n","\n","Epoch 3878\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.320138 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.820961 \n","\n","Epoch 3879\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.304515 \n","\n","Test Error: \n"," Accuracy: 47.5%, Avg loss: 1.930164 \n","\n","Epoch 3880\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.300281 \n","\n","Test Error: \n"," Accuracy: 47.3%, Avg loss: 1.852756 \n","\n","Epoch 3881\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.6%, Avg loss: 1.353537 \n","\n","Test Error: \n"," Accuracy: 47.5%, Avg loss: 1.769261 \n","\n","Epoch 3882\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.254773 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 1.816916 \n","\n","Epoch 3883\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.280902 \n","\n","Test Error: \n"," Accuracy: 47.3%, Avg loss: 1.822496 \n","\n","Epoch 3884\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.286241 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.832342 \n","\n","Epoch 3885\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.282080 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.825850 \n","\n","Epoch 3886\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.277021 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.807602 \n","\n","Epoch 3887\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.263275 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.834289 \n","\n","Epoch 3888\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.261204 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.840650 \n","\n","Epoch 3889\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.220245 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.781007 \n","\n","Epoch 3890\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.236781 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.822894 \n","\n","Epoch 3891\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.230841 \n","\n","Test Error: \n"," Accuracy: 48.1%, Avg loss: 1.886732 \n","\n","Epoch 3892\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.290163 \n","\n","Test Error: \n"," Accuracy: 47.4%, Avg loss: 1.932881 \n","\n","Epoch 3893\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.240797 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.831654 \n","\n","Epoch 3894\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.244556 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.774376 \n","\n","Epoch 3895\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.235287 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.774338 \n","\n","Epoch 3896\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.211570 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.885700 \n","\n","Epoch 3897\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.244717 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.893128 \n","\n","Epoch 3898\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.259318 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.781657 \n","\n","Epoch 3899\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.242706 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.950723 \n","\n","Epoch 3900\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.5%, Avg loss: 1.345515 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.853589 \n","\n","Epoch 3901\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.286084 \n","\n","Test Error: \n"," Accuracy: 47.5%, Avg loss: 1.855488 \n","\n","Epoch 3902\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.272604 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.773872 \n","\n","Epoch 3903\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.197529 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.709399 \n","\n","Epoch 3904\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.175108 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.760554 \n","\n","Epoch 3905\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.182821 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.757966 \n","\n","Epoch 3906\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.232387 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.831044 \n","\n","Epoch 3907\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.251534 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.765078 \n","\n","Epoch 3908\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.234297 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.831510 \n","\n","Epoch 3909\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.223825 \n","\n","Test Error: \n"," Accuracy: 48.8%, Avg loss: 1.809485 \n","\n","Epoch 3910\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.195443 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.787584 \n","\n","Epoch 3911\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.190328 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.776069 \n","\n","Epoch 3912\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.219666 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.797972 \n","\n","Epoch 3913\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.221782 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.792670 \n","\n","Epoch 3914\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.195707 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.780592 \n","\n","Epoch 3915\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.218798 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.843306 \n","\n","Epoch 3916\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.239740 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.818025 \n","\n","Epoch 3917\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.200842 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.789412 \n","\n","Epoch 3918\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.236538 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.775477 \n","\n","Epoch 3919\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.231661 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.786250 \n","\n","Epoch 3920\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.232304 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.832450 \n","\n","Epoch 3921\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.259146 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.767876 \n","\n","Epoch 3922\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.213578 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.674041 \n","\n","Epoch 3923\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.8%, Avg loss: 1.165887 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.780636 \n","\n","Epoch 3924\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.175131 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.759044 \n","\n","Epoch 3925\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.170697 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.749142 \n","\n","Epoch 3926\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.175919 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.807758 \n","\n","Epoch 3927\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.3%, Avg loss: 1.199257 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.758774 \n","\n","Epoch 3928\n","-------------------------------\n","Training Error: \n"," Accuracy: 52.1%, Avg loss: 1.438290 \n","\n","Test Error: \n"," Accuracy: 47.6%, Avg loss: 2.054505 \n","\n","Epoch 3929\n","-------------------------------\n","Training Error: \n"," Accuracy: 51.7%, Avg loss: 1.393073 \n","\n","Test Error: \n"," Accuracy: 47.0%, Avg loss: 1.979095 \n","\n","Epoch 3930\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.2%, Avg loss: 1.355882 \n","\n","Test Error: \n"," Accuracy: 48.7%, Avg loss: 1.934549 \n","\n","Epoch 3931\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.283834 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.866872 \n","\n","Epoch 3932\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.295720 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.890892 \n","\n","Epoch 3933\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.6%, Avg loss: 1.261120 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.853699 \n","\n","Epoch 3934\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.252203 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.824358 \n","\n","Epoch 3935\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.272164 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.840013 \n","\n","Epoch 3936\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.0%, Avg loss: 1.252931 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.799256 \n","\n","Epoch 3937\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.269327 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.883806 \n","\n","Epoch 3938\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.5%, Avg loss: 1.269928 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.780196 \n","\n","Epoch 3939\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.278316 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.846296 \n","\n","Epoch 3940\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.1%, Avg loss: 1.284656 \n","\n","Test Error: \n"," Accuracy: 48.2%, Avg loss: 1.841832 \n","\n","Epoch 3941\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.4%, Avg loss: 1.256876 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.802866 \n","\n","Epoch 3942\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.215884 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.782239 \n","\n","Epoch 3943\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.209126 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.792482 \n","\n","Epoch 3944\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.221121 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.803208 \n","\n","Epoch 3945\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.221487 \n","\n","Test Error: \n"," Accuracy: 48.5%, Avg loss: 1.706545 \n","\n","Epoch 3946\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.1%, Avg loss: 1.234061 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.798109 \n","\n","Epoch 3947\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.3%, Avg loss: 1.246012 \n","\n","Test Error: \n"," Accuracy: 48.0%, Avg loss: 1.812359 \n","\n","Epoch 3948\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.2%, Avg loss: 1.274568 \n","\n","Test Error: \n"," Accuracy: 47.9%, Avg loss: 1.809173 \n","\n","Epoch 3949\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.9%, Avg loss: 1.227775 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.757135 \n","\n","Epoch 3950\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.8%, Avg loss: 1.246005 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.775537 \n","\n","Epoch 3951\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.269546 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.763641 \n","\n","Epoch 3952\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.214695 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.763368 \n","\n","Epoch 3953\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.7%, Avg loss: 1.210755 \n","\n","Test Error: \n"," Accuracy: 48.6%, Avg loss: 1.782653 \n","\n","Epoch 3954\n","-------------------------------\n","Training Error: \n"," Accuracy: 53.7%, Avg loss: 1.281203 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.718183 \n","\n","Epoch 3955\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.5%, Avg loss: 1.210161 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.800337 \n","\n","Epoch 3956\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.0%, Avg loss: 1.241113 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.768781 \n","\n","Epoch 3957\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.171506 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.753663 \n","\n","Epoch 3958\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.169226 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.717724 \n","\n","Epoch 3959\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.160802 \n","\n","Test Error: \n"," Accuracy: 51.4%, Avg loss: 1.709280 \n","\n","Epoch 3960\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.176899 \n","\n","Test Error: \n"," Accuracy: 50.5%, Avg loss: 1.700420 \n","\n","Epoch 3961\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.168201 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.705008 \n","\n","Epoch 3962\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.161429 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.614978 \n","\n","Epoch 3963\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.148629 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.713256 \n","\n","Epoch 3964\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.139328 \n","\n","Test Error: \n"," Accuracy: 50.7%, Avg loss: 1.714354 \n","\n","Epoch 3965\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.144369 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.725367 \n","\n","Epoch 3966\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.136820 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.701136 \n","\n","Epoch 3967\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.163886 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.757100 \n","\n","Epoch 3968\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.140975 \n","\n","Test Error: \n"," Accuracy: 49.8%, Avg loss: 1.723828 \n","\n","Epoch 3969\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.135754 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.710552 \n","\n","Epoch 3970\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.9%, Avg loss: 1.128570 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.708356 \n","\n","Epoch 3971\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.128810 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.677493 \n","\n","Epoch 3972\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.0%, Avg loss: 1.136325 \n","\n","Test Error: \n"," Accuracy: 49.9%, Avg loss: 1.739109 \n","\n","Epoch 3973\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.5%, Avg loss: 1.120739 \n","\n","Test Error: \n"," Accuracy: 49.6%, Avg loss: 1.723267 \n","\n","Epoch 3974\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.142375 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.665184 \n","\n","Epoch 3975\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.124198 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.693688 \n","\n","Epoch 3976\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.8%, Avg loss: 1.141362 \n","\n","Test Error: \n"," Accuracy: 48.9%, Avg loss: 1.754434 \n","\n","Epoch 3977\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.143464 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.727753 \n","\n","Epoch 3978\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.161076 \n","\n","Test Error: \n"," Accuracy: 50.1%, Avg loss: 1.752941 \n","\n","Epoch 3979\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.150360 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.724307 \n","\n","Epoch 3980\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.139168 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.727439 \n","\n","Epoch 3981\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.6%, Avg loss: 1.128575 \n","\n","Test Error: \n"," Accuracy: 50.0%, Avg loss: 1.743083 \n","\n","Epoch 3982\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.1%, Avg loss: 1.168073 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.799713 \n","\n","Epoch 3983\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.157759 \n","\n","Test Error: \n"," Accuracy: 50.6%, Avg loss: 1.704369 \n","\n","Epoch 3984\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.152838 \n","\n","Test Error: \n"," Accuracy: 50.3%, Avg loss: 1.727778 \n","\n","Epoch 3985\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.9%, Avg loss: 1.209980 \n","\n","Test Error: \n"," Accuracy: 49.3%, Avg loss: 1.739085 \n","\n","Epoch 3986\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.181377 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.749755 \n","\n","Epoch 3987\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.182268 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.732842 \n","\n","Epoch 3988\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.150640 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.727940 \n","\n","Epoch 3989\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.0%, Avg loss: 1.156700 \n","\n","Test Error: \n"," Accuracy: 49.1%, Avg loss: 1.726336 \n","\n","Epoch 3990\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.7%, Avg loss: 1.155224 \n","\n","Test Error: \n"," Accuracy: 49.2%, Avg loss: 1.717908 \n","\n","Epoch 3991\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.3%, Avg loss: 1.152048 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.678462 \n","\n","Epoch 3992\n","-------------------------------\n","Training Error: \n"," Accuracy: 56.1%, Avg loss: 1.128021 \n","\n","Test Error: \n"," Accuracy: 50.4%, Avg loss: 1.739389 \n","\n","Epoch 3993\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.4%, Avg loss: 1.146654 \n","\n","Test Error: \n"," Accuracy: 48.3%, Avg loss: 1.766039 \n","\n","Epoch 3994\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.6%, Avg loss: 1.186578 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.713197 \n","\n","Epoch 3995\n","-------------------------------\n","Training Error: \n"," Accuracy: 54.4%, Avg loss: 1.182801 \n","\n","Test Error: \n"," Accuracy: 49.4%, Avg loss: 1.727501 \n","\n","Epoch 3996\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.162509 \n","\n","Test Error: \n"," Accuracy: 49.0%, Avg loss: 1.760133 \n","\n","Epoch 3997\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.145998 \n","\n","Test Error: \n"," Accuracy: 50.2%, Avg loss: 1.637506 \n","\n","Epoch 3998\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.5%, Avg loss: 1.163820 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.726508 \n","\n","Epoch 3999\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.3%, Avg loss: 1.161726 \n","\n","Test Error: \n"," Accuracy: 49.5%, Avg loss: 1.740122 \n","\n","Epoch 4000\n","-------------------------------\n","Training Error: \n"," Accuracy: 55.2%, Avg loss: 1.170818 \n","\n","Test Error: \n"," Accuracy: 49.7%, Avg loss: 1.683312 \n","\n","Done!\n"]}],"source":["# Training setup and loop\n","\n","# Initialize the loss function\n","#  nn.CrossEntropyLoss() encapsulates nn.LogSoftmax and nn.NLLLoss\n","loss_fn = nn.CrossEntropyLoss()\n","# Parameter adjustment protocol\n","# He: always start w/ Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# For plotting results\n","animator = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0.0, 1.0],\n","                        legend=['train loss', 'train acc', 'test acc'])\n","binMaskSizeLs = []\n","\n","# Load pipe_to_pipeidx numpy file (for use w/ confussion matrix)\n","pToPIdx_dic = np.load(pToPIdx_filenm, allow_pickle='TRUE').item()   # pToPIdx_filenm from filepaths() call above.\n","# print(f'pToPIdx dict: {pToPIdx_dic}')\n","# print(len(pToPIdx_dic))   # dicts have lengths.\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=False)\n","    # for x,y in train_dataloader:\n","    #   print(x,y)\n","    #   break\n","    tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, entrop_decay, loss_fn, optimizer, pToPIdx_dic, epoch=t+1, mod=mod)\n","    # tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, loss_fn, optimizer, epoch=t+1, mod=mod)\n","    train_metrics = (tr_loss, __)\n","\n","    test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n","    test_acc, confusion_matrix = test_loop(test_dataloader, model, loss_fn, pToPIdx_dic, out_dim=output_dim)\n","    # animator\n","    animator.add(t + 1, train_metrics + (test_acc,))\n","\n","torch.save(model.state_dict(), sensgrid_filenm)   # sensgrid_filenm from filepaths() call above.\n","\n","# Not sure the following block is necessary\n","# train_loss, train_acc = train_metrics\n","# assert train_loss < 0.5, train_loss\n","# assert train_acc <= 1 and train_acc > 0.7, train_acc\n","# assert test_acc <= 1 and test_acc > 0.7, test_acc\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"output_embedded_package_id":"1knHgya57m1YeCUSMGDkUvpAdiNubkPjb","base_uri":"https://localhost:8080/","height":1000},"id":"2gTMxTXWcdMz","outputId":"6b8a9de4-3070-47fc-cabd-900cc8f022b7","executionInfo":{"status":"ok","timestamp":1655288882095,"user_tz":420,"elapsed":74108,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Graphics\n","\n","# %matplotlib inline\n","animator.display_plt()\n","# Save to file that is replaced on every run.\n","animator.fig.savefig(loss_filenm, bbox_inches='tight')   # loss_filenm from filepath() call above.\n","# Automate improved filename description.\n","# animator.fig.savefig('loss.png', bbox_inches='tight')\n","# plt.savefig('loss.png', bbox_inches='tight')   # Less specific. Targets active figure.\n","\n","# Pipe labels (strings) are located in simdata_to_csv notebook\n","predictions = [f'{i}' for i in range(output_dim)]\n","# predictions = decode_labels(regdict_filenm)   # May no longer need to maintain regdict_filenm input path or decode_labels()\n","#                                 # Ordered pipe names is lexiconic and not consistent w/ conf mat (regions in number order)\n","# print(predictions)\n","# labels = range(output_dim)\n","# labels = decode_labels(regdict_filenm)\n","# print(ts_dataset[:][2])\n","# print(len(ts_dataset[:][2]))\n","# temp = [x.item() for x in ts_dataset[:][2]]\n","# print(temp)\n","# print(len(temp))\n","# labels = sorted(set(temp))\n","labels = [i for i in range(len(pToPIdx_dic))]\n","# print(labels)\n","# print(len(labels))\n","fig, ax = plt.subplots(1, 1, figsize=(100,100))\n","# fig.set_facecolor('#7d7f7c')\n","im = ax.imshow(confusion_matrix)\n","ax.set_xticks(np.arange(len(predictions)))\n","ax.set_yticks(np.arange(len(labels)))\n","# ax.set_xticklabels(predictions)\n","# ax.set_yticklabels(labels)\n","\n","# Set-up for white grid lines on minor ticks. Creates spacing effect.\n","ax.set_xticks(np.arange(len(predictions)+1) - 0.5, minor=True)\n","ax.set_yticks(np.arange(len(labels)+1) - 0.5, minor=True)\n","# Print white grid to space out the squares.\n","ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n","# Remove spines for clarity.\n","for k, v in ax.spines.items() :\n","  v.set_visible(False)\n","# ax.spines['top'].set_visible(False)   # Can't slice a dictionary.\n","ax.tick_params(which=\"minor\", bottom=False, left=False)\n","\n","# Horizontal labeling displays on top\n","ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n","# Rotate tick labels and set alignment.\n","plt.setp(ax.get_xticklabels(), rotation=-45, ha='right', rotation_mode='anchor')\n","plt.xlabel(f'Predictions -- {ts_size}')\n","# Move the x labels to the top\n","ax.xaxis.set_label_position('top')\n","plt.ylabel('Labels')\n","# Annotate matrix with values by looping over data dimensions\n","for i in range(len(labels)) :\n","  for j in range(len(predictions)) :\n","    text = ax.text(j, i, confusion_matrix[i, j].item(),\n","                   ha='center', va='center', color='white')\n","\n","ax.set_title(f'Confusion Matrix -- Epoch {t+1}')\n","fig.tight_layout()\n","# Save to file that is replaced on every run.\n","fig.savefig(conf_filenm)   # conf_filenm from filepath() call above.\n","torch.save(confusion_matrix, conf_mat_filenm)   # conf_mat_filenm from filepath() call above.\n","# plt.show()\n","# plt.close()"]},{"cell_type":"markdown","metadata":{"id":"4W6b5qaI8ZVj"},"source":["####Sanity Check: Pass a sample to the model"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gp6zYEZN8mx5","executionInfo":{"status":"ok","timestamp":1655288882096,"user_tz":420,"elapsed":22,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"4c0c4f37-5832-4cd7-c4c9-c5d1c14bc392"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Evaluation\n","Scenario 28 (pred=33, label=24)\n"]}],"source":["def predict_ch3(net, sample, samp_idx=0):\n","    \"\"\"Predict labels (redefined from d2l Chapter 3.6.7).\"\"\"\n","    print('Model Evaluation')\n","    X, y, pipId = sample[samp_idx]\n","    X = X.reshape([1,-1])\n","    preds = net(X)[0].argmax(axis=1)   # net returns a tuple of preds and prob_params\n","    print(f'Scenario {samp_idx} (pred={preds.item()}, label={y.item()})')\n","\n","predict_ch3(model, tr_dataset, samp_idx=28)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"rv4iOGftxLZi","executionInfo":{"status":"ok","timestamp":1655288882096,"user_tz":420,"elapsed":20,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# Test trained model on time stamps it hasn't seen before\n","# tmstp = 168\n","# X, y = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","# ts_dataset = TensorDataset(X, y)\n","\n","# test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n","# test_acc = test_loop(test_dataloader, model, loss_fn)"]},{"cell_type":"markdown","metadata":{"id":"k5xwqcQyNoBV"},"source":["####Scratch Work"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeYlNlR6WH-P","executionInfo":{"status":"ok","timestamp":1655288882097,"user_tz":420,"elapsed":20,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"ad2f5161-e748-4ac0-802e-1ec137a1acb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3.])\n","tensor([[1., 1.],\n","        [1., 1.]])\n","tensor([[1., 1.],\n","        [1., 1.]])\n","is None!\n"]}],"source":["# catting to an empty tensor -- doesn't work\n","ls = [1, 2, 3]\n","tsr = torch.Tensor(ls)\n","print(tsr)\n","tsr1 = torch.ones([2,2])\n","print(tsr1)\n","tsr = torch.concat([tsr1])\n","print(tsr)\n","test = None\n","if test is None :\n","  print('is None!')"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCehwyn6imcp","executionInfo":{"status":"ok","timestamp":1655288882098,"user_tz":420,"elapsed":21,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"fbb7a935-b750-4377-eecf-c521fd3b7ce4"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","tensor([0.1569, 0.9626, 0.8743, 0.2251, 0.3481])\n","tensor([0.0000, 0.9626, 0.8743, 0.0000, 0.0000])\n"]}],"source":["print(0.991 > 0.99)\n","x = torch.rand(5)\n","print(x)\n","y = torch.where(x > 0.6, x, torch.tensor(0.))\n","print(y)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGxSZV4cDLu7","executionInfo":{"status":"ok","timestamp":1655288882098,"user_tz":420,"elapsed":20,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"9bdea8ee-bf45-47f0-baf2-918810869622"},"outputs":[{"output_type":"stream","name":"stdout","text":["nan\n","False\n","nan\n","tensor(-inf)\n"]}],"source":["import math\n","x = float('nan')\n","print(x)\n","print(not math.isnan(x))\n","print(1 + 0.0 * x)\n","print(torch.log(torch.tensor(0.0)))"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-f8VELnBsjNg","executionInfo":{"status":"ok","timestamp":1655288882098,"user_tz":420,"elapsed":20,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"96b57fe9-d614-4a20-f867-4f62f20866c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1]\n"]}],"source":["x_tup = ([2], [3], [4])\n","# x_tup = (2, 3, 4)\n","x_tup[1][0] -= x_tup[0][0]\n","# x_tup[1] -= x_tup[0]\n","# print(x_tup[1] - x_tup[0])\n","print(x_tup[1])"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKcdDEqt7LVH","executionInfo":{"status":"ok","timestamp":1655288882098,"user_tz":420,"elapsed":20,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"a61f6b08-5d15-4bba-f402-901a4cd207b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.4581, 0.4829, 0.3125, 0.6150, 0.2139, 0.4118, 0.6938, 0.9693, 0.6178,\n","        0.3304])\n","tensor([0.1490, 0.4866, 0.9857, 0.1684, 0.5839, 0.6936, 0.5831, 0.1284, 0.5721,\n","        0.9248])\n","tensor([0.3125, 0.2139, 0.4118, 0.6938, 0.6178, 0.3304])\n","tensor([0.4581, 0.4829, 0.0000, 0.6150, 0.0000, 0.0000, 0.0000, 0.9693, 0.0000,\n","        0.0000])\n","tensor([0., 0., 0., 0., 0., 0.])\n","tensor(0.4581)\n","tensor([0.4581])\n","tensor([2.7486e-33])\n","tensor([0.3803])\n"]}],"source":["t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n","print(t)\n","t1 = torch.rand(10, generator=torch.Generator().manual_seed(11))\n","print(t1)\n","print(t[t1 > 0.5])   # returns a tensor containing only those elems for which test returns true.\n","t[t1 > 0.5] = 0   # Assigns zero to only those elems for which the test returns true.\n","print(t)\n","print(t[t1 > 0.5])\n","print(t[0])   # Returns a zero dim tensor. (num)\n","print(t[0:1])   # Returns a 1-dim tensor. ([num])\n","print(t[0:1].new(1))\n","print(t[0:1].new_empty(1).uniform_())"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9aks4N-SHt5","executionInfo":{"status":"ok","timestamp":1655288882098,"user_tz":420,"elapsed":19,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"aa29676c-cd89-4f92-b9f1-b288d35634f3"},"outputs":[{"output_type":"stream","name":"stdout","text":[" t: tensor([0.4581, 0.4829, 0.3125, 0.6150, 0.2139, 0.4118, 0.6938, 0.9693, 0.6178,\n","        0.3304])\n","tensor([2, 0, 1, 4, 3])\n"," t: tensor([0.4581, 0.4829, 0.3125, 0.6150, 0.2139, 0.4118, 0.6938, 0.9693, 0.6178,\n","        0.3304])\n","tn: tensor([0.3125, 0.4581, 0.4829, 0.2139, 0.6150])\n"," t: tensor(0.3125)\n","tn: tensor(0.3125)\n"]}],"source":["t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n","print(' t:', t)\n","idxs = torch.randperm(5, generator=torch.Generator().manual_seed(10))\n","print(idxs)\n","tn = t[idxs]\n","print(' t:', t)\n","print('tn:', tn)\n","print(' t:', t[2])\n","print('tn:', tn[0])"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpjdHiXWtwOl","executionInfo":{"status":"ok","timestamp":1655288882098,"user_tz":420,"elapsed":19,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}},"outputId":"caca6944-bdf6-42d2-b73c-b24bf98c111d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n","tensor([ 45, 145])\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n","tensor([9, 9])\n","[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]\n","[[1, 1]]\n","[[1, 1], [3, 3], [4, 4]]\n"]}],"source":["y_hat = torch.arange(20).reshape([2, -1])\n","print(y_hat)\n","len(y_hat)\n","print(y_hat.sum(1))\n","print(y_hat.argmax(dim=0))\n","print(y_hat.argmax(dim=1))\n","data = [[1.0, 1.0], [1.0, 1.0]] * 2   # multiplies the number of elements (like if you had 2 apples and then multiplied them by 2; you now have four apples)\n","print(data)\n","# print(data / data)   # dividing a list is not defined\n","(1,2) + (3,)   # cats the three elems\n","len((1,2))   # tuples have __len__ defined\n","[[] for _ in range(3)]\n","rows = [[1,1]]\n","print(rows)\n","[rows.append(i) for i in [[3,3],[4,4]]]\n","print(rows)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"MPQBiMXCOahN","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":19,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# X_masked = None\n","# masked_feats = torch.rand(15)\n","# print(masked_feats)\n","# mask = torch.randint(2, [15])\n","# print(mask)\n","# # Mask and masked features\n","# # May want sensing_mask_rand() to process batches of samples\n","# for i in range(5):\n","#   temp = torch.cat((masked_feats, mask)).reshape([1,-1])\n","#   print(temp)\n","\n","#   if X_masked is None:\n","#     X_masked = temp   \n","#     print(X_masked)\n","#   else:  \n","#     X_masked = torch.cat((X_masked, temp))\n","#     print(X_masked)\n","\n","# for i in X_masked:\n","#   print(i)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"i4kiAKeKfBIp","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":19,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# size = [100,]\n","# K = 20\n","# tn = torch.zeros(size)\n","# mask = torch.zeros(tn.size())\n","# print(mask.size())\n","# print(mask)\n","# indices = torch.randint(len(tn), size=(K,))\n","# print(indices)\n","# for idx in indices:\n","#   mask[idx] = 1\n","# print(mask)\n","\n","# mask = torch.cuda.FloatTensor(3, 3).uniform_()\n","# # tensor of floats\n","# mask = torch.FloatTensor(3,3).uniform_()\n","# print(mask)\n","# # tensor of booleans (?? how ??)\n","# mask = torch.FloatTensor(3,3).uniform_() > 0.8\n","# print(mask)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"NxvQSxfCAX_R","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":19,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# converting string labs to labels ranging from 0 -> num_of_classes (i.e. possible leak locations)\n","#  what if not all of the possible leak locations are used?\n","#  1) I can set the output dim to len of label_subset (easiser)\n","#  2) I can force the set to be all the possible fixed pipe locations (coordinating this will be tricky)\n","# labs = [1,2,2,3,1,4,4,3]\n","# lab_dict = {}\n","# encoded_labs = []\n","# label_subset = set(labs)\n","# print(label_subset)\n","# print(type(label_subset))\n","# print(len(label_subset))\n","# for i, key in enumerate(label_subset):\n","#   lab_dict[key] = i\n","# print(lab_dict)\n","# for key in labs:\n","#   encoded_labs.append(lab_dict[key])\n","# print(encoded_labs)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"snpJEOWKoc5Y","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":18,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# Reshaping practice\n","# base_file = 'simdata/_base_/node_demand.csv'\n","# data_file = base_file\n","# data = pd.read_csv(data_file)\n","# data_tn = torch.tensor(data.values, dtype=torch.float32)\n","# data_tn[:,1:].reshape([1,-1])\n","\n","# data_tn = torch.arange(20).reshape([4,5])\n","# print(data_tn)\n","# data_tn = data_tn[:,1:].reshape([1,-1])\n","# print(data_tn)\n","# data_tn1 = torch.arange(20).reshape([4,5])\n","# print(data_tn1)\n","# data_tn1 = data_tn1[:,1:].reshape([1, -1])\n","# print(data_tn1)\n","# torch.cat((data_tn1, data_tn))"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"2DGW06k6V-qI","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":18,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# Extracting an intelligible answer from the model\n","# x = torch.arange(16, dtype=torch.float32).reshape((4,4))\n","# print(x)\n","# print(x.sum(axis=0))\n","# print(x.sum(axis=[0,1]))\n","# mean = x.sum() / x.numel()\n","# print(mean)\n","# # notice we keep all dims (tensor of a tensor ie. two brackets) vs above we lost one (just a tensor)\n","# print(x.sum(dim=0, keepdim=True))\n","\n","# y = torch.tensor([3,3,3,3])\n","# # x.argmax(1).type(y.dtype) == y\n","# correct = 0\n","# correct += (x.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","# correct"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"kndCfp0DOku8","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":17,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# Handy timer class\n","class Timer:\n","    \"\"\"Record multiple running times.\"\"\"\n","    def __init__(self):\n","        self.times = []\n","        self.start()\n","\n","    def start(self):\n","        \"\"\"Start the timer.\"\"\"\n","        self.tik = time.time()\n","\n","    def stop(self):\n","        \"\"\"Stop the timer and record the time in a list.\"\"\"\n","        self.times.append(time.time() - self.tik)\n","        return self.times[-1]\n","\n","    def avg(self):\n","        \"\"\"Return the average time.\"\"\"\n","        return sum(self.times) / len(self.times)\n","\n","    def sum(self):\n","        \"\"\"Return the sum of time.\"\"\"\n","        return sum(self.times)\n","\n","    def cumsum(self):\n","        \"\"\"Return the accumulated time.\"\"\"\n","        return np.array(self.times).cumsum().tolist()"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"m3m7r6ps_rDM","executionInfo":{"status":"ok","timestamp":1655288882099,"user_tz":420,"elapsed":17,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"}}},"outputs":[],"source":["# Target transform\n","from torchvision.transforms import Lambda\n","\n","train_size = 700\n","# target_transform = Lambda(lambda y: torch.zeros(\n","#     (train_size, output_dim), dtype=torch.float).scatter_(\n","#         dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))\n","\n","# one-hot classification label vector\n","target_transform = Lambda(lambda y: torch.scatter_(\n","        dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["4Epm5SJkrzLx","VQzqACaz6D71","BTY1vNInM3RI","ydc9rlm86Hmt","3ocO0Om3ofeX","RVZsPo1p2RoP","TmFOoA-k258k","5wCj-uKc561c","7KkFT5x93F5H","if63qNQuS0HA","j_Zwmb6sYpbm","qysp3xV5W2dg","uBUCPLjMROO0","cQdyRZRYXiQJ","4W6b5qaI8ZVj","k5xwqcQyNoBV"],"name":"bernoulli7_39regions_mult_tmstps_v1.ipynb","provenance":[{"file_id":"1YeSJUnijlqIt0y5eDayQvwNGRwSKSkYZ","timestamp":1649218750773},{"file_id":"13Yy2CsIHuUcYXGOkYgQOdktXEL2CHOFJ","timestamp":1639297478820},{"file_id":"1pSJ226BsrXMVoAQJ0BKrxwgrHyxGHvI5","timestamp":1636667778711},{"file_id":"1VHyXaGHoAm4NI3ahzbXhGxODiO4mg4AM","timestamp":1632939473633},{"file_id":"19ehUGgFEEdAcgFs-fy_SW0vkxJggJ5XW","timestamp":1631899911015},{"file_id":"1_vt4FQCGh7KSHkU7GKarP807bTNxTZV7","timestamp":1631190383013},{"file_id":"1Nf2Ay7YXjx6JBvs9gsySlwirTOO-Ownd","timestamp":1628629596966},{"file_id":"1pp1nL2XUKNxN0QscNo-VzAeWMS07sl53","timestamp":1627669468832},{"file_id":"1JCW99kbx6_NEJS5KKhCRGP9tz5YE3eCZ","timestamp":1627178038295},{"file_id":"1Hhtzwkvq30pxSybpIcmD4Wb4eRB0bVmO","timestamp":1626718176246},{"file_id":"1rFofuDkzfAOVxLTpBsYt28pthUMCMtOL","timestamp":1626651085984},{"file_id":"19eoMaxuZB18-ZWdJZdgDr1MD6qAJR0L3","timestamp":1626329222475}],"mount_file_id":"1kFwXWhxiRGIE43J0_XDZD8xQXvduRE5I","authorship_tag":"ABX9TyPRzBHh6cRzXGhlJZNsc7po"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}