{"cells":[{"cell_type":"markdown","metadata":{"id":"j1q0Q-CJEFf5"},"source":["####Notes and Imports"]},{"cell_type":"markdown","metadata":{"id":"4Epm5SJkrzLx"},"source":["#####Notes\n","- Using SimData_to_csv notebook to create dataset csv\n","- I may want a labels map (dict) for pipe labels like the one here\n","  - https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterating-and-visualizing-the-dataset\n","- Unclear what a one-hot encoded tensor might be used for\n","  - used here as a target label transformation\n","    - https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#lambda-transforms\n","\n","- How do I handle two labels?\n","- Is normalization required?\n"," - ans: desirable when features have different ranges. https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=The%20goal%20of%20normalization%20is,when%20features%20have%20different%20ranges.\n","- Complete a panda tutorial\n","- He: Classifer tutorial; CIFAR10 dataset (3 channel images)\n"," - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","- Colab Pro\n"," - One important caveat to remember while using Colab is that the files you upload to it wonâ€™t be available forever. Colab is a temporary environment with an idle timeout of 90 minutes and an absolute timeout of 12 hours. This means that the runtime will disconnect if it has remained idle for 90 minutes, or if it has been in use for 12 hours [longer for Colab Pro]. On disconnection, you lose all your variables, states, installed packages, and files and will be connected to an entirely new and clean environment on reconnecting. source:https://neptune.ai/blog/google-colab-dealing-with-files"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655291171661,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"CZl-N7iuq70P"},"outputs":[],"source":["# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)\n","\n","# from psutil import virtual_memory\n","# ram_gb = virtual_memory().total / 1e9\n","# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","# if ram_gb < 20:\n","#   print('Not using a high-RAM runtime')\n","# else:\n","#   print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"VQzqACaz6D71"},"source":["####Model framework"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":915,"status":"ok","timestamp":1655291172571,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"YPV_XlfZFlI-","outputId":"0321514f-aa7e-4719-847d-847563f36186"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["%matplotlib inline\n","\n","import os\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","\n","import pandas as pd   # For loading csv file dataset\n","import random\n","\n","import math   # Used for nan checking (math.isnan())\n","import numpy as np   # Used w/ pipe_to_pipeIdx file and graphics.\n","import pickle   # Used for loading reg_dict (stored w/ training data csv's; constructed in graph_partition notebook)\n","\n","# Straight-Through Estimator\n","# https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html\n","\n","# Autograd\n","# https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n","\n","# Define model\n","#  -- when called, model returns a output_dim dimensional tensor\n","# def sparseProbMap(batch_probMap, sparsity) :\n","def sparseProbMap(probMap, sparsity) :\n","  \"\"\"Rescale probability map (batch_probMap) to obtain desired sparsity in\n","  measuremets that are turned on.\n","  sparsity = budget (int) / training sample length (meas)\n","    budget -- number of measurement to turn on/ sensor to deploy\n","  \"\"\"\n","  mean = torch.mean(probMap, dim=0, keepdim=True)\n","  scalar = sparsity / mean\n","  beta_scalar = (1 - sparsity) / (1 - mean)\n","  toggle = torch.le(scalar, 1).float()\n","  sparse_probMap = ( toggle * scalar * probMap\n","                      + (1 - toggle) * (1 - (1 - probMap) * beta_scalar) )\n","  # print(f'sparseProbMap(): samp_means {samp_means}')\n","  # print(f'sparseProbMap(): samp_means size {samp_means.size()}')\n","  # print(f'sparseProbMap(): samp_scalars {samp_scalars}')\n","  # print(f'sparseProbMap(): samp_beta_scalars {samp_beta_scalars}')\n","  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n","  return sparse_probMap\n","  \n","  # Alt approaches\n","  # torch method -- with prob_map repeat before this func call (i.e. batch_probMap)\n","  # Notice batch_probMap contains batch_size copies of one prob_map\n","  #  Might be able to make this even faster by running the calc once, then repeating after this call.\n","  # Tensor version -- might speed up training time.\n","  # samp_means = torch.mean(batch_probMap, dim=1, keepdim=True)\n","  # samp_scalars = sparsity / samp_means\n","  # samp_beta_scalars = (1 - sparsity) / (1 - samp_means)\n","  # toggles = torch.le(samp_scalars, 1).float()\n","  # sparse_probMap = ( toggles * samp_scalars * batch_probMap\n","  #                     + (1 - toggles) * (1 - (1 - batch_probMap) * samp_beta_scalars) )\n","\n","  # for samp_probMap in batch_probMap :\n","  #   mean_sampProbMap = torch.mean(samp_probMap)\n","  #   scalar_sampProbMap = sparsity / mean_sampProbMap\n","  #   beta_scalar = (1 - sparsity) / (1 - mean_sampProbMap) # Rename variable to something more descriptive\n","  #   # print(f'sparseProbMap(): samp_probMap {samp_probMap}')\n","  #   # print(f'sparseProbMap(): samp_probMap size {samp_probMap.size()}')\n","  #   # print(f'sparseProbMap(): mean_sampProbMap {mean_sampProbMap}')\n","  #   # assert False\n","  #   toggle = torch.le(scalar_sampProbMap, 1).float()\n","  #   scaled_sampMap = ( toggle * scalar_sampProbMap * samp_probMap\n","  #                     + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar) ).reshape([1, -1])\n","  #   # print(f'sparseProbMap(): scaled_sampMap type {type(scaled_sampMap)}')\n","  #   if scaled_sampMaps == None :\n","  #     scaled_sampMaps = torch.cat((scaled_sampMap,), dim=0).to(device)\n","  #   else :\n","  #     scaled_sampMaps = torch.cat((scaled_sampMaps, scaled_sampMap,), dim=0).to(device)\n","  #   # scaled_sampMaps.append(toggle * scalar_sampProbMap * samp_probMap + (1 - toggle) * (1 - (1-samp_probMap) * beta_scalar))\n","  # # print(f'sparseProbMap(): scaled_sampMaps size {scaled_sampMaps.size()}')\n","\n","  # NOTE: Dramatic slowdown after incorporating this function.\n","  #  May be due to use of for loop.\n","  # Use with scaled_sampMaps list version.\n","  # One big tensor. I'd prefer rows. Try adding brackets to append statement.\n","  # print(f'sparseProbMap(): sparse_probMap size {sparse_probMap.size()}')\n","  # sparse_probMap = torch.cat((scaled_sampMaps,), dim=0)\n","  # sparse_probMap  = sparse_probMap.reshape([batch_size, -1])\n","  # assert False\n","  # return scaled_sampMaps   # Equivalent to sparse_probMap for a batch.\n","\n","class STEFunction(torch.autograd.Function) :\n","    \"\"\"\n","    We can implement our own custom autograd Functions by subclassing\n","    torch.autograd.Function and implementing the forward and backward passes\n","    which operate on Tensors.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, probability_mask) :\n","        \"\"\"\n","        In the forward pass we receive a Tensor containing the input (prob_mask) and return\n","        a Tensor containing the output (binary_mask).\n","        ctx is a context object that can be used\n","        to stash information for backward computation. You can cache arbitrary\n","        objects for use in the backward pass using the ctx.save_for_backward method.\n","        \"\"\"\n","        # Push probability map through a bernoulli sampling to create 0/1 mask.\n","        # Return mask.\n","\n","        prob_mask_size = probability_mask.size()\n","        # print(f'STEFunc() forward(): prob_mask_size {prob_mask_size}')\n","        # print(f'STEFunc() forward(): batch_size {batch_size}')\n","\n","        # Bernoulli sampling.\n","        # Sample from a uniform distribution.\n","        # uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n","        uni_samples = probability_mask.new_empty(prob_mask_size).uniform_()\n","        # Bernoulli sampled binary mask.\n","        binary_mask = (probability_mask > uni_samples).float()\n","        # Note the different sizes of probability_mask and uni_sample. The\n","        #  following operation returns a tensor shaped like uni_sample.\n","        # Note: torch.bernoulli() is not usable because func return only the prob mask,\n","        #  but the uniform distributions samples are needed for gradient calcs.\n","        # bin_mask = torch.bernoulli(probablility_mask)\n","        # print(f'STEFunc() forward(): bin_mask_size {binary_mask.size()}')\n","        # print(f'STEFunc() forward(): probability_mask {probability_mask},\\n\\tuni_samples {uni_samples}')\n","        # assert False\n","\n","        ctx.save_for_backward(probability_mask, uni_samples)\n","        return binary_mask\n","    \n","    @staticmethod\n","    def backward(ctx, grad_output) :\n","      # return F.hardtanh(grad_output)\n","      # SigDeriv graph: https://www.desmos.com/calculator/icbxupp3dh\n","      alpha = 1\n","      prob_mask, uni_samples = ctx.saved_tensors\n","\n","      # Sigmoid function derivative\n","      grad_est = (alpha * torch.exp(-alpha * (prob_mask - uni_samples))\n","          / (1 + torch.exp(-alpha * (prob_mask - uni_samples))) ** 2)\n","          # / torch.exp(1 + -alpha * (prob_mask - uni_samples)) ** 2)\n","      return grad_est * grad_output\n","\n","class StraightThroughEstimator(nn.Module) :\n","  def __init__(self) :\n","    # Consider moving probability map parameters here.\n","    #  If so, change STEFunction call.\n","    super(StraightThroughEstimator, self).__init__()\n","  \n","  def forward(self, probability_mask) :\n","    binary_mask = STEFunction.apply(probability_mask)\n","  # def forward(self, probability_mask, batch_size) :\n","    # binary_mask = STEFunction.apply(probability_mask, batch_size)\n","    return binary_mask\n","\n","class Encoder(nn.Module):\n","    # Consider removing the default parameter values.\n","    def __init__(self, input_dim=0, output_dim=0):\n","        super(Encoder, self).__init__()\n","        self.flatten = nn.Flatten()\n","        # Note: nn.Sequential may take only a single argument. A tuple may work,\n","        #  but unclear if this is stable.\n","        self.binary_STE_stack = nn.Sequential(\n","            # Define the input dimensions\n","            # STE is placed at the bottleneck of the autoencoder.\n","            StraightThroughEstimator(),\n","        )\n","        # self.ste = StraightThroughEstimator()\n","        # self.steFunc = STEFunction().apply\n","        self.sparProbMapFunc = sparseProbMap\n","        unif_samp_tn = torch.zeros([input_dim]).uniform_()\n","        self.mask_params = nn.Parameter(unif_samp_tn)\n","        # fill_val_tn = torch.zeros([input_dim]).fill_(0.5)\n","        # self.mask_params = nn.Parameter(fill_val_tn)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        prob_mask = torch.sigmoid(self.mask_params)   # Ensures probabilities lie btwn (0, 1)\n","        batch_size = x.size()[0]\n","        # Used to create bin_mask for ea training sample.\n","        #  To switch to one bin_mask per batch, comment out the following line.\n","        # prob_mask = prob_mask.repeat(batch_size, 1)\n","        sparse_probMask = self.sparProbMapFunc(prob_mask, sparsity=0.012)\n","        sparse_probMask = sparse_probMask.repeat(batch_size, 1)\n","        # sparse_probMask = prob_mask.repeat(batch_size, 1)\n","        # Using a tuple input is problematic because the model expects a grad for all inputs to nn.Sequential.\n","        binary_mask = self.binary_STE_stack(sparse_probMask)\n","        # binary_mask = self.binary_STE_stack(prob_mask)\n","        # binary_mask = self.ste(prob_mask)\n","        # binary_mask = self.steFunc(prob_mask)\n","        # print(f'Encoder.forward(): prob_mask size {prob_mask.size()}')\n","        # print(f'Encoder.forward(): prob_mask size {prob_mask}')\n","        # assert False\n","        # print(x.size())\n","        # print(f'Encoder.forward(): mask_params {self.mask_params}')\n","        # print('Encoder.forward(): batch_size', batch_size)\n","        # print('Encoder.forward(): binary_mask', binary_mask)\n","        return x * binary_mask, binary_mask, self.mask_params\n","\n","    # May or may not need to define the backward behavior of this class.\n","    # def backward(self, x):\n","    #     pass\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, input_dim, output_dim):   # output_dim is not yet used.\n","        super(Decoder, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_lrelu_stack = nn.Sequential(\n","            # Define the input dimensions\n","            nn.Linear(input_dim, 512),\n","            # nn.Linear(input_dim, output_dim),\n","            nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # nn.Linear(512, 512),\n","            # nn.LeakyReLU(),\n","            # # Define the output dimensions\n","            nn.Linear(512, output_dim),\n","            nn.LeakyReLU(),\n","        )\n","        # Yisong: initialize the weights in the first layer, and the following layers will follow suit.\n","        # self.linear_lrelu_stack[0].weight.data /= 100.\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        # print(x.size())\n","        logits = self.linear_lrelu_stack(x)\n","        return logits\n","\n","class Autoencoder(nn.Module) :\n","    def __init__(self, input_dim, output_dim) :\n","        super(Autoencoder, self).__init__()\n","        # self.auto_stack = nn.Sequential(\n","        #     Encoder(input_dim, output_dim),\n","        #     Decoder(),\n","        # )\n","        self.encoder = Encoder(input_dim=input_dim)\n","        self.decoder = Decoder(input_dim, output_dim)\n","\n","    def forward(self, x, binMaskSizeLs=[], encode=False, decode=False) :\n","        if encode :\n","            pass\n","        elif decode :\n","            pass\n","        else :\n","            # x = self.auto_stack(x)\n","            under_samp_meas, binary_mask, mask_params = self.encoder(x)\n","            # Working out what to do with the binary_mask -- i.e. if it will be fed to decoder or not.\n","            bin_mask_size = binary_mask.sum()\n","            batch_size = x.size()[0]\n","            # binMaskSizeLs.append(bin_mask_size)\n","            # print(f'AE forward(): avg binary_mask size {int(bin_mask_size / batch_size)}')\n","            # print(f'Auto.forward(): mask_params {mask_params}')\n","            x = self.decoder(under_samp_meas)\n","        return x, mask_params\n","\n","# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using {} device\".format(device))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655291172571,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"_Bs_956ARP3c"},"outputs":[],"source":["# input_dim = 10\n","# init_tensor = torch.zeros([input_dim]).uniform_()\n","# print(init_tensor)\n","# p_mask = nn.Parameter(init_tensor)\n","# print(p_mask)\n","# ste = STEFunction()\n","# print(ste)\n","# masks = ste.forward(init_tensor, p_mask)\n","# a = torch.randn(1, 2, 3, 4)\n","# b = torch.randn(2, 2)\n","# print(a)\n","# print(a.size())\n","# print(a.view(1, 1, 2, 3, 2, 2))\n","# print(b)\n","# print(b * a.view(1, 1, 2, 3, 2, 2))\n","\n","# Create a mask for each sample by first creating a uniform value tn of size\n","#  [batch_size, prob_mask_size], then \n","# c = torch.zeros([10]).uniform_()\n","# print(c)\n","# tn = c.new_empty([3, 10]).uniform_()\n","# print(tn)\n","# bin_mask = (c > tn).float()\n","# print(bin_mask)\n","# End \"Create a mask ...\"\n","\n","# print(c)\n","# mask = STEFunction.forward(c, c)   # Comment out ctx line.\n","# print(mask)   # OK\n","# module = StraightThroughEstimator()\n","# mask = module(c)\n","# print(mask)\n","# module1 = Encoder(input_dim=10)\n","# meas = torch.randint(100, (10,))\n","# print(meas)\n","# mask = module1(meas)\n","# print(mask)   # OK"]},{"cell_type":"markdown","metadata":{"id":"BTY1vNInM3RI"},"source":["####Training and Testing"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1227,"status":"ok","timestamp":1655291173796,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"Fi2L7VdNGqNb"},"outputs":[],"source":["def train_loop(dataloader, model, entrop_decay, loss_fn, optimizer, pToPIdxDict=None, epoch=0, mod=100):\n","    \"\"\"\n","    pipIdx (for loop) is a tensor containing the pipe indices associated with a training leak scenario.\n","    pToPIdxDict is a dict that maps pipe names (str) to a tuple of (region, pipe_idx)\n","      used to determine number of rows.\n","    \"\"\"\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)   # For avg training loss\n","    # print(f'train_loop(): dataset size: {size}')\n","    # print(f'train_loop(): num_batches: {num_batches}')\n","    train_loss, train_accuracy = 0, 0\n","    confusion_matrix = torch.zeros(len(pToPIdxDict), 4, dtype=torch.int16)   # Think of way to make # of regions automatic.\n","                                                                             # Maybe add a 'lengths' key, value: (pipe_ct, reg_ct)\n","    prevLoss = 100.\n","    for batch, (X, y, pipIdx) in enumerate(dataloader):\n","        # print(batch, X)\n","        # print(y)\n","        # print(pipIdx)\n","        X = X.to(device)\n","        y = y.to(device)\n","        # Compute prediction and loss\n","        pred, prob_params = model(X)  # params for flexibility; update sanity check\n","        # print(y.size())\n","        # print(pred.size())\n","        # print(pred)\n","        # print(prob_params)\n","        \n","        if (True) :\n","        # if (epoch < 100) :\n","          # prob_mask * X -> classifier -> pred\n","          # # print(f\"mask_params {model.state_dict()['encoder.mask_params']}\")\n","          # prob_params = model.state_dict()['encoder.mask_params']  # could be slower than passing the params out\n","          prob_map = torch.sigmoid(prob_params)   # Ensures values (probabilities) lie btwn (0, 1)\n","          splus = nn.functional.softplus(prob_params, -1)   # -log(1 + e^(-x)); careful with the sign when using in expressions.\n","          # print(splus[0], prob_params[0])   # Sanity check\n","          # print(torch.max(splus))\n","          # assert torch.max(splus) < 0\n","          # print(epoch)\n","          # loss_KL = ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) - torch.log(torch.tensor(0.5)) ).sum()\n","          # p_m = torch.where(prob_map > 0, prob_map, torch.tensor(0.001).to(device))\n","          # p_m = torch.where(prob_map < 1, prob_map, torch.tensor(0.999).to(device))\n","          # # print(f'p_m max {torch.max(p_m)}, min {torch.min(p_m)}')\n","          # # print(f'p_m range {torch.max(p_m) + abs(torch.min(p_m))}')\n","          # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) - torch.log(torch.tensor(0.5)) ).sum()\n","          # # loss_KL = ( (1 - p_m) * torch.log(1 - p_m) + p_m * torch.log(p_m) ).sum()\n","          # loss_KL = lamb * ( (1 - prob_map) * torch.log(1 - prob_map) + prob_map * torch.log(prob_map) ).sum()\n","          # Natural log is the result of torch.log(...)\n","          loss_KL = ( (1 - prob_map) * ((-1)*prob_params + splus) + prob_map * splus - torch.log(torch.tensor(0.5)) ).sum()   # includes constant term\n","          # loss_KL = ( (1 - prob_map) * ((-1)*prob_params + splus) + prob_map * splus ).sum()\n","          # Dramatic change between including lamb and not may affect learning.\n","          lamb = torch.exp(torch.tensor(entrop_decay * epoch))\n","          # lamb = torch.exp(torch.tensor(-1/1*epoch))   # zeros around (very early) epochs; https://www.desmos.com/calculator/dem8eqibe6\n","          # lamb = torch.exp(torch.tensor(-1/25*epoch))   # zeros around 250 epochs; https://www.desmos.com/calculator/miayf0qrbe\n","          # lamb = torch.exp(torch.tensor(-1/100*epoch))   # zeros around 1000 epochs; https://www.desmos.com/calculator/dem8eqibe6\n","          # lamb = torch.exp(torch.tensor(-1/4000*epoch))   # zeros around 28k epochs; https://www.desmos.com/calculator/6fi4dfzuji\n","          # lamb = torch.exp(torch.tensor(-1/6000*epoch))   # zeros around 44k epochs; https://www.desmos.com/calculator/cwlhq5xe4n\n","          # lamb = torch.exp(torch.tensor(-1/10000*epoch))   # zeros around 70k epochs \n","          # lamb = torch.exp(torch.tensor(-1/14000*epoch))   # zeros around 100k epochs # Try 1 / sqrt(epoch)   # Was using 1 / e^(-alpha*x) => e^(alpha*x) i.e. not what I wanted.\n","          # lamb = 1 / epoch\n","          loss_KL = lamb * loss_KL   # Anneal diversity loss\n","        else :\n","          loss_KL = 0.0\n","        # # print(f'loss_KL {loss_KL}')\n","        # # assert loss_KL != float('nan')\n","        # assert not math.isnan(loss_KL)\n","\n","        # **** Notice pred is formed on the batch level, but loss_KL is a one shot ****\n","        loss = loss_fn(pred, y) + loss_KL  # returns single value; avg loss across batch\n","        # loss = loss_fn(pred, y)   # returns single value; avg loss across batch\n","        assert loss != float('nan')\n","        # loss = loss_fn(pred, y)  # returns single value; avg loss across batch\n","\n","        # print(loss * len(y))\n","        # for i in range(len(y)) :\n","        #   print(pred[i])\n","        #   print(y[i])\n","        #   ls = loss_fn(pred[i], y[i])\n","        #   print(f'loss {i} {ls}')\n","        # print(y)\n","        # print(pred.argmax(1).type(y.dtype))\n","\n","        # Confusion Matrix\n","        # (pipe label vs region) i.e. the resolution of the row is pipe name (not just region)\n","        # m(n, r)  where n is the cardinality of the leakpipe set, r is the number of regions\n","        # At the time of assignment,\n","        #  need a dict that maps pipe str to pipe idx\n","        #  the pipe label (str) (not the region label) for the training sample\n","        #  the model predicted region\n","        # preds = pred.argmax(1).type(y.dtype)\n","        # for i in range(len(y)) :\n","        #   confusion_matrix[pipIdx[i]][preds[i]] += 1\n","        # print(f'train_loop(): preds {preds}')\n","        # print(confusion_matrix)\n","        # print(f'train_loop(): conf_mat {confusion_matrix.size()}')\n","        # assert False\n","        # Older version of conf mat.\n","        # preds = pred.argmax(1).type(y.dtype)\n","        # for i in range(len(y)) :\n","        #   confusion_matrix[y[i]][preds[i]] += 1\n","        \n","        # Top-k predictions\n","        # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n","        # k = 1\n","        # top_k = torch.topk(input=pred, k=k, dim=1,)\n","        # print(top_k)\n","\n","        # Disaggregate performance -- save model\n","        #  goal: extract outliers (in another notebook)\n","        #  Make into a function.\n","        # if epoch > 2000 and loss.item() > (prevLoss * 15) :\n","        # if epoch == 10000 :\n","        #   print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n","        #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n","        # # print(prevLoss)\n","        # prevLoss = loss.item()\n","        # print(prevLoss > loss * 20)\n","        # print(epoch)\n","        # assert False\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # train_loss += loss_fn(pred, y).item() * len(y)   # I think this second call to loss_fn runs the grad twice. ???\n","        train_loss += loss.item() * len(y)\n","        # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n","        train_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","        # print(loss)\n","\n","        if batch % mod == 0:\n","            # print(batch)\n","            # print(y)\n","            # print(pred)\n","            loss, current = loss.item(), batch * len(X)\n","            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","    # Save last model\n","    # if epoch == 10000 :\n","    #     print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n","    #     torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n","\n","    train_loss /= size    # weighted avg training loss\n","    train_accuracy /= size\n","    print(f\"Training Error: \\n Accuracy: {(100*train_accuracy):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n","    return train_loss, train_accuracy, confusion_matrix\n","\n","\n","def test_loop(dataloader, model, loss_fn, pToPIdxDict=None, out_dim=10):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    # print(f'test dataset size: {size}')\n","    # print(f'test num_batches: {num_batches}')\n","    test_loss, test_accuracy = 0, 0\n","    confusion_matrix = torch.zeros(len(pToPIdxDict), out_dim, dtype=torch.int32)\n","\n","    with torch.no_grad():\n","        for X, y, pipIdx in dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","\n","            pred, _ = model(X)\n","            # print(pred)\n","            # print(y)\n","            test_loss += loss_fn(pred, y).item()\n","            # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n","            test_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","            # Confusion matrix\n","            preds = pred.argmax(1).type(y.dtype)\n","            for i in range(len(y)) :\n","              # confusion_matrix[y[i]][preds[i]] += 1\n","              confusion_matrix[pipIdx[i]][preds[i]] += 1\n","            # print(f'test_loop(): conf mat {confusion_matrix}')\n","\n","    test_loss /= num_batches\n","    test_accuracy /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","    # return test accuracy percentage for epoch\n","    return test_accuracy, confusion_matrix"]},{"cell_type":"markdown","metadata":{"id":"ydc9rlm86Hmt"},"source":["####Animator (d2l)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291173796,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"9KnHcBLdIJYH"},"outputs":[],"source":["def use_svg_display():\n","    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n","    display.set_matplotlib_formats('svg')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655291173797,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"XKViWEWBIK2E"},"outputs":[],"source":["def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n","    \"\"\"Set the axes for matplotlib.\"\"\"\n","    axes.set_xlabel(xlabel)\n","    axes.set_ylabel(ylabel)\n","    axes.set_xscale(xscale)\n","    axes.set_yscale(yscale)\n","    axes.set_xlim(xlim)\n","    axes.set_ylim(ylim)\n","    if legend:\n","        axes.legend(legend)\n","    axes.grid()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291173797,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"xM5q0mh96LMw"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","from IPython import display   # Try commenting out. Maybe I'll be able to save image from print out (rather than files)\n","\n","class Animator:\n","    \"\"\"For plotting data in animation.\"\"\"\n","    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n","                 ylim=None, xscale='linear', yscale='linear',\n","                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n","                 figsize=(3.5, 2.5)):\n","        # Incrementally plot multiple lines\n","        if legend is None:\n","            legend = []\n","      \n","        self.nrows = nrows\n","        self.ncols = ncols\n","        self.figsize = figsize\n","\n","        self.xlabel = xlabel\n","        self.ylabel = ylabel\n","        self.xlim = xlim\n","        self.ylim = ylim\n","        self.xscale = xscale\n","        self.yscale = yscale\n","        self.legend = legend\n","\n","        self.X, self.Y, self.fmts = None, None, fmts\n","\n","    def add(self, x, y):\n","        # Add multiple data points into the figure\n","        if not hasattr(y, \"__len__\"):\n","            y = [y]\n","        n = len(y)\n","        if not hasattr(x, \"__len__\"):\n","            x = [x] * n\n","        if not self.X:\n","            self.X = [[] for _ in range(n)]\n","        if not self.Y:\n","            self.Y = [[] for _ in range(n)]\n","        for i, (a, b) in enumerate(zip(x, y)):\n","            if a is not None and b is not None:\n","                self.X[i].append(a)\n","                self.Y[i].append(b)\n","\n","    \n","    def display_plt(self):\n","        # Borrowed use_svg_display() implementation from d2l\n","        use_svg_display()\n","        # matplot function\n","        self.fig, self.axes = plt.subplots(self.nrows, self.ncols, figsize=self.figsize)\n","        if self.nrows * self.ncols == 1:\n","            self.axes = [self.axes,]\n","        # Use a lambda function to capture arguments; set_axes in d2l API\n","        self.config_axes = lambda: set_axes(self.axes[0],\n","                                            self.xlabel,\n","                                            self.ylabel,\n","                                            self.xlim,\n","                                            self.ylim,\n","                                            self.xscale, self.yscale, self.legend)\n","        self.axes[0].cla()\n","        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n","            self.axes[0].plot(x, y, fmt)\n","        self.config_axes()\n","        display.display(self.fig)\n","        display.clear_output(wait=True)\n","\n","        # return self.fig"]},{"cell_type":"markdown","metadata":{"id":"3ocO0Om3ofeX"},"source":["####Confusion Matrix"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291173797,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"AxqpHHcoomED"},"outputs":[],"source":["# %run /content/drive/MyDrive/'Colab Notebooks'/'Water Distribution Network'/'Input Pipeline'/'Data To File'/SimData_to_csv(hdf)_v1.ipynb\n","\n","class Conf_Mat() :\n","  def __init__(self, classes=10) :\n","    \"\"\"classes (int): the number of classes in the classifier.\"\"\"\n","    self.confusion_matrix = torch.zeros(classes, classes, dtype=torch.int32)\n","    # Load labels via JSON or csv file.\n","    self.labels_str = ['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024']\n","  \n","  def addValues(self, pred, y) :\n","    \"\"\"Add values to the confusion matrix.\n","    pred (tensor): tensor containing model predictions.\n","    y (tensor): tensor containing ground truth labels.\n","    return None\n","    \"\"\"\n","    # Confusion Matrix\n","    for i in range(len(y)) :\n","      preds = pred.argmax(1).type(y.dtype)\n","      confusion_matrix[y[i]][preds[i]] += 1\n","    print(confusion_matrix)\n","  \n","  def displayConfMat(self) :\n","    pass\n","\n","def decode_labels(regdict_filenm) :\n","  # Might be easier to place the encoder in a file and read it here.\n","  # WARNING: lab_subset order is not consistent.\n","  # NOTE: sets are not subscriptable\n","  # return sorted(['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024'])\n","  # regdict_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/leak_pipes_all/39regions/00/'\n","  # file_nm = f'region_dict_{partitions}.pickle'\n","  load_loc = regdict_filenm\n","\n","  # For loading\n","  with open(load_loc, 'rb') as handle:\n","      reg_dict = pickle.load(handle)\n","  \n","  prediction_labels = []\n","  for reg_nm in reg_dict['reg_partits'] :   # reg_partits is a dict; key: reg_nm (str), value: ls of pipe in region\n","    prediction_labels.append(reg_nm)\n","  return sorted(prediction_labels)   # sorted in lexiconic order. Not cosistent w/ conf_mat pred col order.\n","# print(decode_labels())   # For testing.\n","\n","def leak_pipe_strs() :\n","  # Would like to develop an automatic method to transfer pipe names of labels (rather copy paste).\n","  #  In this noteboook, labels have been encoded (mapped) to a unique int in \n","  # Copy-paste list of pipe names for labels here from simdata_to_csv notebook.\n","  return sorted({'P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P1022',\n","                 'P1023', 'P1024', 'P1025', 'P1026', 'P1027', 'P1028', 'P1029', \n","                 'P103', 'P1030', 'P1031', 'P1032', 'P1033', 'P1034', 'P1035', \n","                 'P1036', 'P1039', 'P104', 'P1040', 'P1041', 'P1042', 'P1044', \n","                 'P1045', 'P106', 'P107', 'P108', 'P109', 'P11', 'P110', 'P111', \n","                 'P112', 'P113', 'P115', 'P116', 'P117', 'P118', 'P119', 'P12', \n","                 'P120', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', \n","                 'P128', 'P129', 'P13', 'P130', 'P131', 'P132', 'P134', 'P136', \n","                 'P138', 'P139', 'P14', 'P140', 'P141', 'P142', 'P144', 'P147', \n","                 'P148', 'P15', 'P150', 'P154', 'P155', 'P156', 'P157', 'P158', \n","                 'P159', 'P16', 'P160', 'P161', 'P162', 'P163', 'P165', 'P166', \n","                 'P17', 'P174', 'P177', 'P18', 'P184', 'P19', 'P195', 'P2', 'P20', \n","                 'P201', 'P21', 'P211', 'P215', 'P218', 'P219', 'P22', 'P220', 'P223', \n","                 'P225', 'P228', 'P23', 'P230', 'P231', 'P233', 'P234', 'P235', 'P237', \n","                 'P238', 'P24', 'P241', 'P242', 'P243', 'P245', 'P246', 'P248', 'P249', \n","                 'P25', 'P251', 'P252', 'P255', 'P256', 'P258', 'P259', 'P26', 'P264', \n","                 'P266', 'P267', 'P268', 'P27', 'P270', 'P272', 'P275', 'P28', 'P280', \n","                 'P282', 'P284', 'P285', 'P286', 'P287', 'P288', 'P29', 'P290', 'P291', \n","                 'P292', 'P293', 'P294', 'P295', 'P296', 'P297', 'P298', 'P299', 'P3', \n","                 'P30', 'P301', 'P302', 'P303', 'P304', 'P305', 'P307', 'P308', 'P309', 'P31', 'P310', 'P316', 'P319', 'P32', 'P320', 'P322', 'P323', 'P329', 'P33', 'P330', 'P331', 'P336', 'P337', 'P338', 'P339', 'P34', 'P340', 'P341', 'P343', 'P344', 'P346', 'P347', 'P348', 'P349', 'P35', 'P350', 'P37', 'P372', 'P374', 'P375', 'P376', 'P378', 'P379', 'P38', 'P380', 'P381', 'P383', 'P384', 'P385', 'P386', 'P39', 'P397', 'P398', 'P399', 'P40', 'P402', 'P403', 'P409', 'P410', 'P42', 'P424', 'P43', 'P44', 'P443', 'P445', 'P446', 'P450', 'P46', 'P465', 'P467', 'P468', 'P48', 'P482', 'P484', 'P49', 'P492', 'P5', 'P500', 'P501', 'P502', 'P51', 'P510', 'P52', 'P524', 'P527', 'P529', 'P53', 'P54', 'P55', 'P57', 'P58', 'P596', 'P597', 'P6', 'P609', 'P610', 'P63', 'P633', 'P64', 'P65', 'P67', 'P670', 'P671', 'P68', 'P69', 'P697', 'P7', 'P70', 'P71', 'P72', 'P724', 'P725', 'P752', 'P753', 'P754', 'P755', 'P756', 'P757', 'P758', 'P759', 'P760', 'P761', 'P763', 'P766', 'P767', 'P768', 'P769', 'P771', 'P772', 'P775', 'P776', 'P777', 'P779', 'P780', 'P781', 'P783', 'P784', 'P785', 'P786', 'P787', 'P788', 'P789', 'P791', 'P794', 'P795', 'P796', 'P797', 'P798', 'P8', 'P800', 'P801', 'P804', 'P805', 'P806', 'P807', 'P808', 'P809', 'P810', 'P811', 'P813', 'P815', 'P817', 'P819', 'P821', 'P822', 'P823', 'P826', 'P827', 'P83', 'P830', 'P831', 'P84', 'P840', 'P841', 'P842', 'P844', 'P846', 'P847', 'P85', 'P850', 'P851', 'P852', 'P853', 'P855', 'P858', 'P859', 'P86', 'P861', 'P866', 'P87', 'P871', 'P880', 'P889', 'P89', 'P892', 'P9', 'P90', 'P91', 'P914', 'P915', 'P92', 'P924', 'P927', 'P929', 'P930', 'P931', 'P932', 'P933', 'P934', 'P935', 'P937', 'P938', 'P939', 'P94', 'P940', 'P941', 'P942', 'P943', 'P944', 'P946', 'P947', 'P948', 'P949', 'P95', 'P951', 'P953', 'P954', 'P955', 'P956', 'P957', 'P958', 'P959', 'P96', 'P961', 'P962', 'P963', 'P964', 'P965', 'P966', 'P967', 'P968', 'P969', 'P97', 'P970', 'P971', 'P972', 'P973', 'P974', 'P975', 'P976', 'P977', 'P978', 'P98', 'P981', 'P982', 'P983', 'P984', 'P986', 'P987', 'P988', 'P989', 'P99', 'P990', 'P991', 'P992', 'P993', 'P994', 'P995', 'P996', 'P997', 'P998', 'P999'})"]},{"cell_type":"markdown","metadata":{"id":"RVZsPo1p2RoP"},"source":["####Normalization"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655291173797,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"dDXCstVB2XIb"},"outputs":[],"source":["def norm(features) :\n","  # print(f'norm(): {features} size {features.size()}')\n","  sq_feat = features ** 2\n","  # print(f'norm(): {sq_feat} size {sq_feat.size()}')\n","  sum_feat = sq_feat.sum(1)\n","  # print(f'norm(): {sum_feat} size {sum_feat.size()}')\n","  norm_feat = torch.sqrt(sum_feat)\n","  # print(f'norm(): {norm_feat} size {norm_feat.size()}')\n","  # print(norm_feat.view(features.size(0), 1))\n","  unit_feat = features / norm_feat.view(features.size(0), 1)\n","  # print(f'norm(): {unit_feat} size {unit_feat.size()}')\n","  return unit_feat\n","# _, observed, __ = SimData(net_char=4, tmstp=80)\n","# norm(observed)"]},{"cell_type":"markdown","metadata":{"id":"TmFOoA-k258k"},"source":["####Subsampling"]},{"cell_type":"markdown","metadata":{"id":"5wCj-uKc561c"},"source":["#####Mask Generation\n","- (1/0) Sensing Mask Vector"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655291173798,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"QZnE0eEEGO88"},"outputs":[],"source":["def sensing_mask_rand(feature_vec, max_sense = 30):\n","  \"\"\"\n","  preconditions: feature tensor must be flat.\n","\n","  Parameters\n","  feature_vec: feature tensor to be masked\n","   shape: list of dimensions\n","  max_sense: randomly choose 1 to max_sense to be 1, rest 0\n","\n","  returns 0/1 sensing mask tensor of shape feature_vec w/ max_sense elems set to 1 (rest 0)\n","  \"\"\"\n","\n","  # Create (0/1) mask populated w/ max_sense ones\n","  mask = torch.zeros(feature_vec.size()).to(device)\n","  # print(mask)\n","  # indices = torch.randint(len(feature_vec), size=(max_sense,))   # may contain fewer than max_sense unique indices\n","  # Katie: Randomize indices and choose the first max_sense\n","  indices = [*range(len(feature_vec))]\n","  random.shuffle(indices)\n","  # print(indices)\n","  # for idx in indices:   # for use with line 16\n","  for idx in indices[:max_sense]:\n","    mask[idx] = 1\n","  # print(mask)\n","\n","  # Pass features through the mask\n","  masked_features = feature_vec * mask\n","\n","  return mask, masked_features.to(device)\n","# feature_vec = torch.rand([100])\n","# print(feature_vec)\n","# mask, masked_feats = sensing_mask_rand(feature_vec)\n","# print(mask, '\\n', masked_feats)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655291173798,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"jaJ04dzFpbzL"},"outputs":[],"source":["def sensing_mask_alternate(feature_vec):\n","  \"\"\"\n","  preconditions: feature tensor must be flat.\n","\n","  Parameters\n","  feature_vec: feature tensor to be masked\n","\n","  returns 0/1 sensing mask tensor of shape feature_vec w/ even indexed elems set to 1 (rest 0)\n","  \"\"\"\n","\n","  # Create (0/1) mask populated w/ max_sense ones\n","  mask = torch.zeros(feature_vec.size()).to(device)\n","  # print(mask)\n","  for idx in range(0, feature_vec.size()[0], 2) :\n","    mask[idx] = 1\n","  # print(mask)\n","\n","  # Pass features through the mask\n","  # masked_features = feature_vec * mask\n","\n","  # Simplification: This version reduces the size of the feature\n","  #  vector by half (mask not concat).\n","  masked_features = feature_vec[0].reshape([1])\n","  for idx in range(2, feature_vec.size()[0], 2) :\n","    masked_features = torch.cat((masked_features, feature_vec[idx].reshape([1])))\n","\n","  return mask, masked_features.to(device)\n","# feature_vec = torch.rand([100]).to(device)\n","# print(feature_vec)\n","# mask, masked_feats = sensing_mask_alternate(feature_vec)\n","# print(mask, '\\n', masked_feats.size(), '\\n', masked_feats)"]},{"cell_type":"markdown","metadata":{"id":"7KkFT5x93F5H"},"source":["#####Random subsamples"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655291173798,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"xfeD3clN3QKu"},"outputs":[],"source":["def random_subset(meas_tensor, size, seed=None) :\n","  \"\"\"Random subset of measurements (fixed seed).\n","  Must be able to change the cardinality of the subset.\n","  Algorithm\n","    create list of indices\n","    randomize indices\n","    select first x number of indices (where x is the subset cardinality)\n","    form a tensor containing the the sample measurements matching those indices.\n","    return this tensor\n","  \"\"\"\n","  gen = None\n","  if seed :\n","    gen = torch.Generator().manual_seed(seed)\n","\n","  rand_idxs = torch.randperm(meas_tensor.size()[0], generator=gen)\n","  sub_idxs = rand_idxs[:size]\n","  # print('random_subset():', sub_idxs, sub_idxs.size())\n","  subset = meas_tensor[sub_idxs]\n","  # print(meas_tensor.size())\n","  return subset.to(device), sub_idxs\n","\n","# Test code\n","# X = torch.rand(100)\n","# rand_sub = random_subset(X, 50, 1000)\n","# print('random_subset results:', rand_sub)\n","# print(rand_sub.size())\n","# assert False"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655291173798,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"Di_I5E4g4PDR"},"outputs":[],"source":["def rand_sub_dataset(X, size, seed=None) :\n","  reduced_meas_X = torch.zeros([X.size(0), size]).to(device)\n","  # print(X.size(0))\n","  for i in range(len(X)) :\n","    rand_sub, sub_idxs = random_subset(X[i], size, seed)\n","    # print('random_subset results:', rand_sub)\n","    # print(rand_sub.size())\n","    reduced_meas_X[i] = rand_sub\n","  return reduced_meas_X, sub_idxs\n","\n","# Test Code\n","# X = torch.rand([3, 20])\n","# size, seed = 10, 1000\n","# reduced_meas_X, sub_idxs = rand_sub_dataset(X, size, seed)\n","# # print(sub_idxs)\n","# for i, idx in enumerate(sub_idxs) :\n","#   # print(reduced_meas_X[0, i], X[0, idx])\n","#   if reduced_meas_X[0, i].item() != X[0, idx].item() :\n","#     raise Exception('rand_sub_dataset(): Error: Value doesnt match.')\n","# # print(X[0, 307], X[0, 536], X[0, 329])\n","# # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n","# print('Done!')"]},{"cell_type":"markdown","metadata":{"id":"if63qNQuS0HA"},"source":["####Load CSV (SimData) and form concat dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655291173798,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"uY7IINhjLGSQ"},"outputs":[],"source":["def SimData(net_char, dset_size, tmstp_path, tmstp) :\n","  \"\"\"\n","  Note: Dataset file generated in SimData_to_csv notebook\n","   row 0: base_case\n","   rows 1->set_size: observed; one leak scenario per row; 1hr (out of one week)\n","    Labels included (last column); pipe index (second to last column) for use w/ conf mat\n","  \"\"\"\n","\n","  # Load dataframe\n","  # Moved file path assembly to filepaths(); centralizing input/output path update\n","  # src_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/'\n","  # leak_pip_ct = 'all'\n","  # leak_pipes = f'leak_pipes_{leak_pip_ct}/'\n","  # regions = '39regions/00/'\n","  if tmstp :\n","    ts_dir = f'tmstp{tmstp}/'\n","  else :\n","    ts_dir = ''\n","  # # NOTE: double check dataset file name\n","  # dset_size = 5000\n","  # dataset_file = regions + ts_dir + f'dataset{dset_size}'\n","  dataset_file = ts_dir + f'dataset{dset_size}'\n","  areaLo, areaHi = 0.01, 0.1\n","  leak_area = f'area{areaLo}_{areaHi}'\n","  if net_char == 0:   dataset_file += f'_link_flowrate_{leak_area}.csv'\n","  elif net_char == 1: dataset_file += f'_link_headloss_{leak_area}.csv'\n","  elif net_char == 2: dataset_file += f'_link_velocity_{leak_area}.csv'\n","  elif net_char == 3: dataset_file += f'_node_demand_{leak_area}.csv'\n","  elif net_char == 4: dataset_file += f'_node_head_{leak_area}.csv'\n","  elif net_char == 5: dataset_file += f'_node_pressure_{leak_area}.csv'\n","  \n","  data_file = tmstp_path + dataset_file\n","  # print(data_file)\n","  # data_file = src_dir + leak_pipes + dataset_file\n","  datast_ds = pd.read_csv(data_file)\n","  # print(datast_ds.head())   # View csv data.\n","  \n","  # Separate base_case, raw_data, and encoded labels\n","  # Base Case\n","  # print(datast_ds.values[0, :-1].astype(np.float32))\n","  # print(type(datast_ds.values[0, :-1].astype(np.float32)))\n","  # base_case = torch.tensor(datast_ds.values[0, :-1], dtype=torch.float32).to(device)\n","  # base_case = torch.tensor(datast_ds.values[0, :-1].astype(np.float32), dtype=torch.float32).to(device)\n","  base_case = torch.tensor(datast_ds.values[0, :-2], dtype=torch.float32).to(device)   # Used w/ separate cols for pipeIdx and Label (no tuple)\n","  # print(base_case)\n","  # raw_data\n","  # raw_data = torch.tensor(datast_ds.values[1: , :-1], dtype=torch.float32).to(device)\n","  # raw_data = torch.tensor(datast_ds.values[1: , :-1].astype(np.float32), dtype=torch.float32).to(device)  # Had to add astype() when I added tuples in label col\n","  raw_data = torch.tensor(datast_ds.values[1: , :-2], dtype=torch.float32).to(device)   # Used w/ separate cols for pipeIdx and Label (no tuple)\n","  print(f'SimData(): raw_data {raw_data.size()}')\n","  # encoded labels\n","  # print(datast_ds['Label'][1])\n","  # print(type(datast_ds['Label'].astype('int')[1]))\n","  pipe_idxs = torch.tensor(datast_ds['PipeIdx'], dtype=torch.long)[1:].to(device)\n","  labels = torch.tensor(datast_ds['Label'], dtype=torch.long)[1:].to(device)\n","  # Possible implementation for pipe_idx (mapping of pipe_nms to pipe_idx done in simdata_to_csv notebook)\n","  #  Change base_case and raw_data to [1: , :-2]; the last two rows would contain pipe_idx and label values respectively.\n","  # pipe_idx = torch.tensor(datast_ds['Pipe_idx'], dtype=torch.long)[1:].to(device)\n","  print(f'SimData(): labels {labels.size()}')\n","\n","  return base_case, raw_data, labels, pipe_idxs\n","# _, __, ___ = SimData(0, 80)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655291173799,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"A6Xn9Due5kYf"},"outputs":[],"source":["def leakpipe_subset(pipe_ct, raw, enc_labs):\n","  \"Used to limit the number of pipes in the dataset\"\n","  tmp_raw = torch.tensor([[0]]).to(device)   # dim trouble w/ cat\n","  tmp_labs = torch.tensor([0]).to(device)\n","  print(tmp_labs.size())\n","\n","  for i in range(len(enc_labs)):\n","    if enc_labs[i] < pipe_ct:\n","      tmp_raw = torch.cat((tmp_raw, raw[i].reshape([444])))\n","      tmp_labs = torch.cat((tmp_labs, enc_labs[i].reshape([1])))\n","\n","  return tmp_raw[1:], tmp_labs[1:]"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655291173799,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"alN3uGMtzKbn"},"outputs":[],"source":["def cat_net_attr(net_ls, dset_size, ts_path, tmstp, residual=False, norm_feats=False) :\n","  \"\"\"Creates three flat vectors containing concatinations of each attribute's\n","       (1) base_case, (2) observed measurements, and (3) a single copy of label data (leak labels for all attrs are the same).\n","     **If residual is true, don't use norm here.**\n","     \"\"\"\n","  cat_attrs = None\n","  for net_char in net_ls :\n","    # base_case, X_raw, encoded_labels\n","    data = SimData(net_char, dset_size, ts_path, tmstp)\n","    data_ls = list(data)   # Creates 3 elem list from data 3-tuple returned by SimData.\n","\n","    if residual :   # Residual before normalization of features.\n","      data_ls[1] -= data_ls[0]\n","    # print(data_ls[1])\n","    # assert False\n","    if norm_feats :   # Normalize each attribute data separtely before cat.\n","      data_ls[1] = norm(data_ls[1])\n","\n","    if not cat_attrs :\n","      cat_attrs = data_ls   # Includes labels. Only needed once.\n","    else :\n","      cat_attrs[0] = torch.cat((cat_attrs[0], data_ls[0]))  # Update base_case by cat'ing next net_attr.\n","      cat_attrs[1] = torch.cat((cat_attrs[1], data_ls[1]), dim=1)   # Update features by cat'ing next net_attr.\n","      # only need one set of label; no need to cat those again.\n","    # Normalize Features -- all together (i.e. if more than one attribute is cat'ed, all will be norm'ed together)\n","    # if norm_feats:\n","      # Raw data normed across all samples\n","      #  Would it make more sense to normalize ea sample individually?\n","      # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n","      # cat_attrs[1] = norm(cat_attrs[1])\n","  print(f'cat_net_attr(): Base {cat_attrs[0].size()}, X {cat_attrs[1].size()}, Labels {cat_attrs[2].size()}')\n","  return cat_attrs   # cat_attrs is a list (of tensors), but can be expanded by the caller.\n","# _, __, ___ = cat_net_attr([0, 4], 80, True)\n","# print(_.size(), __.size(), ___.size())\n","# assert False"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1863,"status":"ok","timestamp":1655291175655,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"WWwFaq5FMiPh"},"outputs":[],"source":["def cat_data(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], dset_size=-1, ts_path=None, tmstp=None) :\n","  \"\"\"Idea: pass list of tmstps to this function, for loop the list to create a training set w/ training samples from mult tmstps.\n","  \"\"\"\n","  if isinstance(net_char, int) :\n","    net_char = [net_char]\n","  \n","  n_fs = norm_feats\n","  if residual :\n","    n_fs = False\n","  \n","  # Consider writing a script that automates both pulling mult tmstps of sim data together, but also cats features\n","  #  or perhaps just a dataset constructor script where multiple tmstps can be assembled into a single training set.\n","  base_case, X_raw, encoded_labels, pipIdxs = cat_net_attr(net_char, dset_size, ts_path, tmstp, residual, norm_feats=n_fs)   # Don't norm_feat in cat_net_attr if residual is true.\n","  # base_case, X_raw, encoded_labels = SimData(net_char, tmstp)\n","  print(f'cat_data(): X_raw max {torch.max(X_raw)}, min {torch.min(X_raw)}')\n","  print(f'cat_data(): X_raw range {torch.max(X_raw) + abs(torch.min(X_raw))}')\n","  print(f'cat_data(): X_raw {X_raw.size()}')\n","  # print(f'cat_data(): X_raw {X_raw[0]}')\n","  # print(f'cat_data(): base_case {base_case}')\n","  # print(f'cat_data(): pipe_idxs {encoded_labels}')\n","  \n","  # work in progress\n","  # X_raw, encoded_labels = leakpipe_subset(10, X_raw, encoded_labels)\n","  # print(f'cat_data: encoded labels \\n{encoded_labels}')\n","  # print(f'cat_data: encoded labels {encoded_labels.size()}')\n","\n","  # X_cat = None\n","  # norm_str = ''   # Used for debug output\n","  # Residual -- handled in cat_net_data\n","  # div_by_base = base_case   # What is this used for?\n","  # if residual:\n","    # Avoid dividing by zero\n","    # for idx in range(len(base_case)):\n","      # if base_case[idx] < 1.0e-12:\n","      #   div_by_base[idx] = 1.0e-10\n","    # X_raw = X_raw / div_by_base\n","    # X_raw -= base_case\n","    # base_case /= base_case   # might divide by zero\n","    # pass\n","  # print(f'cat_data(): X_raw {X_raw[0]}')\n","  \n","  # Normalizations\n","  #  Not sure this code is doing anything as X_raw has already been formed.\n","  if norm_base:\n","    # Normalize base_case\n","    #  orders of magnitude larger than residual data\n","    #  v = v / max(||v||_p, epsilon)  where epsilon is a small value that void dividing by zero\n","    # norm_base_case = torch.nn.functional.normalize(base_case.reshape([1,-1]))\n","    base_case = torch.nn.functional.normalize(base_case, dim=0) * 10.0   # Out-dated, use with caution.\n","    norm_str = 'norm_'\n","  # if norm_feats:\n","  #   # Raw data normed across all samples\n","  #   #  Would it make more sense to normalize ea sample individually?\n","  #   # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n","  #   X_raw = norm(X_raw)\n","  # print(f'cat_data(): {norm_str}base_case {base_case.size()}')\n","  # # print(f'cat_data(): {norm_str}base_case {base_case}')\n","\n","  # print(X_raw[0])   # X_raw elements are already a flat tensor.\n","  # I think I can move if mask outer and elim the for loop.\n","  # for feature_vec in X_raw:\n","  #   if mask :\n","  #     # Mask and masked features\n","  #     #  May want sensing_mask_rand() that can process batches of samples\n","  #     # mask_tn, masked_feats = sensing_mask_rand(feature_vec)\n","  #     # Construct feature set from concatination of base_case, mask, and masked measuremnts.\n","  #     # temp = torch.cat((masked_feats.to(device), mask.to(device), base_case.to(device))).reshape([1,-1])\n","  #     # base_case measurements not included\n","  #     # temp = torch.cat((masked_feats.to(device), mask_tn.to(device))).reshape([1, -1])\n","  #     # Simplification: notice feature vector size is halved. Update col variable accordingly.\n","  #     mask_tn, masked_feats = sensing_mask_alternate(feature_vec)\n","  #     # Not Needed. Adjust size of label vector.\n","  #     # masked_labels = encoded_labels[0].reshape([1])\n","  #     # print(encoded_labels.size())\n","  #     # assert False\n","  #     # for idx in range(2, encoded_labels.size()[0], 2) :\n","  #     #   masked_labels = torch.cat((masked_labels, encoded_labels[idx].reshape([1])))\n","  #     temp = masked_feats.reshape([1,-1])\n","  #     # encoded_labels = masked_labels\n","  #     # print(f'cat_data(), mask: enc_lab size {encoded_labels.size()}')\n","\n","  #   else :\n","  #     # Features (no mask)\n","  #     # Construct feature set from concatination of base_case and observed measuremnts.\n","  #     # temp = torch.cat((feature_vec.to(device), base_case.to(device))).reshape([1,-1])\n","  #     # I don't think this is doing anything.  Check if X_raw is changed by this. May already be a flat tensor after cat_net_attrs call.\n","  #     temp = feature_vec.reshape([1,-1])   # Tensor containing a tensor. Not sure that's needed.\n","  #   # print(f'cat_data(): temp {temp}')\n","\n","    # if X_cat is None:\n","    #   X_cat = temp\n","    #   # print(X_cat)\n","    # else:  \n","    #   X_cat = torch.cat((X_cat, temp))\n","    #   # print(X_cat)\n","\n","  # print(f'cat_data(): X_cat {X_cat.size()}')\n","  # print(f'cat_data(): X_cat {X_cat[0]}')\n","  # assert False\n","  # return X_cat, encoded_labels   # Not returning base_case at this time.\n","  return X_raw, encoded_labels, pipIdxs   # Not returning base_case at this time.\n","# print(f'output: {cat_data(residual=True, norm_base=False, norm_feats=False, mask=False)}')\n","# assert False"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291175655,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"8lxQz9XbUBgO"},"outputs":[],"source":["def cat_data_mult_tmstps(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], dset_size=-1, ts_path=None, tmstps=None) :\n","  \"\"\"This func wraps cat_data() to concat traning samples from multiple time stamps.\n","  Notice tmstps is a list of time stamps to be included in the training set\n","      tmstps can include one time stamp to mimic previouis version, but must be in a list.\n","  \"\"\"\n","  X_raw, encoded_labels, pipIdxs = None, None, None\n","\n","  for tmstp in tmstps :\n","    X_r, enc_labs, pipIds = cat_data(residual, norm_base, norm_feats, mask, net_char, dset_size, ts_path, tmstp)\n","    if X_raw is None :\n","      X_raw = torch.cat([X_r])\n","      encoded_labels = torch.cat([enc_labs])\n","      pipIdxs = torch.cat([pipIds])\n","    else :\n","      X_raw = torch.cat([X_raw, X_r])\n","      encoded_labels = torch.cat([encoded_labels, enc_labs])\n","      pipIdxs = torch.cat([pipIdxs, pipIds])\n","  # print(f'X_raw {X_raw.size()}')\n","  # assert False\n","\n","  return X_raw, encoded_labels, pipIdxs"]},{"cell_type":"markdown","metadata":{"id":"CdawEj-SS9hP"},"source":["####Train the model"]},{"cell_type":"markdown","metadata":{"id":"j_Zwmb6sYpbm"},"source":["#####Set-up Training"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291175655,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"tcdigZ4CO-pn"},"outputs":[],"source":["def filepaths() :\n","  \"\"\"Consider splitting input and output paths into two separate functions.\n","  \"\"\"\n","  ## Inputs\n","  partitions = 20\n","  leak_pip_ct = 'all'\n","  leak_pipes = f'leak_pipes_{leak_pip_ct}/'\n","\n","  src_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/'\n","  reg_dir = src_dir + f'Input Pipeline/Datasets/' + leak_pipes + f'{partitions}regions/'\n","  version_dir = reg_dir + '00/'\n","\n","  #  file path in Confusion Matrix cell; decode_labels(); region_dict_xx.pickle\n","  file_nm = f'region_dict_{partitions}.pickle'\n","  regdict_filenm = version_dir + file_nm\n","\n","  #  file path in SimData(); dest path from step 3\n","  tmstp_path = version_dir\n","  # NOTE: double check dataset file name\n","  dset_size = 5000\n","\n","  #  file path in Training setup and loop cell; pipe to pipe_idx.npy\n","  pToPId_dir = version_dir\n","  file_nm = 'dictPipeToPipeIdx.npy'\n","  pToPIdx_filenm = pToPId_dir + file_nm\n","  ## -------------- ##\n","\n","  ## Outputs\n","  analy_dir = src_dir + f'Analysis/{partitions}regions/'\n","  download_dir = analy_dir + f'download_{partitions}regs/'\n","\n","  #  file path in Training setup and loop cell; display_of_sensor_grid.pt\n","  sensgrid_filenm = analy_dir + f'display_of_sensor_grid.pt'\n","\n","  #  file paths (3) in Graphics cell; loss.png, conf.png, conf_mat.pt\n","  conf_mat_filenm = analy_dir + 'conf_mat.pt'\n","  loss_filenm = download_dir + 'loss.png'\n","  conf_filenm = download_dir + 'conf.png'\n","  ## -------------- ##\n","\n","  return partitions, regdict_filenm, tmstp_path, dset_size, pToPIdx_filenm, sensgrid_filenm, loss_filenm, conf_filenm, conf_mat_filenm"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655291175655,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"yLehjrJ5SV8M"},"outputs":[],"source":["rows = 0\n","cols = 0\n","input_dim = 0\n","output_dim = 0\n","\n","tr_dataset = None\n","ts_dataset = None"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291175656,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"gJJN6lECYvM_"},"outputs":[],"source":["from collections import Counter\n","import statistics as stats\n","\n","def histo_pipe_dist(labels) :\n","  \"\"\"Display histogram of leakpipe distribution.\n","  labels (tensor) : pytorch tensor of labels (ints).\n","  Note: requires matplotlib and Pandas.\n","  \"\"\"\n","  # Histo\n","  Y = pd.Series(labels.cpu())\n","  recounted = Counter(Y)\n","  print(recounted)\n","  std = stats.stdev(recounted.values())\n","  print(f'stdev {std:.2}')\n","  Y.plot.hist(grid=True, bins=10, alpha=0.7, rwidth=0.8, color='#607c8e', align='mid')\n","  plt.title(f'Label Frequency for {len(Y)} Samples')\n","  plt.xlabel('Label')\n","  plt.grid(axis='x')\n","  # plt.text(6, 200, r'class 5 = 229 (46%)')\n","  # assert False\n","  return std"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291175656,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"PpisWMQEGjxL"},"outputs":[],"source":["def randomize_dataset(X, y) :\n","  random.seed(10343)\n","  ls = []\n","  for i in range(len(y)) :\n","    zipped = X[i], y[i]\n","    # print(zipped, end=\" \")\n","    ls.append(zipped)\n","  # print('randomize_dataset(): ls', len(ls))\n","  random.shuffle(ls)\n","  shuf_X = torch.empty([1, len(X[0])]).to(device)\n","  # print(shuf_X.size(), shuf_X)\n","  shuf_y = torch.empty([1], dtype=torch.long).to(device)\n","  # print(shuf_X.size(), shuf_X)\n","  for feat, label in ls :\n","    # print('randomize_dataset(): feat', feat.reshape([1, -1]).size())\n","    shuf_X = torch.cat((shuf_X, feat.reshape([1, -1])))\n","    # print('randomize_dataset(): label', label.reshape([1]).size())\n","    # label = torch.tensor(label, dtype=torch.long).to(device)\n","    shuf_y = torch.cat( (shuf_y, label.reshape([1])) )\n","  # print(shuf_X)\n","  # print(shuf_y)\n","  # print('randomize_dataset(): shuf_X', shuf_X.size())\n","  # print('randomize_dataset(): shuf_y', shuf_y.size())\n","  return shuf_X[1:], shuf_y[1:]"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655291175656,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"WfhI0Do7Ad_o"},"outputs":[],"source":["def find_seed(X, y, tr_size, tr_or_ts) :\n","  min_std = 1e9\n","  # tr_or_ts = 0  # tr = 0, ts = 1\n","  for i in range(20, 150) :\n","    print(i)\n","    subsets = torch.utils.data.random_split(TensorDataset(X, y),\n","                                            [tr_size, len(y) - tr_size],\n","                                            generator=torch.Generator().manual_seed(i),\n","                                            )\n","    # print(len(subsets),\n","    #       subsets[1].dataset.tensors[1].size(),\n","    #       type(subsets[1].indices),\n","    #       len(subsets[1].indices),\n","    #       subsets[1].dataset.tensors[1][[i for i in subsets[1].indices]],\n","    #       )\n","    std = histo_pipe_dist(subsets[tr_or_ts].dataset.tensors[1][[i for i in subsets[tr_or_ts].indices]])\n","    if std < min_std :\n","      min_std = std\n","      seed = i\n","  print(f'Seed Found: argmin_stdev {seed}\\n')\n","  return seed"]},{"cell_type":"markdown","metadata":{"id":"qysp3xV5W2dg"},"source":["#####MNISTfashion set"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655291175656,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"v2G9BTikRI0k","outputId":"e36ef088-38a5-4fc7-8a56-baf9e61db85a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntr_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=True,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nts_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=False,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nrows = 28\\ncols = 28\\ninput_dim = rows*cols\\noutput_dim = 10\\n\\nlearning_rate = 1e-3\\nepochs = 10\\nbatch_size = 64\\nmod = 100\\n#'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["# mnist\n","\"\"\"\n","tr_dataset = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","\n","ts_dataset = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","\n","rows = 28\n","cols = 28\n","input_dim = rows*cols\n","output_dim = 10\n","\n","learning_rate = 1e-3\n","epochs = 10\n","batch_size = 64\n","mod = 100\n","#\"\"\""]},{"cell_type":"markdown","metadata":{"id":"uBUCPLjMROO0"},"source":["#####CSV data Loading"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":13899,"status":"ok","timestamp":1655291189549,"user":{"displayName":"Hugo Chacon","userId":"09326270256433817317"},"user_tz":420},"id":"sPNhrjauC1OW","outputId":"3b3afc49-9558-4b8b-f642-7e0a83d2b1f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 43.62644958496094, min -23.13990020751953\n","cat_data(): X_raw range 66.76634979248047\n","cat_data(): X_raw torch.Size([5000, 840])\n","SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 41.52079772949219, min -26.361465454101562\n","cat_data(): X_raw range 67.88226318359375\n","cat_data(): X_raw torch.Size([5000, 840])\n","SimData(): raw_data torch.Size([5000, 444])\n","SimData(): labels torch.Size([5000])\n","SimData(): raw_data torch.Size([5000, 396])\n","SimData(): labels torch.Size([5000])\n","cat_net_attr(): Base torch.Size([840]), X torch.Size([5000, 840]), Labels torch.Size([5000])\n","cat_data(): X_raw max 31.60552215576172, min -16.47759246826172\n","cat_data(): X_raw range 48.08311462402344\n","cat_data(): X_raw torch.Size([5000, 840])\n","X size: torch.Size([15000, 840])\n","Counter({18: 686, 19: 679, 12: 624, 16: 602, 9: 596, 17: 580, 14: 573, 3: 549, 10: 532, 11: 499, 1: 487, 15: 483, 5: 479, 8: 477, 4: 476, 13: 474, 7: 473, 0: 423, 2: 409, 6: 399})\n","stdev 8.3e+01\n","cols 840\n","Autoencoder(\n","  (encoder): Encoder(\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (binary_STE_stack): Sequential(\n","      (0): StraightThroughEstimator()\n","    )\n","  )\n","  (decoder): Decoder(\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (linear_lrelu_stack): Sequential(\n","      (0): Linear(in_features=840, out_features=512, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=512, out_features=20, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n",")\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5gcVbnv8e+PBALhloTACEk2Ac0BEUXDcPF42RMRCKiEjZoNWzQgmoOC4gEEvAGKHPG4BcS95RiFQ7hsEgSFbAxCBCLHreESLiFclOEmCYEACQk3wch7/lhrQqfpmeqZTFd3Mr/P8/QzVWutqvVOTU2/VauqqxURmJmZ9WSDZgdgZmatz8nCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCTxQAnaa6kz5W97EAhqU3SLZJekPTDZscz0Eg6XdKlzY5jfeBksZ6Q9JikDzc7ji75n/Rvkl6seJ3U7LiaYCrwLLBFRJywtiuTtK2kWZKelBSSxlbVD5F0oaSVkp6SdHxF3di8TOXf5Fv1LJvr95H0oKSXJd0safse4ny/pD9IWiFpmaT/krTH2v7+1jyDmx2ArddmRsThPTWQNCgi/l5WQE2wPXB/9OHTr5IGR8SqquLXgd8A3wP+UGOx04Fxud+3ADdLuj8iflPRZliN9fa4rKSRwC+BzwH/CZwBzAT2rhH3FsC1wBeAK4CNgA8Ar9bxa1uL8pnFek7ScEnXSnpG0vI8Pbqq2Vsl3ZaPKK+RNKJi+b3zEeLzku6R1LGW8Vwk6XxJsyW9BEyQtJ2kq3KMj0r6ckX7TfIyyyXdL+mrkhZV1Iekt1Wt/7sV8x+VdHeO/w+S3lVR95ikEyUtyEfAMyVtXFE/KS+7UtLDkiZK+qSk+VW/0/GSrqn1uwJTgJPyUfyH89H7ufnM4Mk8PSS375C0SNLJkp4C/m/1OiPi6Yj4CXB7N5t4CnBGRCyPiAeAnwFHdNO2N8seAtwXEb+IiL+SEstuknausZ7/lmO9PCL+HhGvRMQNEbEg/55vlXSTpOckPSvpMknDKrbbY/nvvEDSS5IuUBrOu05pOO+3kobntl1nS1Pz9lwi6cTufsGe9mdJR0h6JPfxqKRP1bndBoaI8Gs9eAGPAR+uUb4V8HFgKLA58Avg6or6ucBiYFdgU+Aq4NJcNwp4DjiQdGCxb57fumLZz3UTz+ld66kqvwhYAbwvr3MoMB84lXQEuiPwCLB/bn8W8P+AEcAYYCGwqGJ9Abytav3fzdPvAZYCewGDSG+GjwFDKrbZbcB2ef0PAEfnuj1znPvmOEcBOwNDgGXA2yv6vAv4eDfbYXU8ef47wDxgG2Br0tnBGbmuA1gFfD/3s0kPf+/B+XcfW1E2PJe1VZR9Arg3T4/N9YuBRaRkNLLOZX8EnF8Vw8JavzewRd5PpgMHAMOr6t+Wt+uQvA1uAc6t2pfnAW15uy8F7sx/z42Bm4DTqn6ny0n77zuBZ8j/C1Tsh/SwP+dlVwI75bbbAu9o9v91K718ZrGei4jnIuKqiHg5Il4AzgT+sarZJRGxMCJeAr4FTJY0CDgcmB0RsyPi9YiYA9xB+merx+R8BNf12i6XXxMR/xURr5P+ubeOiO9ExGsR8QjpiPbQrnUAZ0bEsoh4AjivF7/+VOCnEXFrpCPc6aShkMqhk/Mi4smIWEYaXnl3Lj8KuDAi5uTffXFEPBgRr5KGXw4HkPQO0hvWtXXG9CngOxGxNCKeAb4NfLqi/nXSG+GrEfFKL35XgM3yzxUVZStIBwmQrp3sQRpm2j2XX1bnsptV1VXXrxYRK4H3k97EfwY8o3SdpS3Xd+bt+mreBmfz5n3yx5HOohaTDhZujYi7Ip3V/IqUOCp9OyJeioh7SUnwsOq4KN6fXwd2lbRJRCyJiPtqrGPAcrJYz0kaKumnkh6XtJJ0FDcsJ4MuT1RMPw5sCIwkval8svINn/QmsG2d3V8REcMqXk/W6G97YLuqPr5OOqqEdNRfHV+9tgdOqFr3mLzOLk9VTL/MG2+aY4CHu1nvdOBfJIn0Rn9FTiL12I41f4fHq+J5Jr8h9sWL+ecWFWVbAC8ARMSLEXFHRKyKiKeBY4H9JG1etGyur6yrrl9DRDwQEUdExGjSWet2wLmw+g6xGZIW533yUtL+VunpiulXasxvtmbzN+0j2/Fm3e7P+UDpn4GjgSWSft3NENuA5WSx/jsB2AnYKyK2AD6Yy1XRZkzF9D8AfyMdhT5BOuuofMPfNCLOWsuYKi/2PgE8WtXH5hHRdbS3pEZ8lV4mDWV1eUvVus+sWvfQiLi8jhifAN5aM/iIecBrpIu2/wJcUsf6ujxJetPq8g+5bPXqe7Gu6riWk7bXbhXFuwHdHSF39bVBHcveV1knaVPS9ik8+o6IB0nDcbvmov+V+35n3icPZ839sS+q95Ena7TpcX+OiOsjYl/SwdCDpLMiy5ws1i8bStq44jWYNEzwCvC80oXr02osd7ikXSQNJY2pXxnpDqVLgY9J2l/SoLzODr35AvnauA14IV/U3ST3s6veuM3yCuBrShfqRwNfqlr+btJR/iBJE1lzOONnwNGS9lKyqaSP5CPpIhcARyrdLrqBpFFVR5oXA/8G/C0ift+L3/dy4JuStla6w+hU0naum9JF+CF5dogqLsrnuL6Zt9fOwOdJb9Tk7bBT/n22Ig3pzY2IFUXLkoZ+dpX08dzfqcCCnAiq49tZ0gld+4mkMaRhoXm5SdeZzApJo4Cv9ub378a38ln0O4AjSUOF1brdn/PZzqScBF/N8b3eD3GtN5ws1i+zSYmh63U66dR/E9KZwjzSbZfVLiG9KTxFuoD4ZYB8jWASaVjoGdKR2Vfpx/0mJ6WPkq4VPJrj/DmwZW7ybdKwwqPADbz5KP444GPA86TrAVdXrPsO0hvevwHLgU7qvDMoIm4jvemcQxqb/x1rnhFcQjpS7u0Hvr5LGidfANxLunD73R6XeLNXeGPY6ME83+U00vDZ4znmH8Qbt83uSPr7v0C6OP0qa47td7tsvrbwcdI1r+WkmwYOpbYXcv2tSne8zcv9dX3O5NvAeNJ2/TXplty19TvS3/dG4F8j4obqBgX78wbA8aQzkmWkg44v9ENc6w1F+MuPbN2Rb3W8NI+FNzOOTUh36YyPiIeaGctApvShxEeBDaP2Z0esn/jMwqxvvgDc7kRhA4U/wW3WS5IeI12QPbjJoZiVxsNQZmZWyMNQZmZWaL0chho5cmSMHTu22WGYma1T5s+f/2xEbF2rbr1MFmPHjuWOO+5odhhmZusUSd0+IaFhw1BKz8VfKmlhjboT8pMiR+Z5STpPUqfSkybHV7SdIumh/JrSqHjNzKx7jbxmcREwsbowf5pzP+AvFcUHkJ6jP4708Lfzc9uuTxzvRXoK6GnKjyY2M7PyNCxZRMQtpE9CVjsHOIk1n4EzCbg4knmkB91tC+wPzMlPHF0OzKFGAjIzs8Yq9ZqFpEnA4oi4Jz2wc7VRrPnUyEW5rLvyWuueSjoroa2tjblz5/Zf4GZmA1xpySI/pO7rpCGofhcR04BpAO3t7dHR0dGIbszMBqQyP2fxVmAH4J78CdjRwJ2S3kL65q7KRwyPzmXdlZuZWYlKSxYRcW9EbBMRYyNiLGlIaXxEPAXMAj6T74raG1gREUuA60lfzjI8X9jeL5eZmVmJGnnr7OXAH4GdlL6E/qgems8mfe9yJ+k7CL4IkL/q8gzSl9PfTvo6yloXzc3MrIHWy2dDtbe3hz+UZ2bWO5LmR0R7rbr18hPcZmat6NxLry5utJa+cnhjHobsBwmamVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFWpYspB0oaSlkhZWlP1A0oOSFkj6laRhFXVfk9Qp6U+S9q8on5jLOiWd0qh4zcyse408s7gImFhVNgfYNSLeBfwZ+BqApF2AQ4F35GV+ImmQpEHAvwMHALsAh+W2ZmZWooYli4i4BVhWVXZDRKzKs/OA0Xl6EjAjIl6NiEeBTmDP/OqMiEci4jVgRm5rZmYlGtzEvj8LzMzTo0jJo8uiXAbwRFX5XrVWJmkqMBWgra2NuXPn9mesZmZrbbuhje+jUe99TUkWkr4BrAIu6691RsQ0YBpAe3t7dHR09Neqzcz6xbmXXt3wPiYf0tGQ9ZaeLCQdAXwU2CciIhcvBsZUNBudy+ih3Mys18p4w/7K4Qc3vI+ylXrrrKSJwEnAQRHxckXVLOBQSUMk7QCMA24DbgfGSdpB0kaki+CzyozZzMwaeGYh6XKgAxgpaRFwGunupyHAHEkA8yLi6Ii4T9IVwP2k4aljIuLveT3HAtcDg4ALI+K+RsVsZma1NSxZRMRhNYov6KH9mcCZNcpnA7P7MTQzM+slf4LbzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKNfMR5WbWRH6gnvWGzyzMzKyQk4WZmRVysjAzs0K+ZmEDnsfuzYr5zMLMzAo5WZiZWSEPQ5lZ6Tz0t+7xmYWZmRVysjAzs0IehjJrIg/H2LrCZxZmZlaoYclC0oWSlkpaWFE2QtIcSQ/ln8NzuSSdJ6lT0gJJ4yuWmZLbPyRpSqPiNTOz7jXyzOIiYGJV2SnAjRExDrgxzwMcAIzLr6nA+ZCSC3AasBewJ3BaV4IxM7PyNCxZRMQtwLKq4knA9Dw9HTi4ovziSOYBwyRtC+wPzImIZRGxHJjDmxOQmZk1WNkXuNsiYkmefgpoy9OjgCcq2i3KZd2Vv4mkqaSzEtra2pg7d27/RW3rte2GNr6P7vZH9+2+y+p7bTXtbqiICEnRj+ubBkwDaG9vj46Ojv5ata3nyrgjafIhHe7bfTe177VV9t1QT+fhJfLPpbl8MTCmot3oXNZduZmZlajsZDEL6LqjaQpwTUX5Z/JdUXsDK/Jw1fXAfpKG5wvb++UyMzMrUcOGoSRdDnQAIyUtIt3VdBZwhaSjgMeBybn5bOBAoBN4GTgSICKWSToDuD23+05EVF80NzOzBmtYsoiIw7qp2qdG2wCO6WY9FwIX9mNoZmbWS37cR4vx4x/MrBU5WdhqTlRm1h0/G8rMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkV8q2zNfgWUjOzNfnMwszMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKxQXXdDSXpnRNzb6GBs4PIdaGatrd4zi59Iuk3SFyVt2dCIzMys5dSVLCLiA8CnSN+HPV/Sf0jat6GRmZlZy6j7mkVEPAR8EzgZ+EfgPEkPSjqkUcGZmVlrqCtZSHqXpHOAB4APAR+LiLfn6XMaGJ+ZmbWAeh/38WPg58DXI+KVrsKIeFLSNxsSmZmZtYx6k8VHgFci4u8AkjYANo6IlyPikoZFZ2ZmLaHeaxa/BTapmB+ay/pE0v+UdJ+khZIul7SxpB0k3SqpU9JMSRvltkPyfGeuH9vXfs3MrG/qTRYbR8SLXTN5emhfOpQ0Cvgy0B4RuwKDgEOB7wPnRMTbgOXAUXmRo4Dlufyc3M7MzEpUb7J4SdL4rhlJuwOv9NC+yGBgE0mDSUlnCeli+ZW5fjrQ9QmqSXmeXL+PJK1F32Zm1kuKiOJG0h7ADOBJQMBbgH+OiPl96lQ6DjiTlHBuAI4D5uWzBySNAa6LiF0lLQQmRsSiXPcwsFdEPFu1zqnAVIC2trbdZ8yY0ZfQAFi67Pk+L1uvbUYMc9/u232779L6rseECRPmR0R7rbq6LnBHxO2SdgZ2ykV/ioi/9SUYScNJZws7AM8DvwAm9mVdVTFOA6YBtLe3R0dHR5/XVcajJyYf0uG+3bf7dt+l9b22evNNeXsAY/My4yURERf3oc8PA49GxDMAkn4JvA8YJmlwRKwCRgOLc/vFpE+OL8rDVlsCz/WhXzMz66N6P5R3CfCvwPtJSWMPoOapSh3+AuwtaWi+9rAPcD9wM/CJ3GYKcE2enpXnyfU3RT1jZ2Zm1m/qPbNoB3bpjzfpiLhV0pXAncAq4C7S8NGvgRmSvpvLLsiLXABcIqkTWEa6c8rMzEpUb7JYSLqovaQ/Oo2I04DTqoofAfas0favwCf7o18zM+ubepPFSOB+SbcBr3YVRsRBDYnKzMxaSr3J4vRGBmFmZq2t3ltnfydpe2BcRPxW0lDSJ6/NzGwAqPduqM+TPj3901w0Cmj8DcNmZtYS6n3cxzGkz0KshNVfhLRNo4IyM7PWUm+yeDUiXuuayR+O82cdzMwGiHqTxe8kfZ308L99SY/o+M/GhWVmZq2k3mRxCvAMcC/wP4DZpO/jNjOzAaDeu6FeB36WX2ZmNsDUlSwkPUqNaxQRsWO/R2RmZi2nN8+G6rIx6fEbI/o/HDMza0V1XbOIiOcqXosj4lzgIw2OzczMWkS9w1DjK2Y3IJ1p9Oa7MMzMbB1W7xv+DyumVwGPAZP7PRozM2tJ9d4NNaHRgZiZWeuqdxjq+J7qI+Ls/gnHzMxaUW/uhtqD9BWnAB8DbgMeakRQZmbWWupNFqOB8RHxAoCk04FfR8ThjQrMzMxaR72P+2gDXquYfy2XmZnZAFDvmcXFwG2SfpXnDwamNyYkMzNrNfXeDXWmpOuAD+SiIyPirsaFZWZmraTeYSiAocDKiPgRsEjSDg2KyczMWky9X6t6GnAy8LVctCFwaV87lTRM0pWSHpT0gKT3ShohaY6kh/LP4bmtJJ0nqVPSgqpPk5uZWQnqPbP4J+Ag4CWAiHgS2Hwt+v0R8JuI2BnYDXiA9J0ZN0bEOODGPA9wADAuv6YC569Fv2Zm1gf1JovXIiLIjymXtGlfO5S0JfBB4AKAiHgtIp4HJvHGRfPppIvo5PKLI5kHDJO0bV/7NzOz3lPKAQWNpBNJR/b7At8DPgv8R0T8uNcdSu8GpgH3k84q5gPHAYsjYlhuI2B5RAyTdC1wVkT8PtfdCJwcEXdUrXcq6cyDtra23WfMmNHb0FZbuuz5Pi9br21GDHPf7tt9u+/S+q7HhAkT5kdEe626wruh8hv3TGBnYCWwE3BqRMzpYzyDgfHAlyLiVkk/4o0hJwAiIiQVZ7E1l5lGSkK0t7dHR0dHH8ODcy+9us/L1mvyIR3u2327b/ddWt9rqzBZ5Dfu2RHxTqCvCaLSImBRRNya568kJYunJW0bEUvyMNPSXL8YGFOx/OhcZmZmJan3msWdkvbojw4j4ingCUk75aJ9SENSs4ApuWwKcE2engV8Jt8VtTewIiKW9EcsZmZWn3o/wb0XcLikx0h3RIl00vGuPvb7JeAySRsBjwBHkhLXFZKOAh7nje/LmA0cCHQCL+e2ZmZWoh6ThaR/iIi/APv3Z6cRcTdrfq93l31qtA3gmP7s38zMeqfozOJq0tNmH5d0VUR8vIygzMystRRds1DF9I6NDMTMzFpXUbKIbqbNzGwAKRqG2k3SStIZxiZ5Gt64wL1FQ6MzM7OW0GOyiIhBZQViZmatqzePKDczswHKycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCTUsWkgZJukvStXl+B0m3SuqUNFPSRrl8SJ7vzPVjmxWzmdlA1cwzi+OAByrmvw+cExFvA5YDR+Xyo4Dlufyc3M7MzErUlGQhaTTwEeDneV7Ah4Arc5PpwMF5elKeJ9fvk9ubmVlJFBHldypdCXwP2Bw4ETgCmJfPHpA0BrguInaVtBCYGBGLct3DwF4R8WzVOqcCUwHa2tp2nzFjRp/jW7rs+T4vW69tRgxz3+7bfbvv0vqux4QJE+ZHRHutusF9XmsfSfoosDQi5kvq6K/1RsQ0YBpAe3t7dHT0fdXnXnp1P0XVvcmHdLhv9+2+3Xdpfa+t0pMF8D7gIEkHAhsDWwA/AoZJGhwRq4DRwOLcfjEwBlgkaTCwJfBc+WGbmQ1cpV+ziIivRcToiBgLHArcFBGfAm4GPpGbTQGuydOz8jy5/qZoxtiZmdkA1kqfszgZOF5SJ7AVcEEuvwDYKpcfD5zSpPjMzAasZgxDrRYRc4G5efoRYM8abf4KfLLUwMzMbA2tdGZhZmYtysnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoVKTxaSxki6WdL9ku6TdFwuHyFpjqSH8s/huVySzpPUKWmBpPFlx2xmNtA148xiFXBCROwC7A0cI2kX4BTgxogYB9yY5wEOAMbl11Tg/PJDNjMb2EpPFhGxJCLuzNMvAA8Ao4BJwPTcbDpwcJ6eBFwcyTxgmKRtSw7bzGxAU0Q0r3NpLHALsCvwl4gYlssFLI+IYZKuBc6KiN/nuhuBkyPijqp1TSWdedDW1rb7jBkz+hzX0mXP93nZem0zYpj7dt/u232X1nc9JkyYMD8i2mvVDe7zWteSpM2Aq4CvRMTKlB+SiAhJvcpiETENmAbQ3t4eHR0dfY7t3Euv7vOy9Zp8SIf7dt/u232X1vfaasrdUJI2JCWKyyLil7n46a7hpfxzaS5fDIypWHx0LjMzs5I0424oARcAD0TE2RVVs4ApeXoKcE1F+WfyXVF7AysiYklpAZuZWVOGod4HfBq4V9LduezrwFnAFZKOAh4HJue62cCBQCfwMnBkueGamVnpySJfqFY31fvUaB/AMQ0NyszMeuRPcJuZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NC60yykDRR0p8kdUo6pdnxmJkNJOtEspA0CPh34ABgF+AwSbs0Nyozs4FjnUgWwJ5AZ0Q8EhGvATOASU2OycxswFBENDuGQpI+AUyMiM/l+U8De0XEsRVtpgJT8+xOwJ962c1I4Nl+CLdRWj0+cIz9xTH2D8fYe9tHxNa1KgaXHUmjRMQ0YFpfl5d0R0S092NI/arV4wPH2F8cY/9wjP1rXRmGWgyMqZgfncvMzKwE60qyuB0YJ2kHSRsBhwKzmhyTmdmAsU4MQ0XEKknHAtcDg4ALI+K+fu6mz0NYJWn1+MAx9hfH2D8cYz9aJy5wm5lZc60rw1BmZtZEThZmZlZoQCWLokeGSBoiaWauv1XS2JLjGyPpZkn3S7pP0nE12nRIWiHp7vw6tcwYcwyPSbo3939HjXpJOi9vxwWSxpcc304V2+duSSslfaWqTenbUdKFkpZKWlhRNkLSHEkP5Z/Du1l2Sm7zkKQpJcf4A0kP5r/lryQN62bZHveLBsd4uqTFFX/PA7tZtpTHBnUT48yK+B6TdHc3y5ayHXstIgbEi3Rh/GFgR2Aj4B5gl6o2XwT+T54+FJhZcozbAuPz9ObAn2vE2AFc2+Rt+Rgwsof6A4HrAAF7A7c2+e/+FOnDRk3djsAHgfHAwoqy/w2ckqdPAb5fY7kRwCP55/A8PbzEGPcDBufp79eKsZ79osExng6cWMe+0ON7QCNjrKr/IXBqM7djb18D6cyinkeGTAKm5+krgX0kqawAI2JJRNyZp18AHgBGldV/P5oEXBzJPGCYpG2bFMs+wMMR8XiT+l8tIm4BllUVV+5z04GDayy6PzAnIpZFxHJgDjCxrBgj4oaIWJVn55E+59Q03WzHepT22KCeYszvKZOByxvRd6MMpGQxCniiYn4Rb34jXt0m/3OsALYqJboqeQjsPcCtNarfK+keSddJekepgSUB3CBpfn7MSrV6tnVZDqX7f8pmb0eAtohYkqefAtpqtGml7flZ0lljLUX7RaMdm4fKLuxmOK9VtuMHgKcj4qFu6pu9HWsaSMlinSFpM+Aq4CsRsbKq+k7SkMpuwI+Bq8uOD3h/RIwnPQX4GEkfbEIMhfIHOA8CflGjuhW24xoijUG07L3skr4BrAIu66ZJM/eL84G3Au8GlpCGeVrVYfR8VtGS/18DKVnU88iQ1W0kDQa2BJ4rJbpM0oakRHFZRPyyuj4iVkbEi3l6NrChpJFlxhgRi/PPpcCvSKf3lVrl8SwHAHdGxNPVFa2wHbOnu4bo8s+lNdo0fXtKOgL4KPCpnNTepI79omEi4umI+HtEvA78rJu+W2E7DgYOAWZ216aZ27EnAylZ1PPIkFlA150mnwBu6u4foxHyWOYFwAMRcXY3bd7SdR1F0p6kv2FpCU3SppI275omXfxcWNVsFvCZfFfU3sCKiqGWMnV7BNfs7Vihcp+bAlxTo831wH6Shufhlf1yWSkkTQROAg6KiJe7aVPPftHIGCuvif1TN323wmODPgw8GBGLalU2ezv2qNlX2Mt8ke7S+TPpjohv5LLvkP4JADYmDVl0ArcBO5Yc3/tJwxALgLvz60DgaODo3OZY4D7SnRzzgP9ecow75r7vyXF0bcfKGEX6sqqHgXuB9ib8rTclvflvWVHW1O1ISlxLgL+RxsuPIl0TuxF4CPgtMCK3bQd+XrHsZ/N+2QkcWXKMnaSx/q59suuOwe2A2T3tFyXGeEne1xaQEsC21THm+Te9B5QVYy6/qGsfrGjblO3Y25cf92FmZoUG0jCUmZn1kZOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZitBUkv9qLt6ZJObNT6zRrJycLMzAo5WZj1M0kfU/o+lLsk/VZS5cMBd5P0x/y9FJ+vWOarkm7PD8L7dhPCNuuRk4VZ//s9sHdEvIf0GOyTKureBXwIeC9wqqTtJO0HjCM9A+jdwO6t8vA4sy6Dmx2A2XpoNDAzP69oI+DRirprIhh9raMAAADPSURBVOIV4BVJN5MSxPtJzwC6K7fZjJQ8bikvZLOeOVmY9b8fA2dHxCxJHaRvcetS/XydID1L63sR8dNywjPrPQ9DmfW/LXnj0dfV35c9SdLGkrYifbXr7aQnyH42f48JkkZJ2qasYM3q4TMLs7UzVFLl46bPJp1J/ELScuAmYIeK+gXAzcBI4IyIeBJ4UtLbgT/mp6a/CBxO7e+2MGsKP3XWzMwKeRjKzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQv8fcAyFncUIKgcAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}],"source":["# Network characterist options:\n","#  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n","#   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n","### Centralize file paths; update them all from here; pass them along through func calls.\n","###   Hunting for things to update is tedious.\n","residual = True  # Subtract measured from base_case; 0 if normal behavior, else non-zero.\n","norm_base = False\n","norm_feats = True   \n","mask = False\n","net_char = 0\n","tmstps = [80, 81, 82]\n","# tmstps = [78, 79, 80, 81, 82, 83, 84, 85]\n","entrop_decay = -1/1\n","cat_attrs = True\n","subsample = False\n","if cat_attrs :\n","  net_char = [0, 4]\n","# Update filepaths() when changing meta parameters outside this notebook (e.g. number of regions has changed)\n","(partit_cnt, regdict_filenm, ts_path, dset_size, pToPIdx_filenm,\n","    sensgrid_filenm, loss_filenm, conf_filenm, conf_mat_filenm) = filepaths()\n","\n","# print(regdict_filenm, ts_path, dset_size, pToPIdx_filenm,\n","#     sensgrid_filenm, loss_filenm, conf_filenm, conf_mat_filenm)\n","# assert False\n","\n","# Training data is loaded from csv file constructed in SimData_to_cvs script.\n","# For single tmstp:\n","# X, y, pipIdx = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","# For mult tmstp (requires list of tmstps):\n","X, y, pipIdx = cat_data_mult_tmstps(residual, norm_base, norm_feats, mask, net_char, dset_size, ts_path, tmstps)\n","# print(y)\n","# print(pipIdx)\n","print(f'X size: {X.size()}')\n","# torch.set_printoptions(edgeitems=50)\n","# print(f'Training Set: X[0] {X[0]}')\n","\n","if subsample :\n","  perc_of_meas = 0.01\n","  size = int(X.size(1) * perc_of_meas)\n","  print('subsample: size ', size)\n","  seed = 1001\n","  reduced_meas_X, __ = rand_sub_dataset(X, size, seed)\n","  print('subsample:', reduced_meas_X.size())\n","  # print(X[0, 307], X[0, 536], X[0, 329])\n","  # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n","  X = reduced_meas_X\n","\n","torch.set_printoptions(edgeitems=3)\n","tr_size = int(len(X)*0.7)\n","ts_size = len(y) - tr_size\n","# tr_dataset = TensorDataset(X[:split_idx], y[:split_idx])\n","# ts_dataset = TensorDataset(X[split_idx:], y[split_idx:])\n","# Find a seed that creates a training set pipe distribution with smallest stdev.\n","seed = 69   # find_seed prints and returns best seed.\n","# seed = find_seed(X, y, tr_size, 0)   # Commented out to eliminate the overhead of looking for a seed everytime.\n","# assert False\n","# Two ways to deal w/ pipe labels\n","#  1. add a third element to the datasets (done here)\n","#  2. make y a tuple and decompose in training/test loop\n","tr_dataset, ts_dataset = torch.utils.data.random_split(TensorDataset(X, y, pipIdx),\n","                                        [tr_size, ts_size],\n","                                        generator=torch.Generator().manual_seed(seed),\n","                                        )\n","\n","# Visualize the dataset leak pipe distribution.\n","# X, y = randomize_dataset(X, y)\n","# histo_pipe_dist(y[:split_idx])\n","# histo_pipe_dist(y[split_idx:])\n","idx = tr_dataset.indices\n","histo_pipe_dist(tr_dataset.dataset.tensors[1][[i for i in idx]])\n","\n","# Determine learning rate and measurement vector size\n","if net_char == 0:\n","  # link_flowrate\n","  cols = 444   # links = 444; junctions = 396\n","  # learning_rate = 9e-3   # single layer\n","  learning_rate = 8e-2\n","if net_char == 2:\n","  # link_velocity\n","  cols = 444   # links = 444; junctions = 396\n","  learning_rate = 2e-7\n","elif net_char == 3:\n","  # node_demand\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 2e-6\n","  epochs = 2000\n","elif net_char == 4:\n","  # node_head\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 7e-2\n","  epochs = 2000\n","elif net_char == 5:\n","  # node_pressure\n","  cols = 396   # links = 444; junctions = 396\n","  learning_rate = 1e-3\n","elif isinstance(net_char, list) :\n","  if norm_feats and cat_attrs :\n","    learning_rate = 2e-2\n","  else :\n","    learning_rate = 8e-2   #(f, h, nf, nh, f+h)\n","\n","# Determine number of concatenated vectors. Used for determining input_dim\n","concats = 1\n","if mask :\n","  concats = 1\n","\n","elif norm_feats :\n","  pass\n","# Simplification adjustments\n","cols = X.size(1)\n","print(f'cols {cols}')\n","\n","# Use region_dict to assign output_dim (i.e. # of regions)\n","rows = 1\n","input_dim = rows*cols*concats\n","output_dim = partit_cnt\n","epochs = 10000\n","# epochs = 50   # For testing.\n","batch_size = 128\n","mod = 5\n","\n","# Instantiate model framework\n","model = Autoencoder(input_dim, output_dim).to(device)\n","print(model)\n","# print(*model.parameters())\n","# print(f\"mask_params {model.state_dict()['encoder.mask_params'].size()}\")\n","# assert False"]},{"cell_type":"markdown","metadata":{"id":"cQdyRZRYXiQJ"},"source":["#####Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LItPDTO9LCoY","outputId":"b1631e18-8574-4dea-d836-2db97c9825c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Test Error: \n"," Accuracy: 57.2%, Avg loss: 1.255238 \n","\n","Epoch 6081\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.6%, Avg loss: 1.084650 \n","\n","Test Error: \n"," Accuracy: 55.9%, Avg loss: 1.304765 \n","\n","Epoch 6082\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.3%, Avg loss: 1.100023 \n","\n","Test Error: \n"," Accuracy: 56.3%, Avg loss: 1.209516 \n","\n","Epoch 6083\n","-------------------------------\n","Training Error: \n"," Accuracy: 58.6%, Avg loss: 1.108187 \n","\n","Test Error: \n"," Accuracy: 55.6%, Avg loss: 1.214078 \n","\n","Epoch 6084\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.4%, Avg loss: 1.092147 \n","\n","Test Error: \n"," Accuracy: 57.0%, Avg loss: 1.178058 \n","\n","Epoch 6085\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.3%, Avg loss: 1.068509 \n","\n","Test Error: \n"," Accuracy: 57.0%, Avg loss: 1.173414 \n","\n","Epoch 6086\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.3%, Avg loss: 1.070742 \n","\n","Test Error: \n"," Accuracy: 56.2%, Avg loss: 1.142392 \n","\n","Epoch 6087\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.1%, Avg loss: 1.067724 \n","\n","Test Error: \n"," Accuracy: 57.0%, Avg loss: 1.258867 \n","\n","Epoch 6088\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.8%, Avg loss: 1.081069 \n","\n","Test Error: \n"," Accuracy: 55.4%, Avg loss: 1.201156 \n","\n","Epoch 6089\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.2%, Avg loss: 1.070904 \n","\n","Test Error: \n"," Accuracy: 57.4%, Avg loss: 1.173611 \n","\n","Epoch 6090\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.9%, Avg loss: 1.071317 \n","\n","Test Error: \n"," Accuracy: 55.5%, Avg loss: 1.191885 \n","\n","Epoch 6091\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.8%, Avg loss: 1.163713 \n","\n","Test Error: \n"," Accuracy: 57.3%, Avg loss: 1.161470 \n","\n","Epoch 6092\n","-------------------------------\n","Training Error: \n"," Accuracy: 59.8%, Avg loss: 1.075353 \n","\n","Test Error: \n"," Accuracy: 57.3%, Avg loss: 1.167927 \n","\n","Epoch 6093\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.1%, Avg loss: 1.045611 \n","\n","Test Error: \n"," Accuracy: 56.2%, Avg loss: 1.218359 \n","\n","Epoch 6094\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.6%, Avg loss: 1.042275 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.124681 \n","\n","Epoch 6095\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.3%, Avg loss: 1.066431 \n","\n","Test Error: \n"," Accuracy: 57.1%, Avg loss: 1.201077 \n","\n","Epoch 6096\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.7%, Avg loss: 1.049873 \n","\n","Test Error: \n"," Accuracy: 57.1%, Avg loss: 1.155220 \n","\n","Epoch 6097\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 1.002531 \n","\n","Test Error: \n"," Accuracy: 58.5%, Avg loss: 1.105674 \n","\n","Epoch 6098\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.8%, Avg loss: 1.013851 \n","\n","Test Error: \n"," Accuracy: 57.6%, Avg loss: 1.131192 \n","\n","Epoch 6099\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.9%, Avg loss: 1.015881 \n","\n","Test Error: \n"," Accuracy: 57.9%, Avg loss: 1.143078 \n","\n","Epoch 6100\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.4%, Avg loss: 1.006166 \n","\n","Test Error: \n"," Accuracy: 57.6%, Avg loss: 1.180857 \n","\n","Epoch 6101\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.9%, Avg loss: 1.020331 \n","\n","Test Error: \n"," Accuracy: 56.9%, Avg loss: 1.128052 \n","\n","Epoch 6102\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 1.013411 \n","\n","Test Error: \n"," Accuracy: 57.4%, Avg loss: 1.162315 \n","\n","Epoch 6103\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.5%, Avg loss: 1.031429 \n","\n","Test Error: \n"," Accuracy: 57.5%, Avg loss: 1.143487 \n","\n","Epoch 6104\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 1.008739 \n","\n","Test Error: \n"," Accuracy: 55.9%, Avg loss: 1.144370 \n","\n","Epoch 6105\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.0%, Avg loss: 0.993021 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.110169 \n","\n","Epoch 6106\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 0.983523 \n","\n","Test Error: \n"," Accuracy: 57.1%, Avg loss: 1.144608 \n","\n","Epoch 6107\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 1.005210 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.085994 \n","\n","Epoch 6108\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.9%, Avg loss: 1.017725 \n","\n","Test Error: \n"," Accuracy: 58.4%, Avg loss: 1.113196 \n","\n","Epoch 6109\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.993052 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.088765 \n","\n","Epoch 6110\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.986199 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.064902 \n","\n","Epoch 6111\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 0.965092 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.109498 \n","\n","Epoch 6112\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.960376 \n","\n","Test Error: \n"," Accuracy: 57.6%, Avg loss: 1.115564 \n","\n","Epoch 6113\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.5%, Avg loss: 0.998024 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.068698 \n","\n","Epoch 6114\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.6%, Avg loss: 0.984377 \n","\n","Test Error: \n"," Accuracy: 58.0%, Avg loss: 1.100668 \n","\n","Epoch 6115\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 1.006941 \n","\n","Test Error: \n"," Accuracy: 57.5%, Avg loss: 1.138473 \n","\n","Epoch 6116\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.6%, Avg loss: 1.007533 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.093865 \n","\n","Epoch 6117\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.5%, Avg loss: 1.011845 \n","\n","Test Error: \n"," Accuracy: 57.7%, Avg loss: 1.135540 \n","\n","Epoch 6118\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.6%, Avg loss: 1.015464 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.110921 \n","\n","Epoch 6119\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.7%, Avg loss: 0.973371 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.100633 \n","\n","Epoch 6120\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.954211 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.056873 \n","\n","Epoch 6121\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.8%, Avg loss: 0.980343 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.073587 \n","\n","Epoch 6122\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.954515 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.101322 \n","\n","Epoch 6123\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.993225 \n","\n","Test Error: \n"," Accuracy: 57.8%, Avg loss: 1.102786 \n","\n","Epoch 6124\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.962192 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.050429 \n","\n","Epoch 6125\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.936890 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.109963 \n","\n","Epoch 6126\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.978497 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.121092 \n","\n","Epoch 6127\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.963996 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.091794 \n","\n","Epoch 6128\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 0.962407 \n","\n","Test Error: \n"," Accuracy: 58.0%, Avg loss: 1.084925 \n","\n","Epoch 6129\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.952977 \n","\n","Test Error: \n"," Accuracy: 58.5%, Avg loss: 1.139382 \n","\n","Epoch 6130\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.945848 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.079651 \n","\n","Epoch 6131\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.958086 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.083493 \n","\n","Epoch 6132\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.989892 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.075573 \n","\n","Epoch 6133\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 1.008535 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.108914 \n","\n","Epoch 6134\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.955617 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.114995 \n","\n","Epoch 6135\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.959144 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.030382 \n","\n","Epoch 6136\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 0.996263 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.070665 \n","\n","Epoch 6137\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.975518 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.125304 \n","\n","Epoch 6138\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.969674 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.087783 \n","\n","Epoch 6139\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.952098 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.035952 \n","\n","Epoch 6140\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.975175 \n","\n","Test Error: \n"," Accuracy: 57.8%, Avg loss: 1.122255 \n","\n","Epoch 6141\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.954100 \n","\n","Test Error: \n"," Accuracy: 58.5%, Avg loss: 1.131596 \n","\n","Epoch 6142\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 1.052439 \n","\n","Test Error: \n"," Accuracy: 57.8%, Avg loss: 1.125319 \n","\n","Epoch 6143\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 1.002391 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.087036 \n","\n","Epoch 6144\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.966114 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.040799 \n","\n","Epoch 6145\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.993039 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.067143 \n","\n","Epoch 6146\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 0.977860 \n","\n","Test Error: \n"," Accuracy: 57.8%, Avg loss: 1.063468 \n","\n","Epoch 6147\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.953978 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.083845 \n","\n","Epoch 6148\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.962023 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.055463 \n","\n","Epoch 6149\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.948513 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.064235 \n","\n","Epoch 6150\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.926194 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.069760 \n","\n","Epoch 6151\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.928751 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.034819 \n","\n","Epoch 6152\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.963523 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.083727 \n","\n","Epoch 6153\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.926963 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.025840 \n","\n","Epoch 6154\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.950805 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.014055 \n","\n","Epoch 6155\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.935345 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.047189 \n","\n","Epoch 6156\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.929795 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.055310 \n","\n","Epoch 6157\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.922011 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.050317 \n","\n","Epoch 6158\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.952891 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.072152 \n","\n","Epoch 6159\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.955495 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.067353 \n","\n","Epoch 6160\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.906959 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.048642 \n","\n","Epoch 6161\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.967207 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.086199 \n","\n","Epoch 6162\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.924858 \n","\n","Test Error: \n"," Accuracy: 58.1%, Avg loss: 1.061030 \n","\n","Epoch 6163\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.933850 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.088910 \n","\n","Epoch 6164\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.942006 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.029391 \n","\n","Epoch 6165\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.932986 \n","\n","Test Error: \n"," Accuracy: 60.9%, Avg loss: 1.036202 \n","\n","Epoch 6166\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.925053 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.035220 \n","\n","Epoch 6167\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.895632 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.060516 \n","\n","Epoch 6168\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.901347 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.007422 \n","\n","Epoch 6169\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.921974 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.075551 \n","\n","Epoch 6170\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.913720 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.051435 \n","\n","Epoch 6171\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.972149 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.038492 \n","\n","Epoch 6172\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.930192 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.075937 \n","\n","Epoch 6173\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.955175 \n","\n","Test Error: \n"," Accuracy: 58.2%, Avg loss: 1.095158 \n","\n","Epoch 6174\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.968001 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.061845 \n","\n","Epoch 6175\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.969168 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.103373 \n","\n","Epoch 6176\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.928968 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.074579 \n","\n","Epoch 6177\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.950726 \n","\n","Test Error: \n"," Accuracy: 58.2%, Avg loss: 1.081481 \n","\n","Epoch 6178\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.958901 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.080949 \n","\n","Epoch 6179\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.972330 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.074911 \n","\n","Epoch 6180\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.960254 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.083905 \n","\n","Epoch 6181\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.965005 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.063337 \n","\n","Epoch 6182\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.975972 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.123541 \n","\n","Epoch 6183\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.951452 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.065383 \n","\n","Epoch 6184\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 0.997451 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.133896 \n","\n","Epoch 6185\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.993073 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.067065 \n","\n","Epoch 6186\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.939181 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.081499 \n","\n","Epoch 6187\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.961968 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.082010 \n","\n","Epoch 6188\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.951154 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.054818 \n","\n","Epoch 6189\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 1.003032 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.097844 \n","\n","Epoch 6190\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.4%, Avg loss: 0.999851 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.131344 \n","\n","Epoch 6191\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.987448 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.076008 \n","\n","Epoch 6192\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 0.999150 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.144965 \n","\n","Epoch 6193\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.984185 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.041762 \n","\n","Epoch 6194\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.982448 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.113135 \n","\n","Epoch 6195\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.957950 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.040857 \n","\n","Epoch 6196\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.944635 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.121965 \n","\n","Epoch 6197\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 1.001075 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.078436 \n","\n","Epoch 6198\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.972130 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.097096 \n","\n","Epoch 6199\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.952912 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.100590 \n","\n","Epoch 6200\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.948303 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.076093 \n","\n","Epoch 6201\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.938773 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.095983 \n","\n","Epoch 6202\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.974867 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.077001 \n","\n","Epoch 6203\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 0.977506 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.076397 \n","\n","Epoch 6204\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.952697 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.076478 \n","\n","Epoch 6205\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.943623 \n","\n","Test Error: \n"," Accuracy: 58.1%, Avg loss: 1.081225 \n","\n","Epoch 6206\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.939560 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.088509 \n","\n","Epoch 6207\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.923680 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.077292 \n","\n","Epoch 6208\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.962694 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.104828 \n","\n","Epoch 6209\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.978865 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.100384 \n","\n","Epoch 6210\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.945508 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.026618 \n","\n","Epoch 6211\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.921137 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.069636 \n","\n","Epoch 6212\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.950905 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.049278 \n","\n","Epoch 6213\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.914330 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.072506 \n","\n","Epoch 6214\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.918848 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.094490 \n","\n","Epoch 6215\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.944564 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.030210 \n","\n","Epoch 6216\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.935717 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.092196 \n","\n","Epoch 6217\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.968080 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.056420 \n","\n","Epoch 6218\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.910802 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.020533 \n","\n","Epoch 6219\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.888537 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.026114 \n","\n","Epoch 6220\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.915350 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 0.994422 \n","\n","Epoch 6221\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.900467 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.037606 \n","\n","Epoch 6222\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.933495 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.057114 \n","\n","Epoch 6223\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.931745 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.029701 \n","\n","Epoch 6224\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.941638 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.126168 \n","\n","Epoch 6225\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.921512 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.040074 \n","\n","Epoch 6226\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.904267 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.054356 \n","\n","Epoch 6227\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.924846 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.073335 \n","\n","Epoch 6228\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.946631 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.030035 \n","\n","Epoch 6229\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.933485 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.101121 \n","\n","Epoch 6230\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.931987 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.097415 \n","\n","Epoch 6231\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.927810 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.053075 \n","\n","Epoch 6232\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.917420 \n","\n","Test Error: \n"," Accuracy: 61.5%, Avg loss: 1.041462 \n","\n","Epoch 6233\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.906670 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.042674 \n","\n","Epoch 6234\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.913915 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.044441 \n","\n","Epoch 6235\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.931830 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.049920 \n","\n","Epoch 6236\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.935690 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.068030 \n","\n","Epoch 6237\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.948838 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.098855 \n","\n","Epoch 6238\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 1.025443 \n","\n","Test Error: \n"," Accuracy: 58.1%, Avg loss: 1.154794 \n","\n","Epoch 6239\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.4%, Avg loss: 1.021747 \n","\n","Test Error: \n"," Accuracy: 57.5%, Avg loss: 1.170364 \n","\n","Epoch 6240\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.7%, Avg loss: 1.024010 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.090437 \n","\n","Epoch 6241\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.8%, Avg loss: 0.991230 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.093580 \n","\n","Epoch 6242\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.996519 \n","\n","Test Error: \n"," Accuracy: 58.0%, Avg loss: 1.078459 \n","\n","Epoch 6243\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.948933 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.087381 \n","\n","Epoch 6244\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.947877 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.097153 \n","\n","Epoch 6245\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.954434 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.084950 \n","\n","Epoch 6246\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.970536 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.063030 \n","\n","Epoch 6247\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.933777 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.083153 \n","\n","Epoch 6248\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.909472 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.061864 \n","\n","Epoch 6249\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.944314 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.095452 \n","\n","Epoch 6250\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.941063 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.111940 \n","\n","Epoch 6251\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.948642 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.097278 \n","\n","Epoch 6252\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.959750 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.109910 \n","\n","Epoch 6253\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.8%, Avg loss: 0.977999 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.153410 \n","\n","Epoch 6254\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.961916 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.086102 \n","\n","Epoch 6255\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.942697 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.080749 \n","\n","Epoch 6256\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.918457 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.084439 \n","\n","Epoch 6257\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.931722 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.017034 \n","\n","Epoch 6258\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.938647 \n","\n","Test Error: \n"," Accuracy: 58.4%, Avg loss: 1.086617 \n","\n","Epoch 6259\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.970308 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.023501 \n","\n","Epoch 6260\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.918664 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.031441 \n","\n","Epoch 6261\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.902819 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 0.996435 \n","\n","Epoch 6262\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.880709 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.000445 \n","\n","Epoch 6263\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.884334 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.021507 \n","\n","Epoch 6264\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.899354 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.001292 \n","\n","Epoch 6265\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.876847 \n","\n","Test Error: \n"," Accuracy: 61.4%, Avg loss: 0.990169 \n","\n","Epoch 6266\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.876684 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.023115 \n","\n","Epoch 6267\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.888191 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.027933 \n","\n","Epoch 6268\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.894039 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.068361 \n","\n","Epoch 6269\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.914219 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.034589 \n","\n","Epoch 6270\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.7%, Avg loss: 0.903046 \n","\n","Test Error: \n"," Accuracy: 61.5%, Avg loss: 0.992706 \n","\n","Epoch 6271\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.871658 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 0.991201 \n","\n","Epoch 6272\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.878931 \n","\n","Test Error: \n"," Accuracy: 61.8%, Avg loss: 1.006223 \n","\n","Epoch 6273\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.892723 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.034858 \n","\n","Epoch 6274\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.881636 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 1.028473 \n","\n","Epoch 6275\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.880731 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 1.049022 \n","\n","Epoch 6276\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.910632 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.013927 \n","\n","Epoch 6277\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.945068 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.042837 \n","\n","Epoch 6278\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.876843 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.040955 \n","\n","Epoch 6279\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.868926 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.042221 \n","\n","Epoch 6280\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.7%, Avg loss: 0.890727 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 0.986665 \n","\n","Epoch 6281\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.880411 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 0.990626 \n","\n","Epoch 6282\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.886797 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.048548 \n","\n","Epoch 6283\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.907056 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.018566 \n","\n","Epoch 6284\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.899535 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.010406 \n","\n","Epoch 6285\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.899750 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.051264 \n","\n","Epoch 6286\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.916048 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.042472 \n","\n","Epoch 6287\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.926644 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.045223 \n","\n","Epoch 6288\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.897122 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.049469 \n","\n","Epoch 6289\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.883564 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 1.067796 \n","\n","Epoch 6290\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.883731 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.043379 \n","\n","Epoch 6291\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.908727 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.050375 \n","\n","Epoch 6292\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.872477 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 0.987122 \n","\n","Epoch 6293\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.3%, Avg loss: 0.883022 \n","\n","Test Error: \n"," Accuracy: 61.5%, Avg loss: 1.005945 \n","\n","Epoch 6294\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.1%, Avg loss: 0.875444 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 0.997572 \n","\n","Epoch 6295\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.876472 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.044304 \n","\n","Epoch 6296\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.951376 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.048787 \n","\n","Epoch 6297\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.904352 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.006980 \n","\n","Epoch 6298\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.904260 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.018373 \n","\n","Epoch 6299\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.894993 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.001343 \n","\n","Epoch 6300\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.881265 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 0.990252 \n","\n","Epoch 6301\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.901499 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.055286 \n","\n","Epoch 6302\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.889422 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 0.994996 \n","\n","Epoch 6303\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.883777 \n","\n","Test Error: \n"," Accuracy: 61.1%, Avg loss: 1.012529 \n","\n","Epoch 6304\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.0%, Avg loss: 0.870412 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.006684 \n","\n","Epoch 6305\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.7%, Avg loss: 0.873768 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.004470 \n","\n","Epoch 6306\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.7%, Avg loss: 0.866356 \n","\n","Test Error: \n"," Accuracy: 62.2%, Avg loss: 0.963754 \n","\n","Epoch 6307\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.3%, Avg loss: 0.866433 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 0.990553 \n","\n","Epoch 6308\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.8%, Avg loss: 0.855359 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 0.974911 \n","\n","Epoch 6309\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.2%, Avg loss: 0.850277 \n","\n","Test Error: \n"," Accuracy: 61.8%, Avg loss: 0.991939 \n","\n","Epoch 6310\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.2%, Avg loss: 0.866124 \n","\n","Test Error: \n"," Accuracy: 61.6%, Avg loss: 0.979990 \n","\n","Epoch 6311\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.7%, Avg loss: 0.838776 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 0.991910 \n","\n","Epoch 6312\n","-------------------------------\n","Training Error: \n"," Accuracy: 65.1%, Avg loss: 0.864006 \n","\n","Test Error: \n"," Accuracy: 61.3%, Avg loss: 0.995258 \n","\n","Epoch 6313\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.8%, Avg loss: 0.876817 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.007357 \n","\n","Epoch 6314\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.893035 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.017267 \n","\n","Epoch 6315\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.897262 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.022144 \n","\n","Epoch 6316\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.921488 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.066728 \n","\n","Epoch 6317\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.909651 \n","\n","Test Error: \n"," Accuracy: 61.1%, Avg loss: 1.035509 \n","\n","Epoch 6318\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.899700 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.014962 \n","\n","Epoch 6319\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.892800 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.014167 \n","\n","Epoch 6320\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.908425 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.051924 \n","\n","Epoch 6321\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.915366 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.034013 \n","\n","Epoch 6322\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.913124 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.033591 \n","\n","Epoch 6323\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.918252 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.041534 \n","\n","Epoch 6324\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.925628 \n","\n","Test Error: \n"," Accuracy: 57.1%, Avg loss: 1.090152 \n","\n","Epoch 6325\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.926347 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.114051 \n","\n","Epoch 6326\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.883369 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.033439 \n","\n","Epoch 6327\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.906811 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.024134 \n","\n","Epoch 6328\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.893656 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.096218 \n","\n","Epoch 6329\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.921648 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.057695 \n","\n","Epoch 6330\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.961848 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.027518 \n","\n","Epoch 6331\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.920303 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.048999 \n","\n","Epoch 6332\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.914888 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.037615 \n","\n","Epoch 6333\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.898787 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 0.993539 \n","\n","Epoch 6334\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.904065 \n","\n","Test Error: \n"," Accuracy: 61.6%, Avg loss: 1.022341 \n","\n","Epoch 6335\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.896917 \n","\n","Test Error: \n"," Accuracy: 61.6%, Avg loss: 1.016119 \n","\n","Epoch 6336\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.956888 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.053217 \n","\n","Epoch 6337\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.939054 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.094312 \n","\n","Epoch 6338\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.945212 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.051352 \n","\n","Epoch 6339\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.916558 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.072126 \n","\n","Epoch 6340\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.924624 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.080617 \n","\n","Epoch 6341\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.932658 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.068125 \n","\n","Epoch 6342\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.950311 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.036720 \n","\n","Epoch 6343\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.927018 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.057599 \n","\n","Epoch 6344\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.923702 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.042310 \n","\n","Epoch 6345\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.933552 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.041309 \n","\n","Epoch 6346\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.918559 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.024296 \n","\n","Epoch 6347\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.918706 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.044918 \n","\n","Epoch 6348\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.935085 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.069988 \n","\n","Epoch 6349\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.951037 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.071417 \n","\n","Epoch 6350\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.951265 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.032616 \n","\n","Epoch 6351\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.921026 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.044714 \n","\n","Epoch 6352\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.939934 \n","\n","Test Error: \n"," Accuracy: 58.4%, Avg loss: 1.074234 \n","\n","Epoch 6353\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.949436 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.080175 \n","\n","Epoch 6354\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.937212 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.038190 \n","\n","Epoch 6355\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.948766 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.062348 \n","\n","Epoch 6356\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.919844 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.025087 \n","\n","Epoch 6357\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.918918 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.051404 \n","\n","Epoch 6358\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.923562 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.026380 \n","\n","Epoch 6359\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.927700 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.071019 \n","\n","Epoch 6360\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.940947 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.062982 \n","\n","Epoch 6361\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.923848 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.062632 \n","\n","Epoch 6362\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.924503 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.015457 \n","\n","Epoch 6363\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.922863 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.034667 \n","\n","Epoch 6364\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.934748 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.094816 \n","\n","Epoch 6365\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.948214 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.046762 \n","\n","Epoch 6366\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.943983 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.054277 \n","\n","Epoch 6367\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.929960 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.063841 \n","\n","Epoch 6368\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.2%, Avg loss: 1.094800 \n","\n","Test Error: \n"," Accuracy: 58.4%, Avg loss: 1.054304 \n","\n","Epoch 6369\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.929923 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.031908 \n","\n","Epoch 6370\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.921595 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.044248 \n","\n","Epoch 6371\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.949327 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.093992 \n","\n","Epoch 6372\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.953287 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.026248 \n","\n","Epoch 6373\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.934483 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.079271 \n","\n","Epoch 6374\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.929633 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.121628 \n","\n","Epoch 6375\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.944082 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.081399 \n","\n","Epoch 6376\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.905327 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.030672 \n","\n","Epoch 6377\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.928982 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.043076 \n","\n","Epoch 6378\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.981549 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.046137 \n","\n","Epoch 6379\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.927798 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.032534 \n","\n","Epoch 6380\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.907313 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.017226 \n","\n","Epoch 6381\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.892783 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.040380 \n","\n","Epoch 6382\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.902765 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.071608 \n","\n","Epoch 6383\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.912826 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.022921 \n","\n","Epoch 6384\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.925811 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.086424 \n","\n","Epoch 6385\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.922594 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.105742 \n","\n","Epoch 6386\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.920710 \n","\n","Test Error: \n"," Accuracy: 61.1%, Avg loss: 1.035004 \n","\n","Epoch 6387\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.904985 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.044800 \n","\n","Epoch 6388\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.926297 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.077974 \n","\n","Epoch 6389\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.927371 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.051526 \n","\n","Epoch 6390\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.887096 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.024239 \n","\n","Epoch 6391\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.885636 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.025686 \n","\n","Epoch 6392\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.883815 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 0.996008 \n","\n","Epoch 6393\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.907057 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.040781 \n","\n","Epoch 6394\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.890869 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.082063 \n","\n","Epoch 6395\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.931295 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.046370 \n","\n","Epoch 6396\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.890877 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.031176 \n","\n","Epoch 6397\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.919865 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.054445 \n","\n","Epoch 6398\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.898957 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.050286 \n","\n","Epoch 6399\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.913504 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.018448 \n","\n","Epoch 6400\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.916891 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.063529 \n","\n","Epoch 6401\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.899078 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.016261 \n","\n","Epoch 6402\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.892727 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.067676 \n","\n","Epoch 6403\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 0.981837 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.043466 \n","\n","Epoch 6404\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.942379 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.045136 \n","\n","Epoch 6405\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.939892 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.045436 \n","\n","Epoch 6406\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.907882 \n","\n","Test Error: \n"," Accuracy: 60.9%, Avg loss: 1.019959 \n","\n","Epoch 6407\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.897674 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.055484 \n","\n","Epoch 6408\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.880590 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.026377 \n","\n","Epoch 6409\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.895083 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.008183 \n","\n","Epoch 6410\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.914745 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.048986 \n","\n","Epoch 6411\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.906734 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.045029 \n","\n","Epoch 6412\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.907235 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.073907 \n","\n","Epoch 6413\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.928899 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.060922 \n","\n","Epoch 6414\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.926975 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.060169 \n","\n","Epoch 6415\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.900229 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.036697 \n","\n","Epoch 6416\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.911483 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.000248 \n","\n","Epoch 6417\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.884394 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.034593 \n","\n","Epoch 6418\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.895145 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.075071 \n","\n","Epoch 6419\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.927791 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.039074 \n","\n","Epoch 6420\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.935395 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.039657 \n","\n","Epoch 6421\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.935904 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.066361 \n","\n","Epoch 6422\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.924568 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.025894 \n","\n","Epoch 6423\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.937282 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.056329 \n","\n","Epoch 6424\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.936862 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.050458 \n","\n","Epoch 6425\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.956708 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.085909 \n","\n","Epoch 6426\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.932746 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.075902 \n","\n","Epoch 6427\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.929532 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.047442 \n","\n","Epoch 6428\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.946314 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.061970 \n","\n","Epoch 6429\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.928030 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.047989 \n","\n","Epoch 6430\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.902962 \n","\n","Test Error: \n"," Accuracy: 60.9%, Avg loss: 1.055765 \n","\n","Epoch 6431\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.917265 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.048953 \n","\n","Epoch 6432\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.935138 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.025010 \n","\n","Epoch 6433\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.919508 \n","\n","Test Error: \n"," Accuracy: 60.9%, Avg loss: 1.053283 \n","\n","Epoch 6434\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.890542 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.024791 \n","\n","Epoch 6435\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.933903 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.074961 \n","\n","Epoch 6436\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.915379 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.060269 \n","\n","Epoch 6437\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.923062 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.071440 \n","\n","Epoch 6438\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.911371 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.046607 \n","\n","Epoch 6439\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.913261 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.068993 \n","\n","Epoch 6440\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.904484 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.038871 \n","\n","Epoch 6441\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.937597 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.052397 \n","\n","Epoch 6442\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.941275 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.041088 \n","\n","Epoch 6443\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.906953 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.079883 \n","\n","Epoch 6444\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.974835 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.036627 \n","\n","Epoch 6445\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.923258 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.053974 \n","\n","Epoch 6446\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.917316 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.051115 \n","\n","Epoch 6447\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.905139 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.040059 \n","\n","Epoch 6448\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.949701 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.049767 \n","\n","Epoch 6449\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.929180 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.077410 \n","\n","Epoch 6450\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.947035 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.089328 \n","\n","Epoch 6451\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.978921 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.153654 \n","\n","Epoch 6452\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.955662 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.049725 \n","\n","Epoch 6453\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.942493 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.084483 \n","\n","Epoch 6454\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.981062 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.046774 \n","\n","Epoch 6455\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.948711 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.094059 \n","\n","Epoch 6456\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.973678 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.128918 \n","\n","Epoch 6457\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.962279 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.066903 \n","\n","Epoch 6458\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.959379 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.072495 \n","\n","Epoch 6459\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.936403 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.050927 \n","\n","Epoch 6460\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.928220 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.047472 \n","\n","Epoch 6461\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.918942 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.026860 \n","\n","Epoch 6462\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.923969 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.042577 \n","\n","Epoch 6463\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.909495 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.024193 \n","\n","Epoch 6464\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.913051 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.083285 \n","\n","Epoch 6465\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.920659 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.021969 \n","\n","Epoch 6466\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.915891 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.071787 \n","\n","Epoch 6467\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.956130 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.032265 \n","\n","Epoch 6468\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.934602 \n","\n","Test Error: \n"," Accuracy: 61.6%, Avg loss: 1.013709 \n","\n","Epoch 6469\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.901329 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.038235 \n","\n","Epoch 6470\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.931258 \n","\n","Test Error: \n"," Accuracy: 61.6%, Avg loss: 0.979360 \n","\n","Epoch 6471\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.8%, Avg loss: 0.894822 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.034861 \n","\n","Epoch 6472\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.886717 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.054727 \n","\n","Epoch 6473\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.7%, Avg loss: 0.918764 \n","\n","Test Error: \n"," Accuracy: 61.4%, Avg loss: 1.005466 \n","\n","Epoch 6474\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.921281 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.019803 \n","\n","Epoch 6475\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.902573 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.008835 \n","\n","Epoch 6476\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.942574 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.063261 \n","\n","Epoch 6477\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.950535 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.082792 \n","\n","Epoch 6478\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.928933 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.120012 \n","\n","Epoch 6479\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.966489 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.058681 \n","\n","Epoch 6480\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.951105 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.112197 \n","\n","Epoch 6481\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.948497 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.109380 \n","\n","Epoch 6482\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.951669 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.032354 \n","\n","Epoch 6483\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.969876 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.082628 \n","\n","Epoch 6484\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.942185 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.094961 \n","\n","Epoch 6485\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.967985 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.047177 \n","\n","Epoch 6486\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.972213 \n","\n","Test Error: \n"," Accuracy: 58.5%, Avg loss: 1.081014 \n","\n","Epoch 6487\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.944418 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.078258 \n","\n","Epoch 6488\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.947421 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.077120 \n","\n","Epoch 6489\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 1.012874 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.086503 \n","\n","Epoch 6490\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.949182 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.037861 \n","\n","Epoch 6491\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.931146 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.032268 \n","\n","Epoch 6492\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.927868 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.041091 \n","\n","Epoch 6493\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.959014 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.054012 \n","\n","Epoch 6494\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.935676 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.082772 \n","\n","Epoch 6495\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.938509 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.043464 \n","\n","Epoch 6496\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.947360 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.072722 \n","\n","Epoch 6497\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.948360 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.048543 \n","\n","Epoch 6498\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.946583 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.090338 \n","\n","Epoch 6499\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.959703 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.052190 \n","\n","Epoch 6500\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.948382 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.028812 \n","\n","Epoch 6501\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.946414 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.044835 \n","\n","Epoch 6502\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.934354 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.023371 \n","\n","Epoch 6503\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 1.033749 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.026924 \n","\n","Epoch 6504\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.910284 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.055465 \n","\n","Epoch 6505\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.946172 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 1.000639 \n","\n","Epoch 6506\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.915220 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.011524 \n","\n","Epoch 6507\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.878012 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.090749 \n","\n","Epoch 6508\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.880214 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.043547 \n","\n","Epoch 6509\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.885591 \n","\n","Test Error: \n"," Accuracy: 61.6%, Avg loss: 0.997050 \n","\n","Epoch 6510\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.891381 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.015586 \n","\n","Epoch 6511\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.890605 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.001031 \n","\n","Epoch 6512\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.903495 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 1.011516 \n","\n","Epoch 6513\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.7%, Avg loss: 0.873007 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 0.998928 \n","\n","Epoch 6514\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.903357 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.062124 \n","\n","Epoch 6515\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.898331 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.000583 \n","\n","Epoch 6516\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.903770 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.004142 \n","\n","Epoch 6517\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.910738 \n","\n","Test Error: \n"," Accuracy: 61.5%, Avg loss: 1.009819 \n","\n","Epoch 6518\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.888779 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.020557 \n","\n","Epoch 6519\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.926172 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.004203 \n","\n","Epoch 6520\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.888465 \n","\n","Test Error: \n"," Accuracy: 61.3%, Avg loss: 1.031801 \n","\n","Epoch 6521\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.896380 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.001898 \n","\n","Epoch 6522\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.896609 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.003652 \n","\n","Epoch 6523\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.897278 \n","\n","Test Error: \n"," Accuracy: 61.1%, Avg loss: 1.031560 \n","\n","Epoch 6524\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.891287 \n","\n","Test Error: \n"," Accuracy: 61.4%, Avg loss: 1.068170 \n","\n","Epoch 6525\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.911328 \n","\n","Test Error: \n"," Accuracy: 61.3%, Avg loss: 1.042353 \n","\n","Epoch 6526\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.902544 \n","\n","Test Error: \n"," Accuracy: 62.1%, Avg loss: 0.990527 \n","\n","Epoch 6527\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.897993 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.026697 \n","\n","Epoch 6528\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.897638 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.020966 \n","\n","Epoch 6529\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.907763 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 0.974346 \n","\n","Epoch 6530\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.901872 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.022545 \n","\n","Epoch 6531\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.892314 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.025016 \n","\n","Epoch 6532\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.889430 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.015541 \n","\n","Epoch 6533\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.893671 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.038874 \n","\n","Epoch 6534\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.909288 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.037694 \n","\n","Epoch 6535\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.934395 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.038879 \n","\n","Epoch 6536\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.947952 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.016291 \n","\n","Epoch 6537\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.877208 \n","\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 1.003302 \n","\n","Epoch 6538\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.881765 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.049702 \n","\n","Epoch 6539\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.4%, Avg loss: 0.879351 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 0.996027 \n","\n","Epoch 6540\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.890787 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.059899 \n","\n","Epoch 6541\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.885116 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.022198 \n","\n","Epoch 6542\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.916600 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.028675 \n","\n","Epoch 6543\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.908554 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.006025 \n","\n","Epoch 6544\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.888758 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.013965 \n","\n","Epoch 6545\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.909276 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.032552 \n","\n","Epoch 6546\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.916403 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.063854 \n","\n","Epoch 6547\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.996789 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.051583 \n","\n","Epoch 6548\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.925628 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.041975 \n","\n","Epoch 6549\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.919442 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.036792 \n","\n","Epoch 6550\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.946002 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.373525 \n","\n","Epoch 6551\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.911922 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.026834 \n","\n","Epoch 6552\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.894248 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.070172 \n","\n","Epoch 6553\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.911621 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.036314 \n","\n","Epoch 6554\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.948959 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.066115 \n","\n","Epoch 6555\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.957202 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.058351 \n","\n","Epoch 6556\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.922533 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.027957 \n","\n","Epoch 6557\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.954771 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.046561 \n","\n","Epoch 6558\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.945770 \n","\n","Test Error: \n"," Accuracy: 58.2%, Avg loss: 1.140322 \n","\n","Epoch 6559\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 1.000200 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.087476 \n","\n","Epoch 6560\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.932828 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.052741 \n","\n","Epoch 6561\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.939847 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.075282 \n","\n","Epoch 6562\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.961180 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.053445 \n","\n","Epoch 6563\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.955602 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.043652 \n","\n","Epoch 6564\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.3%, Avg loss: 1.056156 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.163435 \n","\n","Epoch 6565\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.4%, Avg loss: 1.024315 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.066088 \n","\n","Epoch 6566\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.993506 \n","\n","Test Error: \n"," Accuracy: 57.8%, Avg loss: 1.112079 \n","\n","Epoch 6567\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.5%, Avg loss: 0.997396 \n","\n","Test Error: \n"," Accuracy: 58.5%, Avg loss: 1.068755 \n","\n","Epoch 6568\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.938620 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.097014 \n","\n","Epoch 6569\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.949084 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.053953 \n","\n","Epoch 6570\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.954112 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.113786 \n","\n","Epoch 6571\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.959378 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.105863 \n","\n","Epoch 6572\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.970537 \n","\n","Test Error: \n"," Accuracy: 58.5%, Avg loss: 1.079185 \n","\n","Epoch 6573\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.947999 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.032851 \n","\n","Epoch 6574\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.913988 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.028537 \n","\n","Epoch 6575\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.917476 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.035108 \n","\n","Epoch 6576\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.930266 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.058237 \n","\n","Epoch 6577\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.917437 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.059574 \n","\n","Epoch 6578\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.927184 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.025510 \n","\n","Epoch 6579\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.926751 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.026529 \n","\n","Epoch 6580\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.906596 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.030071 \n","\n","Epoch 6581\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.950334 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.055151 \n","\n","Epoch 6582\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.920360 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.025459 \n","\n","Epoch 6583\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.912353 \n","\n","Test Error: \n"," Accuracy: 61.3%, Avg loss: 1.001465 \n","\n","Epoch 6584\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.917067 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.048269 \n","\n","Epoch 6585\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.916686 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.023228 \n","\n","Epoch 6586\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.933131 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.058487 \n","\n","Epoch 6587\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.920938 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.086219 \n","\n","Epoch 6588\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.905070 \n","\n","Test Error: \n"," Accuracy: 58.0%, Avg loss: 1.066961 \n","\n","Epoch 6589\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.917704 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.063365 \n","\n","Epoch 6590\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.901590 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.035332 \n","\n","Epoch 6591\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.921600 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.032722 \n","\n","Epoch 6592\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.893063 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.033726 \n","\n","Epoch 6593\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.925630 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.002998 \n","\n","Epoch 6594\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.924770 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.024020 \n","\n","Epoch 6595\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.3%, Avg loss: 0.902310 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.041774 \n","\n","Epoch 6596\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.947835 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.002811 \n","\n","Epoch 6597\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.911693 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.019228 \n","\n","Epoch 6598\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.919616 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.034979 \n","\n","Epoch 6599\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.932351 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.030777 \n","\n","Epoch 6600\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.921572 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.042565 \n","\n","Epoch 6601\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.898839 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.025349 \n","\n","Epoch 6602\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.900161 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.043847 \n","\n","Epoch 6603\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.930321 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.002679 \n","\n","Epoch 6604\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.899957 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.016592 \n","\n","Epoch 6605\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.933880 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.096219 \n","\n","Epoch 6606\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.912398 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.056311 \n","\n","Epoch 6607\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.895342 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.029497 \n","\n","Epoch 6608\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.917383 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.072205 \n","\n","Epoch 6609\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.911460 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.012309 \n","\n","Epoch 6610\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.939117 \n","\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 1.024681 \n","\n","Epoch 6611\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.896893 \n","\n","Test Error: \n"," Accuracy: 61.5%, Avg loss: 1.005174 \n","\n","Epoch 6612\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.895825 \n","\n","Test Error: \n"," Accuracy: 61.3%, Avg loss: 0.992620 \n","\n","Epoch 6613\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.909521 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.014127 \n","\n","Epoch 6614\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.890182 \n","\n","Test Error: \n"," Accuracy: 60.5%, Avg loss: 1.013073 \n","\n","Epoch 6615\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.898442 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.011984 \n","\n","Epoch 6616\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.877040 \n","\n","Test Error: \n"," Accuracy: 61.8%, Avg loss: 0.975077 \n","\n","Epoch 6617\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.6%, Avg loss: 0.892713 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.033559 \n","\n","Epoch 6618\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.1%, Avg loss: 0.890203 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.006258 \n","\n","Epoch 6619\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.933016 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.067750 \n","\n","Epoch 6620\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.920911 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.089437 \n","\n","Epoch 6621\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.2%, Avg loss: 0.912512 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.047971 \n","\n","Epoch 6622\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.908756 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.055584 \n","\n","Epoch 6623\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.923300 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.044159 \n","\n","Epoch 6624\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.945929 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.063389 \n","\n","Epoch 6625\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.908116 \n","\n","Test Error: \n"," Accuracy: 60.7%, Avg loss: 1.052195 \n","\n","Epoch 6626\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.938163 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.056612 \n","\n","Epoch 6627\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.929880 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.052227 \n","\n","Epoch 6628\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.5%, Avg loss: 0.887369 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.029972 \n","\n","Epoch 6629\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.906160 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.057112 \n","\n","Epoch 6630\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.917313 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.036992 \n","\n","Epoch 6631\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.900003 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.007965 \n","\n","Epoch 6632\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.911385 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.027784 \n","\n","Epoch 6633\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.895037 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.037341 \n","\n","Epoch 6634\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.930947 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.032236 \n","\n","Epoch 6635\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.944628 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.042227 \n","\n","Epoch 6636\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.955005 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.093860 \n","\n","Epoch 6637\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.955194 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.061307 \n","\n","Epoch 6638\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.948123 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.063171 \n","\n","Epoch 6639\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.948587 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.027622 \n","\n","Epoch 6640\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.944850 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.037023 \n","\n","Epoch 6641\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.957046 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.134839 \n","\n","Epoch 6642\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.948114 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.100633 \n","\n","Epoch 6643\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.920302 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.019879 \n","\n","Epoch 6644\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.938539 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.051387 \n","\n","Epoch 6645\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.929046 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.058901 \n","\n","Epoch 6646\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.965877 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.086747 \n","\n","Epoch 6647\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.958910 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.095288 \n","\n","Epoch 6648\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.973426 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.049940 \n","\n","Epoch 6649\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.952211 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.105080 \n","\n","Epoch 6650\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.948601 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.049779 \n","\n","Epoch 6651\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.6%, Avg loss: 0.961499 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.082702 \n","\n","Epoch 6652\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.6%, Avg loss: 0.941940 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.072129 \n","\n","Epoch 6653\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.960063 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.060493 \n","\n","Epoch 6654\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.963682 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.026360 \n","\n","Epoch 6655\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.947777 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.126196 \n","\n","Epoch 6656\n","-------------------------------\n","Training Error: \n"," Accuracy: 60.1%, Avg loss: 1.133526 \n","\n","Test Error: \n"," Accuracy: 57.4%, Avg loss: 1.180113 \n","\n","Epoch 6657\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.6%, Avg loss: 1.041644 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.105062 \n","\n","Epoch 6658\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 1.006952 \n","\n","Test Error: \n"," Accuracy: 59.1%, Avg loss: 1.119216 \n","\n","Epoch 6659\n","-------------------------------\n","Training Error: \n"," Accuracy: 61.9%, Avg loss: 1.007610 \n","\n","Test Error: \n"," Accuracy: 57.2%, Avg loss: 1.122377 \n","\n","Epoch 6660\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.1%, Avg loss: 0.984457 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.066629 \n","\n","Epoch 6661\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.3%, Avg loss: 0.996967 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.080087 \n","\n","Epoch 6662\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 0.992612 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.108025 \n","\n","Epoch 6663\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.959425 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.063486 \n","\n","Epoch 6664\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.4%, Avg loss: 0.986241 \n","\n","Test Error: \n"," Accuracy: 58.2%, Avg loss: 1.084542 \n","\n","Epoch 6665\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.956266 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.078147 \n","\n","Epoch 6666\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.993620 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.104346 \n","\n","Epoch 6667\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.945960 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.041889 \n","\n","Epoch 6668\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.0%, Avg loss: 0.964827 \n","\n","Test Error: \n"," Accuracy: 59.9%, Avg loss: 1.061593 \n","\n","Epoch 6669\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.953626 \n","\n","Test Error: \n"," Accuracy: 59.6%, Avg loss: 1.083466 \n","\n","Epoch 6670\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.968914 \n","\n","Test Error: \n"," Accuracy: 58.3%, Avg loss: 1.060518 \n","\n","Epoch 6671\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.961998 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.071687 \n","\n","Epoch 6672\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.2%, Avg loss: 0.957172 \n","\n","Test Error: \n"," Accuracy: 58.4%, Avg loss: 1.086997 \n","\n","Epoch 6673\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.954639 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.077956 \n","\n","Epoch 6674\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.932908 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.057622 \n","\n","Epoch 6675\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.937006 \n","\n","Test Error: \n"," Accuracy: 59.2%, Avg loss: 1.081504 \n","\n","Epoch 6676\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.930190 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.030538 \n","\n","Epoch 6677\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.958165 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.089488 \n","\n","Epoch 6678\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.5%, Avg loss: 0.952950 \n","\n","Test Error: \n"," Accuracy: 58.6%, Avg loss: 1.090076 \n","\n","Epoch 6679\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.937176 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.102902 \n","\n","Epoch 6680\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.8%, Avg loss: 0.946342 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.029707 \n","\n","Epoch 6681\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.7%, Avg loss: 0.924280 \n","\n","Test Error: \n"," Accuracy: 59.4%, Avg loss: 1.054121 \n","\n","Epoch 6682\n","-------------------------------\n","Training Error: \n"," Accuracy: 62.9%, Avg loss: 0.946978 \n","\n","Test Error: \n"," Accuracy: 59.5%, Avg loss: 1.061553 \n","\n","Epoch 6683\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.930065 \n","\n","Test Error: \n"," Accuracy: 58.8%, Avg loss: 1.032209 \n","\n","Epoch 6684\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.938512 \n","\n","Test Error: \n"," Accuracy: 59.3%, Avg loss: 1.055687 \n","\n","Epoch 6685\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.0%, Avg loss: 0.931832 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.040126 \n","\n","Epoch 6686\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.900149 \n","\n","Test Error: \n"," Accuracy: 58.9%, Avg loss: 1.070530 \n","\n","Epoch 6687\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.928075 \n","\n","Test Error: \n"," Accuracy: 60.1%, Avg loss: 1.034066 \n","\n","Epoch 6688\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.924096 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.059067 \n","\n","Epoch 6689\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.915702 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.069934 \n","\n","Epoch 6690\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.2%, Avg loss: 0.930396 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.035892 \n","\n","Epoch 6691\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.9%, Avg loss: 0.907037 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.044007 \n","\n","Epoch 6692\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.921252 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.051518 \n","\n","Epoch 6693\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.3%, Avg loss: 0.914220 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.051559 \n","\n","Epoch 6694\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.923553 \n","\n","Test Error: \n"," Accuracy: 60.8%, Avg loss: 1.069254 \n","\n","Epoch 6695\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.961542 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.043641 \n","\n","Epoch 6696\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.919620 \n","\n","Test Error: \n"," Accuracy: 61.0%, Avg loss: 1.008389 \n","\n","Epoch 6697\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.936706 \n","\n","Test Error: \n"," Accuracy: 59.8%, Avg loss: 1.055534 \n","\n","Epoch 6698\n","-------------------------------\n","Training Error: \n"," Accuracy: 64.0%, Avg loss: 0.919486 \n","\n","Test Error: \n"," Accuracy: 60.2%, Avg loss: 1.073984 \n","\n","Epoch 6699\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.4%, Avg loss: 0.933169 \n","\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.085296 \n","\n","Epoch 6700\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.920640 \n","\n","Test Error: \n"," Accuracy: 59.0%, Avg loss: 1.054085 \n","\n","Epoch 6701\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.8%, Avg loss: 0.905404 \n","\n","Test Error: \n"," Accuracy: 60.0%, Avg loss: 1.044199 \n","\n","Epoch 6702\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.918334 \n","\n","Test Error: \n"," Accuracy: 59.7%, Avg loss: 1.046412 \n","\n","Epoch 6703\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.7%, Avg loss: 0.907253 \n","\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.037638 \n","\n","Epoch 6704\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.1%, Avg loss: 0.930841 \n","\n","Test Error: \n"," Accuracy: 60.4%, Avg loss: 1.052128 \n","\n","Epoch 6705\n","-------------------------------\n","Training Error: \n"," Accuracy: 63.5%, Avg loss: 0.932701 \n","\n"]}],"source":["# Training setup and loop\n","\n","# Initialize the loss function\n","#  nn.CrossEntropyLoss() encapsulates nn.LogSoftmax and nn.NLLLoss\n","loss_fn = nn.CrossEntropyLoss()\n","# Parameter adjustment protocol\n","# He: always start w/ Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# For plotting results\n","animator = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0.0, 1.0],\n","                        legend=['train loss', 'train acc', 'test acc'])\n","binMaskSizeLs = []\n","\n","# Load pipe_to_pipeidx numpy file (for use w/ confussion matrix)\n","pToPIdx_dic = np.load(pToPIdx_filenm, allow_pickle='TRUE').item()   # pToPIdx_filenm from filepaths() call above.\n","# print(f'pToPIdx dict: {pToPIdx_dic}')\n","# print(len(pToPIdx_dic))   # dicts have lengths.\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=False)\n","    # for x,y in train_dataloader:\n","    #   print(x,y)\n","    #   break\n","    tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, entrop_decay, loss_fn, optimizer, pToPIdx_dic, epoch=t+1, mod=mod)\n","    # tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, loss_fn, optimizer, epoch=t+1, mod=mod)\n","    train_metrics = (tr_loss, __)\n","\n","    test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n","    test_acc, confusion_matrix = test_loop(test_dataloader, model, loss_fn, pToPIdx_dic, out_dim=output_dim)\n","    # animator\n","    animator.add(t + 1, train_metrics + (test_acc,))\n","# torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Analysis/display_of_sensor_grid.pt')\n","torch.save(model.state_dict(), sensgrid_filenm)   # sensgrid_filenm from filepaths() call above.\n","# Not sure the following block is necessary\n","# train_loss, train_acc = train_metrics\n","# assert train_loss < 0.5, train_loss\n","# assert train_acc <= 1 and train_acc > 0.7, train_acc\n","# assert test_acc <= 1 and test_acc > 0.7, test_acc\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gTMxTXWcdMz"},"outputs":[],"source":["# Graphics\n","\n","# %matplotlib inline\n","animator.display_plt()\n","# Save to file that is replaced on every run.\n","animator.fig.savefig(loss_filenm, bbox_inches='tight')   # loss_filenm from filepath() call above.\n","# Automate improved filename description.\n","# animator.fig.savefig('loss.png', bbox_inches='tight')\n","# plt.savefig('loss.png', bbox_inches='tight')   # Less specific. Targets active figure.\n","\n","# Pipe labels (strings) are located in simdata_to_csv notebook\n","predictions = [f'{i}' for i in range(output_dim)]\n","# predictions = decode_labels(regdict_filenm)   # May no longer need to maintain regdict_filenm input path or decode_labels()\n","#                                 # Ordered pipe names is lexiconic and not consistent w/ conf mat (regions in number order)\n","# print(predictions)\n","# labels = range(output_dim)\n","# labels = decode_labels(regdict_filenm)\n","# print(ts_dataset[:][2])\n","# print(len(ts_dataset[:][2]))\n","# temp = [x.item() for x in ts_dataset[:][2]]\n","# print(temp)\n","# print(len(temp))\n","# labels = sorted(set(temp))\n","labels = [i for i in range(len(pToPIdx_dic))]\n","# print(labels)\n","# print(len(labels))\n","fig, ax = plt.subplots(1, 1, figsize=(100,100))\n","# fig.set_facecolor('#7d7f7c')\n","im = ax.imshow(confusion_matrix)\n","ax.set_xticks(np.arange(len(predictions)))\n","ax.set_yticks(np.arange(len(labels)))\n","# ax.set_xticklabels(predictions)\n","# ax.set_yticklabels(labels)\n","\n","# Set-up for white grid lines on minor ticks. Creates spacing effect.\n","ax.set_xticks(np.arange(len(predictions)+1) - 0.5, minor=True)\n","ax.set_yticks(np.arange(len(labels)+1) - 0.5, minor=True)\n","# Print white grid to space out the squares.\n","ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n","# Remove spines for clarity.\n","for k, v in ax.spines.items() :\n","  v.set_visible(False)\n","# ax.spines['top'].set_visible(False)   # Can't slice a dictionary.\n","ax.tick_params(which=\"minor\", bottom=False, left=False)\n","\n","# Horizontal labeling displays on top\n","ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n","# Rotate tick labels and set alignment.\n","plt.setp(ax.get_xticklabels(), rotation=-45, ha='right', rotation_mode='anchor')\n","plt.xlabel(f'Predictions -- {ts_size}')\n","# Move the x labels to the top\n","ax.xaxis.set_label_position('top')\n","plt.ylabel('Labels')\n","# Annotate matrix with values by looping over data dimensions\n","for i in range(len(labels)) :\n","  for j in range(len(predictions)) :\n","    text = ax.text(j, i, confusion_matrix[i, j].item(),\n","                   ha='center', va='center', color='white')\n","\n","ax.set_title(f'Confusion Matrix -- Epoch {t+1}')\n","fig.tight_layout()\n","# Save to file that is replaced on every run.\n","fig.savefig(conf_filenm)   # conf_filenm from filepath() call above.\n","torch.save(confusion_matrix, conf_mat_filenm)   # conf_mat_filenm from filepath() call above.\n","# plt.show()\n","# plt.close()"]},{"cell_type":"markdown","metadata":{"id":"4W6b5qaI8ZVj"},"source":["####Sanity Check: Pass a sample to the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gp6zYEZN8mx5"},"outputs":[],"source":["def predict_ch3(net, sample, samp_idx=0):\n","    \"\"\"Predict labels (redefined from d2l Chapter 3.6.7).\"\"\"\n","    print('Model Evaluation')\n","    X, y, pipId = sample[samp_idx]\n","    X = X.reshape([1,-1])\n","    preds = net(X)[0].argmax(axis=1)   # net returns a tuple of preds and prob_params\n","    print(f'Scenario {samp_idx} (pred={preds.item()}, label={y.item()})')\n","\n","predict_ch3(model, tr_dataset, samp_idx=28)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rv4iOGftxLZi"},"outputs":[],"source":["# Test trained model on time stamps it hasn't seen before\n","# tmstp = 168\n","# X, y = cat_data(residual, norm_base, norm_feats, mask, net_char, tmstp)\n","# ts_dataset = TensorDataset(X, y)\n","\n","# test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n","# test_acc = test_loop(test_dataloader, model, loss_fn)"]},{"cell_type":"markdown","metadata":{"id":"k5xwqcQyNoBV"},"source":["####Scratch Work"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeYlNlR6WH-P"},"outputs":[],"source":["# catting to an empty tensor -- doesn't work\n","ls = [1, 2, 3]\n","tsr = torch.Tensor(ls)\n","print(tsr)\n","tsr1 = torch.ones([2,2])\n","print(tsr1)\n","tsr = torch.concat([tsr1])\n","print(tsr)\n","test = None\n","if test is None :\n","  print('is None!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCehwyn6imcp"},"outputs":[],"source":["print(0.991 > 0.99)\n","x = torch.rand(5)\n","print(x)\n","y = torch.where(x > 0.6, x, torch.tensor(0.))\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGxSZV4cDLu7"},"outputs":[],"source":["import math\n","x = float('nan')\n","print(x)\n","print(not math.isnan(x))\n","print(1 + 0.0 * x)\n","print(torch.log(torch.tensor(0.0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-f8VELnBsjNg"},"outputs":[],"source":["x_tup = ([2], [3], [4])\n","# x_tup = (2, 3, 4)\n","x_tup[1][0] -= x_tup[0][0]\n","# x_tup[1] -= x_tup[0]\n","# print(x_tup[1] - x_tup[0])\n","print(x_tup[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKcdDEqt7LVH"},"outputs":[],"source":["t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n","print(t)\n","t1 = torch.rand(10, generator=torch.Generator().manual_seed(11))\n","print(t1)\n","print(t[t1 > 0.5])   # returns a tensor containing only those elems for which test returns true.\n","t[t1 > 0.5] = 0   # Assigns zero to only those elems for which the test returns true.\n","print(t)\n","print(t[t1 > 0.5])\n","print(t[0])   # Returns a zero dim tensor. (num)\n","print(t[0:1])   # Returns a 1-dim tensor. ([num])\n","print(t[0:1].new(1))\n","print(t[0:1].new_empty(1).uniform_())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9aks4N-SHt5"},"outputs":[],"source":["t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n","print(' t:', t)\n","idxs = torch.randperm(5, generator=torch.Generator().manual_seed(10))\n","print(idxs)\n","tn = t[idxs]\n","print(' t:', t)\n","print('tn:', tn)\n","print(' t:', t[2])\n","print('tn:', tn[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpjdHiXWtwOl"},"outputs":[],"source":["y_hat = torch.arange(20).reshape([2, -1])\n","print(y_hat)\n","len(y_hat)\n","print(y_hat.sum(1))\n","print(y_hat.argmax(dim=0))\n","print(y_hat.argmax(dim=1))\n","data = [[1.0, 1.0], [1.0, 1.0]] * 2   # multiplies the number of elements (like if you had 2 apples and then multiplied them by 2; you now have four apples)\n","print(data)\n","# print(data / data)   # dividing a list is not defined\n","(1,2) + (3,)   # cats the three elems\n","len((1,2))   # tuples have __len__ defined\n","[[] for _ in range(3)]\n","rows = [[1,1]]\n","print(rows)\n","[rows.append(i) for i in [[3,3],[4,4]]]\n","print(rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPQBiMXCOahN"},"outputs":[],"source":["# X_masked = None\n","# masked_feats = torch.rand(15)\n","# print(masked_feats)\n","# mask = torch.randint(2, [15])\n","# print(mask)\n","# # Mask and masked features\n","# # May want sensing_mask_rand() to process batches of samples\n","# for i in range(5):\n","#   temp = torch.cat((masked_feats, mask)).reshape([1,-1])\n","#   print(temp)\n","\n","#   if X_masked is None:\n","#     X_masked = temp   \n","#     print(X_masked)\n","#   else:  \n","#     X_masked = torch.cat((X_masked, temp))\n","#     print(X_masked)\n","\n","# for i in X_masked:\n","#   print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4kiAKeKfBIp"},"outputs":[],"source":["# size = [100,]\n","# K = 20\n","# tn = torch.zeros(size)\n","# mask = torch.zeros(tn.size())\n","# print(mask.size())\n","# print(mask)\n","# indices = torch.randint(len(tn), size=(K,))\n","# print(indices)\n","# for idx in indices:\n","#   mask[idx] = 1\n","# print(mask)\n","\n","# mask = torch.cuda.FloatTensor(3, 3).uniform_()\n","# # tensor of floats\n","# mask = torch.FloatTensor(3,3).uniform_()\n","# print(mask)\n","# # tensor of booleans (?? how ??)\n","# mask = torch.FloatTensor(3,3).uniform_() > 0.8\n","# print(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxvQSxfCAX_R"},"outputs":[],"source":["# converting string labs to labels ranging from 0 -> num_of_classes (i.e. possible leak locations)\n","#  what if not all of the possible leak locations are used?\n","#  1) I can set the output dim to len of label_subset (easiser)\n","#  2) I can force the set to be all the possible fixed pipe locations (coordinating this will be tricky)\n","# labs = [1,2,2,3,1,4,4,3]\n","# lab_dict = {}\n","# encoded_labs = []\n","# label_subset = set(labs)\n","# print(label_subset)\n","# print(type(label_subset))\n","# print(len(label_subset))\n","# for i, key in enumerate(label_subset):\n","#   lab_dict[key] = i\n","# print(lab_dict)\n","# for key in labs:\n","#   encoded_labs.append(lab_dict[key])\n","# print(encoded_labs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snpJEOWKoc5Y"},"outputs":[],"source":["# Reshaping practice\n","# base_file = 'simdata/_base_/node_demand.csv'\n","# data_file = base_file\n","# data = pd.read_csv(data_file)\n","# data_tn = torch.tensor(data.values, dtype=torch.float32)\n","# data_tn[:,1:].reshape([1,-1])\n","\n","# data_tn = torch.arange(20).reshape([4,5])\n","# print(data_tn)\n","# data_tn = data_tn[:,1:].reshape([1,-1])\n","# print(data_tn)\n","# data_tn1 = torch.arange(20).reshape([4,5])\n","# print(data_tn1)\n","# data_tn1 = data_tn1[:,1:].reshape([1, -1])\n","# print(data_tn1)\n","# torch.cat((data_tn1, data_tn))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DGW06k6V-qI"},"outputs":[],"source":["# Extracting an intelligible answer from the model\n","# x = torch.arange(16, dtype=torch.float32).reshape((4,4))\n","# print(x)\n","# print(x.sum(axis=0))\n","# print(x.sum(axis=[0,1]))\n","# mean = x.sum() / x.numel()\n","# print(mean)\n","# # notice we keep all dims (tensor of a tensor ie. two brackets) vs above we lost one (just a tensor)\n","# print(x.sum(dim=0, keepdim=True))\n","\n","# y = torch.tensor([3,3,3,3])\n","# # x.argmax(1).type(y.dtype) == y\n","# correct = 0\n","# correct += (x.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n","# correct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kndCfp0DOku8"},"outputs":[],"source":["# Handy timer class\n","class Timer:\n","    \"\"\"Record multiple running times.\"\"\"\n","    def __init__(self):\n","        self.times = []\n","        self.start()\n","\n","    def start(self):\n","        \"\"\"Start the timer.\"\"\"\n","        self.tik = time.time()\n","\n","    def stop(self):\n","        \"\"\"Stop the timer and record the time in a list.\"\"\"\n","        self.times.append(time.time() - self.tik)\n","        return self.times[-1]\n","\n","    def avg(self):\n","        \"\"\"Return the average time.\"\"\"\n","        return sum(self.times) / len(self.times)\n","\n","    def sum(self):\n","        \"\"\"Return the sum of time.\"\"\"\n","        return sum(self.times)\n","\n","    def cumsum(self):\n","        \"\"\"Return the accumulated time.\"\"\"\n","        return np.array(self.times).cumsum().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3m7r6ps_rDM"},"outputs":[],"source":["# Target transform\n","from torchvision.transforms import Lambda\n","\n","train_size = 700\n","# target_transform = Lambda(lambda y: torch.zeros(\n","#     (train_size, output_dim), dtype=torch.float).scatter_(\n","#         dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))\n","\n","# one-hot classification label vector\n","target_transform = Lambda(lambda y: torch.scatter_(\n","        dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["4Epm5SJkrzLx","VQzqACaz6D71","ydc9rlm86Hmt","3ocO0Om3ofeX","RVZsPo1p2RoP","TmFOoA-k258k","5wCj-uKc561c","7KkFT5x93F5H","j_Zwmb6sYpbm","qysp3xV5W2dg"],"name":"bernoulli8_20regions_mult_tmstps.ipynb","provenance":[{"file_id":"1YeSJUnijlqIt0y5eDayQvwNGRwSKSkYZ","timestamp":1649218750773},{"file_id":"13Yy2CsIHuUcYXGOkYgQOdktXEL2CHOFJ","timestamp":1639297478820},{"file_id":"1pSJ226BsrXMVoAQJ0BKrxwgrHyxGHvI5","timestamp":1636667778711},{"file_id":"1VHyXaGHoAm4NI3ahzbXhGxODiO4mg4AM","timestamp":1632939473633},{"file_id":"19ehUGgFEEdAcgFs-fy_SW0vkxJggJ5XW","timestamp":1631899911015},{"file_id":"1_vt4FQCGh7KSHkU7GKarP807bTNxTZV7","timestamp":1631190383013},{"file_id":"1Nf2Ay7YXjx6JBvs9gsySlwirTOO-Ownd","timestamp":1628629596966},{"file_id":"1pp1nL2XUKNxN0QscNo-VzAeWMS07sl53","timestamp":1627669468832},{"file_id":"1JCW99kbx6_NEJS5KKhCRGP9tz5YE3eCZ","timestamp":1627178038295},{"file_id":"1Hhtzwkvq30pxSybpIcmD4Wb4eRB0bVmO","timestamp":1626718176246},{"file_id":"1rFofuDkzfAOVxLTpBsYt28pthUMCMtOL","timestamp":1626651085984},{"file_id":"19eoMaxuZB18-ZWdJZdgDr1MD6qAJR0L3","timestamp":1626329222475}],"mount_file_id":"1aMjhOwFRW3UtPGVIo7kBkZoL5q7UQTyH","authorship_tag":"ABX9TyPxX4KtRDN7dTxUEq6J6419"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}